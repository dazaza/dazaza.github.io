<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>关于GPT-3考试的几点思考</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">关于GPT-3考试的几点思考</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2020-06-14 05:53:07</div><div class="story_img_container"><a href="http://img.diglog.com/img/2020/6/a62291d4590d1fd0bee058f8a5f13899.png"><img src="http://img.diglog.com/img/2020/6/a62291d4590d1fd0bee058f8a5f13899.png" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>这是；以前的版本，()。这是修改历史提要的摘要，与我的捐赠者在Patreon上带给您的内容重叠。</p><p>由于达到了TinyLetter5000订阅者的限制，这份时事通讯本月转移到了Substack。除了已知的长度截断问题外，请告诉我任何问题。(请注意，在桌面上阅读网站版本是注释等的推荐方式。)。</p><p>On，Brown等人2020年(比较；随机样本；带真实世界演示的“OpenAI”)。</p><p>学会学习。OA发布了人们期待已久的后续产品，一个可以统治所有人的模型：117✕的175B参数模型，它具有更强大的语言生成功能，使它能够解决从算术到英语翻译，从解字谜到类比的各种问题-纯粹是从文本示例的提示，而不是任何专门的培训或微调，只是对大型互联网文本语料库的下一词预测训练。这意味着它的注意力机制起到了通过训练足够多样的数据1来“学会学习”的作用，迫使它做更多的事情，而不仅仅是学习普通的文本关系。就像OpenAI几周前宣布的那样，似乎已经沉没得几乎无影无踪，所以我将比往常更深入。</p><p>“攻击只会变得更好。”两年前，有趣的是，它的“情感神经元”是有用的预训和可爱的。1年前，凭借其出色的文本生成和微调能力令人印象深刻。今年是可怕的一年，因为与2相比，它是一个小而浅的模型，简单的统一体系结构3以最愚蠢的方式(下一个文本令牌的单向预测)在单个贫穷的模态(随机互联网文本转储4)上对微小数据(适合笔记本电脑)进行训练，然而，第一个版本已经表现出疯狂的运行时元学习-而且伸缩曲线仍然不弯曲！这些样本也比以往任何时候都要好，无论是发明新的阴茎笑话，还是写关于旋转阵列的文章(大部分都是有效的)。</p><p>不是全部，而是很大一部分。它是在每项任务上都设置的吗？不，当然不是。但问题不是我们能否在法律上找到任何可能不起作用的方法，而是。而且有很多方法可以更好地工作(参见“限制”一节中的几个)。有没有像操纵机器人绕着SF向人类发射激光和火箭之类的事情？不，当然不是。它“仅仅”是一个文本预测模型，一个愚蠢的文本专家；但我们应该记住，一个愚蠢的专家只是一个基因突变，或者是一个远离正常人的小脑损伤。如果RL是监督学习顶部的樱桃，而非监督学习是无监督学习蛋糕上的糖霜，那么，面包师们正在变得相当不错。</p><p>缩放仍在工作。我很惊讶，因为我预计更接近100b的参数，我认为/的性能表明，尽管有缩放论文6，但缩放曲线已经开始弯曲，到100b时，可能很难证明进一步缩放是合理的。然而，在比例因子没有明显变化的情况下，它达到了这个数字的两倍：它的比例仍然大致是对数/幂律，就像预测的那样，规模小得多的模型也没有达到这样的水平，即收益实际上停止增长，或者开始需要大幅提高，远远超出了可行性。这表明，采用数万亿个参数是可能的，也是有用的(这些参数仍在可用计算和安培预算之内，只需要数千个标普预算，或许需要1000万至1亿美元预算，假设不会有任何改善，参见本期的Hernandez&amp；Brown 2020等)，再看看图表，许多基准(比如)将下降10个参数。</p><p>“根据”迈向未来“的壮观表现推断，生命、宇宙和万物的答案仅仅是4.398万亿个参数。”</p><p>我们不知道如何训练NN。正如我一直说的那样，“NN是懒惰的”，当我们推动他们超越简单的答案&廉价的捷径时，他们能做的比我们让他们做的要多得多。越硬越大，越好。(此外，还可以提到半监督学习的最新进展&基于模型的复兴。)。</p><p>规模的好处：稳定性，→，泛化，→，元学习。虽然DL受到其训练和数据的阻碍，但DL却享受着一种不合理有效的维度加持--仅仅在大量数据上简单地训练一个大型模型就能产生更好的特性，比如元学习，而不需要在架构中建立哪怕是一丁点的东西；总的来说，在更多更难的任务上进行训练会创造出更接近人类的性能、通用性和健壮性。OA5不仅可以扩展到数百万个小批量，而且可以稳定在数百万个小批量。OA5-Like在大规模图像数据集(如&amp；受益于异常大的小批量)时稳定，而分类器(如Transfer&amp；Robutify)具有类似人类的错误7，多模态学习在较少的数据(例如，激励)上产生更好的表示，而S可以。达到人的水平，与数百名竞争的自我参与者一起覆盖可能的策略。模仿学习喜欢概括在几百个任务上，训练出一张深厚的网。当W嵌入足够深时，或者在//有足够的样本以强制因式分解的情况下，解缠出现。在数百万个域随机的训练中，诱导了类似的隐式元学习，在每个运行时调用期间，探测其环境并将其对机器人手控制的理解编码到其隐藏状态；并且通过2阶缩放比传统的机器人规划器性能更好。或者，数百个级别的培训单独培训特工，但在数千个级别，他们开始概括到看不见的级别。仅仅通过训练一个更大的模型来研究更丰富的信号，而不需要任何搜索，就证明了真正的超人不会产生“错觉”&而且，就这一点而言，还展示了，仅仅是端到端地训练，根据足够的数据预测奖励，就足以取代AlphaZero，隐含地学习树搜索(但更好)。不胜枚举。</p><p>伸缩性假设认为，一旦我们找到了像自我关注或卷积这样的可扩展架构，我们只需训练越来越大的NN，越来越复杂的行为就会自然而然地成为针对所有任务和数据进行优化的最简单方式，这一假设看起来越来越可信。</p><p>一直在追踪。2020年是回顾过去十年的最好时机。在2010年，人们可以很容易地把世界上所有真正相信深度学习的人都安排到一个中等大小的会议室里(其中3个人正在忙于创建，这一事实稍微提供了一些帮助)。2010年，对机器学习感兴趣的人可能已经读到了一些关于使用全部100-200万个参数识别手写数字的内容，或者对标准隐马尔可夫模型语音识别进行了一些适度的神经调整。在2010年，谁会预测，在接下来的10年里，深度学习将经历寒武纪的爆炸，导致整个机器学习中替代方法的大规模灭绝，模型将扩展到1.75亿个参数，这些巨大的模型将自发地发展所有这些能力，除了少数被人工智能社区其他人(别管世界)认为是故意欺骗的老派狂热分子之外的少数顽固连接主义者，如Schmidhuber，Sutskever，Legg，&Amp；Amot；</p><p>后见之明是20/20。即使在2015年，扩展假说似乎也非常值得怀疑：毕竟，你需要一些东西来进行扩展，而且我们很容易看到现有系统中的缺陷，然后想象它们永远不会消失，进展会在现在任何一个月、很快就会出现。就像基因组学革命一样，几位有远见的预言家推断，胚胎所需的基因将呈指数级增长&很快就会产生强大的基因，而清醒的专家则对“缺失的遗传性”绞尽脑汁&嘲笑这样的n要求如何被证明是一个失败的范式。未来一开始是缓慢的，然后很快就会到来。然而，我们在这里：所有的荣誉属于狂热者，耻辱和耻辱属于批评者！8如果有人能回到10年前，甚至5年前，看着每一位人工智能研究人员读到这篇论文…都会头晕目眩就好了。不幸的是，现在似乎没有几个人头脑发热，因为人类事后诸葛亮的能力是无穷无尽的(“我可以通过微调得到那么多，不管怎么说，我一直都预测到了，多么无聊”)。(如果您仍然确定在未来几十年内出现这种情况的可能性几乎为零，为什么？你有没有--书面预测--像这样的能力？这是你认为人工智能失败在未来几十年的样子吗？什么具体的任务，什么具体的数字，会让你信服呢？世界看起来会有什么不同</p><p>，Hernandez&amp；Brown 2020(/；第一个原型从来不是最好的原型，但如果有足够的计算时间，您可以对其进行改进，并弄清楚应该如何从头到尾完成它，本文量化了自2012年以来的神经网络硬件过剩：“现在训练✕需要的计算减少了44…。达到AlexNet的水平“。不足为奇(如in)；参阅/。我们不知道如何训练正确的神经网络，并用最简单的事情犯下巨大的错误，因为能力会像Resnet或EfficientNet一样跳跃，或者偶尔提醒我们。)。</p><p>，Svyatkovski等人2020年(不清楚是否应用；另请参阅少数代码完成功能)。</p><p>(；一个模型，7个数据集，8900万张图像，83个损失/任务，全球搜索质量提升+8%)。</p><p>()；(一个简单但高质量的，也可以在上找到；有趣的是，由于法律上的模糊性&amp；一些艺术家如何表现得像动物一样，咆哮着“偷窃”&amp；免费的Fursonas是一只穿着羊皮的狼，扰乱了他们的啄食顺序9-尽管创作者已经智胜了纸老虎的威胁，但这些令人不快的问题将困扰ML，因为DL模特像兔子一样繁殖)。</p><p>，Sommers&Amp；Bohns 2019(人们不善于预测对警方要求的抵制；另见)。</p><p>，Hedrick 2017 10(“…。在过去的几千年里，我们人类一直是地球上患病最严重的物种“；跟进)。</p><p>，DANES&A；Widom 2020(心理学中没有任何东西是有意义的，除非从个体差异的角度来看)。</p><p>，Curran&amp；Hauser 2019(“蜥蜴人恒定”响应者如何证明这一点？或者，“自由回应是魔鬼”)</p><p>，Bigelow等人2014(为什么数据可视化如此糟糕-表面上很漂亮，但却具有误导性或无用？因为许多设计师不看数据，所以避免自动手动创建，这样他们就可以专注于漂亮的形状/颜色&amp；喜欢摆弄数据，而忽略了读者)。</p><p>Yanet al 2020(“采用广告拦截器的用户随后消费对应于10%多个类别的20%以上的新闻文章。这种影响会随着时间的推移而持续存在…“。；请参阅)。</p><p>(供需情况：如果允许涨价，口罩在任何地方都很容易制造&出售并不违法)。</p><p>(弗兰克·布拉萨(Frank Bourassa)如何欺骗一家瑞士磨坊向他出售独一无二的亚麻纸，为Perfect&amp；创造了3.1725亿美元的2.012亿美元，基本上逍遥法外)。</p><p>“un Jour joueur”(乌鸦的Jig；une semaine chez lesÉcarlates{2018})[古典]。</p><p>“Bons et Mauvais jours”(Raven‘s Jig；une semaine chez lesÉcarlates{2018})[古典]。</p><p>“巴尔蒂马尔的早晨”(Mane in Green；II.The Journey[迷失蓝宝石的探索-EP。2]{2017})[器乐摇滚]。</p><p>“风暴来了[单一目的的回忆]”(UnDreamedPanic壮举。Metajoker；Ignite{2020})[岩石]</p><p>“来吧，甜蜜的死神[Komm，Süsser Tod]”(普拉蒂娜爵士壮举。Niklas Gabrielsson；动画标准第6卷{2019})[爵士乐]。</p><p>(现在)计算成本几乎不会超过几百万美元，而且运行成本也很低(第39页：“即使有完整的175B，从一个训练有素的模型生成100页内容的成本大约是0.4千瓦时，或者只需要几美分的能源成本。”)，而(否则无用的)深蓝人工智能项目据说要花费&GT；最后一次迭代的1051.97亿美元(1.91亿美元的报告似乎与许宗衡的《深蓝背后》(Pg187)中提到的宣传预估价值相混淆)，以及像Blow&GT；5000✕这样的大科学项目的资金大多失败了。几十年前，用全球计算资源和科学预算就可以做到；而今天的硬件和预算，我们根本不知道或不想做，又能做什么呢？存在硬件悬置。(另请参阅全脑仿真路线图&amp；)↩︎。</p><p>例如，不使用或神经体系结构搜索来尝试定制模型，甚至不决定基本的超参数，如宽度(如图所示，即使在“易于理解和手工优化的香草体系结构”中，这也会产生很大的不同)。↩︎。</p><p>甚至没有-所以没有谷歌图书，没有Arxiv，没有利普根，没有Sci-Hub…。↩︎。</p><p>“一个男人在医生的办公室，医生告诉他，”我有一些好消息和一些坏消息要告诉你。“/那个人说，”嗯，我现在不能接受这个坏消息，所以先告诉我这个好消息。“/医生说，”好消息是你有一个18英寸的阴茎。“/这个人看起来呆了一会儿，然后问，”坏消息是什么？“/医生说，”嗯，好消息是你有一个18英寸的阴茎。“/这个男人看起来呆了一会儿，然后问，”坏消息是什么？“/医生说，”嗯，好消息是你的阴茎有18英寸。“。“你的脑子在你那话儿里。”“↩︎”</p><p>具体地说：、。值得注意的是，对大型模型的追求几乎完全是由OpenAI&amp；行业实体推动的(后者满足于小得多的模型)，学术界表现出了几乎完全的厌恶(甚至是厌恶)。尽管所有的比例假设是“显而易见的”，比例是“预测的”，但人们对实际操作的兴趣却非常小。也许我们应该更多地关注人们做什么，而不是他们说什么。↩︎。</p><p>图像缩放实验的一个有趣的方面是，即使在原始任务的性能“持平”于接近标签错误的情况下，迁移学习仍在继续改善。显然，内部表示，即使仅用于分类，因此分数不能增加超过一个小的百分比，也会变得更像人类的编码？我注意到，对于语言模型，损失的最后一部分似乎对生成的样本质量有很大影响，可能是因为只有在完成了所有更简单的建模之后，懒惰的语言模型才被迫通过更正确地建模更复杂的东西(如逻辑、对象、世界知识等)来挤出下一步的性能。↩︎。</p><p>现在，这种可能性很小，而且已经开始让加里·马库斯(Gary Marcus)等人对WinoGrande感到有点紧张，他们已经开始思考为什么Winograd模式是常识推理/智能的良好衡量标准(因为智能，当然，是任何人工智能还不能做的事情)。↩︎</p><p>别担心：我们已经有空头--做空和做空耳朵--来对冲弗索纳通胀。尽管如此，我们建议在90年代的形象宏观基金中持有大量头寸，以便从转向优质和羊群的过程中受益：这将是一个古怪债券的熊市-这并不是牛市。↩︎。</p><p>，Sabin&amp；Boulger 1973(在猴子和猴子组织中穿行数十次)。</p><p>↩︎</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://www.gwern.net/newsletter/2020/05#gpt-3">https://www.gwern.net/newsletter/2020/05#gpt-3</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/gpt/">#gpt</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/学习/">#学习</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>