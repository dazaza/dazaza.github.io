<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>(æµ…ï¼Ÿ)ã€‚å¼ºåŒ–å­¦ä¹ </title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">(æµ…ï¼Ÿ)ã€‚å¼ºåŒ–å­¦ä¹ </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2020-10-18 07:11:18</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2020/10/8901794480b4f9b070c96b46b7180981.jpeg"><img src="http://img2.diglog.com/img/2020/10/8901794480b4f9b070c96b46b7180981.jpeg" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>Recent feats such as AlphaGoâ€™s victory over the worldâ€™s best Go player have brought reinforcement learning (RL) to the spotlight. However, what is RL and how does it achieve such remarkable results?</p><p>æœ€è¿‘çš„å£®ä¸¾ï¼Œæ¯”å¦‚AlphaGoæˆ˜èƒœäº†ä¸–ç•Œä¸Šæœ€å¥½çš„å›´æ£‹é€‰æ‰‹ï¼ŒæŠŠå¼ºåŒ–å­¦ä¹ (RL)å¸¦åˆ°äº†èšå…‰ç¯ä¸‹ã€‚ç„¶è€Œï¼Œä»€ä¹ˆæ˜¯RLï¼Œå®ƒæ˜¯å¦‚ä½•å–å¾—å¦‚æ­¤æ˜¾è‘—çš„æ•ˆæœçš„å‘¢ï¼Ÿ</p><p> In this first article, we will explore the Monte Carlo Control Method (not the deep kind) which, despite being elegantly simple, is the basis upon which some of the most advanced RL is built.</p><p>åœ¨ç¬¬ä¸€ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨è’™ç‰¹å¡ç½—æ§åˆ¶æ–¹æ³•(ä¸æ˜¯æ·±åº¦æ§åˆ¶æ–¹æ³•)ï¼Œå°½ç®¡è¯¥æ–¹æ³•éå¸¸ç®€å•ï¼Œä½†å®ƒæ˜¯æ„å»ºä¸€äº›æœ€å…ˆè¿›çš„RLçš„åŸºç¡€ã€‚</p><p>  RL problems consist of (at least) 2 entities: The  agent and the  environment, as shown in the figure below. The environment gives the agent a  state (also called an  observation). The agent then chooses an  action based on the provided state and applies it to the environment. The environment then replies to the action by giving the agent a  reward (a score for the action).</p><p>RLé—®é¢˜åŒ…æ‹¬(è‡³å°‘)ä¸¤ä¸ªå®ä½“ï¼šä»£ç†å’Œç¯å¢ƒï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚ç¯å¢ƒç»™äºˆä»£ç†ä¸€ç§çŠ¶æ€(ä¹Ÿç§°ä¸ºè§‚å¯Ÿ)ã€‚ç„¶åï¼Œä»£ç†æ ¹æ®æä¾›çš„çŠ¶æ€é€‰æ‹©æ“ä½œï¼Œå¹¶å°†å…¶åº”ç”¨äºç¯å¢ƒã€‚ç„¶åï¼Œç¯å¢ƒé€šè¿‡ç»™ä»£ç†å¥–åŠ±(è¯¥æ“ä½œçš„åˆ†æ•°)æ¥å›åº”è¯¥æ“ä½œã€‚</p><p>  For example, consider a kid (the agent) playing a game (the environment) for the first time. The kid starts by seeing the game screen containing all its elements (the state) and decides on an action to take. To which the game scores him (the reward) and the process reprises until the game ends (we consider environments with a clear termination  episodic). After enough repetitions, the kid will start to understand how his actions influence the environment and (assuming he is a competitive child) choose the actions that maximizes his score.</p><p>ä¾‹å¦‚ï¼Œè€ƒè™‘ä¸€ä¸ªå­©å­(ä»£ç†)ç¬¬ä¸€æ¬¡ç©æ¸¸æˆ(ç¯å¢ƒ)ã€‚å­©å­é¦–å…ˆçœ‹åˆ°åŒ…å«æ‰€æœ‰å…ƒç´ (çŠ¶æ€)çš„æ¸¸æˆå±å¹•ï¼Œç„¶åå†³å®šè¦é‡‡å–çš„è¡ŒåŠ¨ã€‚æ¸¸æˆç»™ä»–æ‰“åˆ†(å¥–åŠ±)ï¼Œè¿‡ç¨‹é‡å¤åˆ°æ¸¸æˆç»“æŸ(æˆ‘ä»¬è®¤ä¸ºç¯å¢ƒæœ‰æ˜ç¡®çš„ç»ˆæ­¢æ’æ›²)ã€‚ç»è¿‡è¶³å¤Ÿçš„é‡å¤ä¹‹åï¼Œå­©å­å°†å¼€å§‹ç†è§£ä»–çš„è¡Œä¸ºå¦‚ä½•å½±å“ç¯å¢ƒï¼Œå¹¶(å‡è®¾ä»–æ˜¯ä¸€ä¸ªå¥½èƒœå¿ƒå¼ºçš„å­©å­)é€‰æ‹©èƒ½ä½¿ä»–çš„åˆ†æ•°æœ€å¤§åŒ–çš„è¡Œä¸ºã€‚</p><p> An RL agent, attempts to do exactly the same. However, unlike a human child, computers donâ€™t yet possess innate intelligence. So how does the computer learn what the best actions should be? For the remainder of the text we will distill this problem and re-derive an algorithm that provides a solution.</p><p>RLä»£ç†è¯•å›¾æ‰§è¡Œå®Œå…¨ç›¸åŒçš„æ“ä½œã€‚ç„¶è€Œï¼Œä¸äººç±»å„¿ç«¥ä¸åŒçš„æ˜¯ï¼Œè®¡ç®—æœºè¿˜ä¸å…·å¤‡å¤©ç”Ÿçš„æ™ºèƒ½ã€‚é‚£ä¹ˆï¼Œè®¡ç®—æœºå¦‚ä½•å­¦ä¹ æœ€å¥½çš„åŠ¨ä½œåº”è¯¥æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿå¯¹äºæ–‡æœ¬çš„å…¶ä½™éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†æå–è¿™ä¸ªé—®é¢˜ï¼Œå¹¶é‡æ–°æ¨å¯¼å‡ºæä¾›è§£å†³æ–¹æ¡ˆçš„ç®—æ³•ã€‚</p><p>  Monte Carlo Learning is a subfield of RL that is well suited for solving finite (limited number of states) Markov Decision Processes (MDPs). An MDP is just a class of problems for which knowledge of the current state provides sufficient information to decide on an optimal action to take. Chess is an example of a problem that can be framed as an MDP. The board state at any point in time contains all the information required to determine the next action.</p><p>è’™ç‰¹å¡ç½—å­¦ä¹ æ˜¯RLçš„ä¸€ä¸ªå­åŸŸï¼Œéå¸¸é€‚åˆäºæ±‚è§£æœ‰é™(æœ‰é™çŠ¶æ€æ•°)çš„é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(MDP)ã€‚MDPåªæ˜¯ä¸€ç±»é—®é¢˜ï¼Œå¯¹äºè¿™äº›é—®é¢˜ï¼Œå½“å‰çŠ¶æ€çš„çŸ¥è¯†æä¾›äº†è¶³å¤Ÿçš„ä¿¡æ¯æ¥å†³å®šè¦é‡‡å–çš„æœ€ä½³è¡ŒåŠ¨ã€‚å›½é™…è±¡æ£‹å°±æ˜¯ä¸€ä¸ªå¯ä»¥è¢«æ¡†å®šä¸ºMDPçš„é—®é¢˜çš„ä¾‹å­ã€‚ä»»ä½•æ—¶é—´ç‚¹çš„æ¿çŠ¶æ€éƒ½åŒ…å«ç¡®å®šä¸‹ä¸€æ­¥æ“ä½œæ‰€éœ€çš„æ‰€æœ‰ä¿¡æ¯ã€‚</p><p> Letâ€™s consider the problem depicted below. We have a household floor cleaning robot (the agent) that has a battery that can be either LOW or HIGH in charge. The robot can be either CLEANING or DOCKED. Thus, the robotâ€™s environment consists of 4 possible states: {CLEANING, HIGH}, {CLEANING, LOW}, {DOCKED, HIGH}, {DOCKED, LOW}. The robot controller always chooses between 3 actions: CLEAN, DOCK or WAIT, and the environment provides the agent with a reward for its action. However, since our battery sensor is not precise and our robot may get lost, our transitions are stochastic (probabilistic in nature).</p><p>è®©æˆ‘ä»¬è€ƒè™‘ä¸€ä¸‹ä¸‹é¢æè¿°çš„é—®é¢˜ã€‚æˆ‘ä»¬æœ‰ä¸€ä¸ªå®¶ç”¨åœ°æ¿æ¸…æ´æœºå™¨äºº(æ¸…æ´å‰‚)ï¼Œå®ƒçš„ç”µæ± å¯ä»¥æ˜¯ä½ç”µé‡çš„ï¼Œä¹Ÿå¯ä»¥æ˜¯é«˜ç”µé‡çš„ã€‚æœºå™¨äººå¯ä»¥æ˜¯æ¸…æ´çš„ï¼Œä¹Ÿå¯ä»¥æ˜¯å¯¹æ¥çš„ã€‚å› æ­¤ï¼Œæœºå™¨äººçš„ç¯å¢ƒç”±4ç§å¯èƒ½çš„çŠ¶æ€ç»„æˆï¼š{æ¸…æ´ï¼Œé«˜}ï¼Œ{æ¸…æ´ï¼Œä½}ï¼Œ{åœé ï¼Œé«˜}ï¼Œ{åœé ï¼Œä½}ã€‚æœºå™¨äººæ§åˆ¶å™¨æ€»æ˜¯åœ¨3ä¸ªåŠ¨ä½œä¸­è¿›è¡Œé€‰æ‹©ï¼šæ¸…æ´ã€åœé æˆ–ç­‰å¾…ï¼Œç¯å¢ƒä¼šä¸ºä»£ç†çš„åŠ¨ä½œæä¾›å¥–åŠ±ã€‚ç„¶è€Œï¼Œç”±äºæˆ‘ä»¬çš„ç”µæ± ä¼ æ„Ÿå™¨ä¸ç²¾ç¡®ï¼Œæˆ‘ä»¬çš„æœºå™¨äººå¯èƒ½ä¼šè¿·è·¯ï¼Œæ‰€ä»¥æˆ‘ä»¬çš„è½¬æ¢æ˜¯éšæœºçš„(æœ¬è´¨ä¸Šæ˜¯æ¦‚ç‡çš„)ã€‚</p><p>  For instance, if the robot is currently in the state of CLEANING with HIGH battery and chooses to continue cleaning it has an 80% chance of receiving a reward of 1 and continuing on the same state. At this point, it becomes important to introduce some notation. We express a probability (from 0 to 1) of a new state  sâ€² and reward  r given a previous state  s and action  a as:  p(sâ€²,r | s,a). Therefore, we could write our opening sentence as:  p({cleaning,high},1 | {cleaning,high},clean)=0.8.</p><p>ä¾‹å¦‚ï¼Œå¦‚æœæœºå™¨äººå½“å‰å¤„äºç”µæ± ç”µé‡è¾ƒé«˜çš„æ¸…æ´çŠ¶æ€ï¼Œå¹¶é€‰æ‹©ç»§ç»­æ¸…æ´ï¼Œåˆ™æœ‰80%çš„æœºä¼šè·å¾—1çš„å¥–åŠ±å¹¶åœ¨ç›¸åŒçŠ¶æ€ä¸‹ç»§ç»­æ¸…æ´ã€‚åœ¨è¿™ä¸€ç‚¹ä¸Šï¼Œå¼•å…¥ä¸€äº›ç¬¦å·å˜å¾—å¾ˆé‡è¦ã€‚æˆ‘ä»¬å°†ç»™å®šå…ˆå‰çŠ¶æ€så’ŒåŠ¨ä½œaçš„æ–°çŠ¶æ€sâ€˜çš„æ¦‚ç‡(ä»0åˆ°1)è¡¨ç¤ºä¸ºï¼šP(sâ€™ï¼Œr|sï¼Œa)ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å°†å¼€åœºç™½å†™æˆï¼šP({CLEVISHï¼ŒHIGH}ï¼Œ1|{CLEVISHï¼ŒHIGH}ï¼ŒCLEAN)=0.8ã€‚</p><p> By inspection of the expected rewards from each path, it is possible to guess that an optimal solution to this environment is likely to be in the form of the blue paths shown below.</p><p>é€šè¿‡æ£€æŸ¥æ¥è‡ªæ¯æ¡è·¯å¾„çš„é¢„æœŸå›æŠ¥ï¼Œå¯ä»¥çŒœæµ‹è¯¥ç¯å¢ƒçš„æœ€ä½³è§£å†³æ–¹æ¡ˆå¯èƒ½æ˜¯ä¸‹é¢æ‰€ç¤ºçš„è“è‰²è·¯å¾„çš„å½¢å¼ã€‚</p><p>  However, the agent has no information about the environmentâ€™s probabilities prior to interacting with the environment. So how can we train an agent to operate on this environment? In other words: How do we find a  policy (represented as  Ï€) that the agent can follow (that maps a state to an action) such that it maximizes total reward?</p><p>ä½†æ˜¯ï¼Œåœ¨ä¸ç¯å¢ƒäº¤äº’ä¹‹å‰ï¼Œä»£ç†æ²¡æœ‰å…³äºç¯å¢ƒæ¦‚ç‡çš„ä¿¡æ¯ã€‚é‚£ä¹ˆï¼Œæˆ‘ä»¬å¦‚ä½•æ‰èƒ½åŸ¹è®­ä¸€ååº§å¸­åœ¨è¿™ç§ç¯å¢ƒä¸‹æ“ä½œå‘¢ï¼Ÿæ¢å¥è¯è¯´ï¼šæˆ‘ä»¬å¦‚ä½•æ‰¾åˆ°ä»£ç†å¯ä»¥éµå¾ªçš„ç­–ç•¥(è¡¨ç¤ºä¸ºÏ€)(å°†çŠ¶æ€æ˜ å°„åˆ°æ“ä½œ)ï¼Œä»è€Œæœ€å¤§åŒ–æ€»å›æŠ¥ï¼Ÿ</p><p>  What if we start with an agent that simply chooses actions at random? Below is an example of the results from such an agent running over an episode consisting of 50 steps (the first 10 of which are shown below):</p><p>å¦‚æœæˆ‘ä»¬ä»ä¸€ä¸ªç®€å•åœ°éšæœºé€‰æ‹©æ“ä½œçš„ä»£ç†å¼€å§‹ä¼šæ€ä¹ˆæ ·ï¼Ÿä¸‹é¢æ˜¯è¿™æ ·ä¸€ä¸ªä»£ç†åœ¨åŒ…å«50ä¸ªæ­¥éª¤(å…¶ä¸­å‰10ä¸ªæ­¥éª¤å¦‚ä¸‹æ‰€ç¤º)çš„æ’æ›²ä¸­è¿è¡Œçš„ç»“æœç¤ºä¾‹ï¼š</p><p> S0: Docked, High A0: Wait R1: 0 S1: Docked, High A1: Wait R2: 0 S2: Docked, High A2: Wait R3: 0 S4: Docked, High A4: Wait R5: 0 S5: Docked, High A5: Clean R6: 1 S6: Cleaning, Low A6: Clean R7: -3 S7: Docked, Low A7: Clean R8: -3 S8: Docked, Low A8: Clean R9: -3 S9: Docked, Low ...</p><p>S0ï¼šåœé ï¼Œé«˜A0ï¼šç­‰å¾…R1ï¼š0 S1ï¼šåœé ï¼Œé«˜A1ï¼šç­‰å¾…R2ï¼š0 S2ï¼šåœé ï¼Œé«˜A2ï¼šç­‰å¾…R3ï¼š0 S4ï¼šåœé ï¼Œé«˜A4ï¼šç­‰å¾…R5ï¼š0 S5ï¼šåœé ï¼Œé«˜A5ï¼šæ¸…æ´R6ï¼š1 S6ï¼šæ¸…æ´ï¼Œä½A6ï¼šæ¸…æ´R7ï¼š-3 S7ï¼šåœé ï¼Œä½A7ï¼šæ¸…æ´R8ï¼š-3 S8ï¼šåœé ï¼Œä½A8ï¼šæ¸…æ´R9ï¼š-3 S9ï¼šåœé ï¼Œä½...ã€‚</p><p> How can we make use of the information we have just collected? To start, we can represent this data using a table. Where the rows will correspond to the states and the columns to actions. The entries would be the weighted-average expected return from taking the given action while in the given state under the current operating policy. With some notation:   Q(s,aâ€‹)â† Q(sâ€‹,aâ€‹)+ Î±( Gâ€‹âˆ’ Q(sâ€‹,aâ€‹)). Where Î± is our weighting factor for the average and is often called the  learning rate since a large alpha means the network learns faster as recent observations have a greater impact. The table below is termed a  Q-table.</p><p>æˆ‘ä»¬æ€æ ·æ‰èƒ½åˆ©ç”¨æˆ‘ä»¬åˆšåˆšæ”¶é›†åˆ°çš„ä¿¡æ¯å‘¢ï¼Ÿé¦–å…ˆï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¡¨æ ¼è¡¨ç¤ºæ­¤æ•°æ®ã€‚å…¶ä¸­è¡Œå°†å¯¹åº”äºçŠ¶æ€ï¼Œåˆ—å°†å¯¹åº”äºåŠ¨ä½œã€‚è¿™äº›æ¡ç›®å°†æ˜¯åœ¨å½“å‰æ“ä½œç­–ç•¥ä¸‹å¤„äºç»™å®šçŠ¶æ€æ—¶é‡‡å–ç»™å®šè¡ŒåŠ¨çš„åŠ æƒå¹³å‡é¢„æœŸå›æŠ¥ã€‚ç”¨æŸç§ç¬¦å·è¡¨ç¤ºï¼šQ(sï¼Œaâ€‹)â†Q(sâ€‹ï¼Œaâ€‹)+Î±(Gâ€‹âˆ’Q(sâ€‹ï¼Œaâ€‹)ã€‚å…¶ä¸­ï¼ŒÎ±æ˜¯æˆ‘ä»¬å¯¹å¹³å‡å€¼çš„åŠ æƒå› å­ï¼Œé€šå¸¸è¢«ç§°ä¸ºå­¦ä¹ ç‡ï¼Œå› ä¸ºé˜¿å°”æ³•è¶Šå¤§æ„å‘³ç€ç½‘ç»œå­¦ä¹ çš„é€Ÿåº¦è¶Šå¿«ï¼Œå› ä¸ºæœ€è¿‘çš„è§‚å¯Ÿç»“æœæœ‰æ›´å¤§çš„å½±å“ã€‚ä¸‹é¢çš„è¡¨æ ¼ç§°ä¸ºQè¡¨ã€‚</p><p>  If we repeat this process for enough episodes we would expect our table to tend towards the actual returns. However, for sufficiently large problems this random exploration can take too long to converge. What if we use the information already in the table to focus our exploration? Instead of always picking random actions we pick the actions that would maximize our return (termed  greedy actions) based on the current values in the table. Under this proposed method, when running episode 2 if the agent were in the cleaning-high state it would always opt to dock. We know from the suggested solution in the previous section that this is likely not an optimal solution. It suggests, that this new approach may be susceptible to local minimas. What if we start with random exploration but over time (as more data is collected and our Q-table tends towards better approximations of the expected returns) we become more greedy?</p><p>å¦‚æœæˆ‘ä»¬é‡å¤è¿™ä¸ªè¿‡ç¨‹è¶³å¤Ÿå¤šçš„å‰§é›†ï¼Œæˆ‘ä»¬é¢„è®¡æˆ‘ä»¬çš„è¡¨æ ¼ä¼šè¶‹å‘äºå®é™…çš„å›æŠ¥ã€‚ç„¶è€Œï¼Œå¯¹äºè¶³å¤Ÿå¤§çš„é—®é¢˜ï¼Œè¿™ç§éšæœºæ¢ç´¢å¯èƒ½éœ€è¦å¤ªé•¿æ—¶é—´æ‰èƒ½æ”¶æ•›ã€‚å¦‚æœæˆ‘ä»¬ä½¿ç”¨è¡¨ä¸­å·²æœ‰çš„ä¿¡æ¯æ¥é›†ä¸­æˆ‘ä»¬çš„æ¢ç´¢ï¼Œä¼šæ€ä¹ˆæ ·ï¼Ÿæˆ‘ä»¬å¹¶ä¸æ€»æ˜¯é€‰æ‹©éšæœºçš„æ“ä½œï¼Œè€Œæ˜¯æ ¹æ®è¡¨ä¸­çš„å½“å‰å€¼é€‰æ‹©å¯ä»¥æœ€å¤§åŒ–å›æŠ¥çš„æ“ä½œ(ç§°ä¸ºè´ªå©ªæ“ä½œ)ã€‚åœ¨è¿™ç§å»ºè®®çš„æ–¹æ³•ä¸‹ï¼Œå½“è¿è¡Œç¬¬2é›†æ—¶ï¼Œå¦‚æœä»£ç†å¤„äºæ¸…ç†é«˜çŠ¶æ€ï¼Œå®ƒå°†å§‹ç»ˆé€‰æ‹©åœé ã€‚æˆ‘ä»¬ä»ä¸Šä¸€èŠ‚å»ºè®®çš„è§£å†³æ–¹æ¡ˆä¸­äº†è§£åˆ°ï¼Œè¿™å¯èƒ½ä¸æ˜¯æœ€ä½³è§£å†³æ–¹æ¡ˆã€‚è¿™è¡¨æ˜ï¼Œè¿™ç§æ–°çš„æ–¹æ³•å¯èƒ½å®¹æ˜“å—åˆ°å±€éƒ¨æå°å€¼çš„å½±å“ã€‚å¦‚æœæˆ‘ä»¬ä»éšæœºæ¢ç´¢å¼€å§‹ï¼Œä½†éšç€æ—¶é—´çš„æ¨ç§»(éšç€æ”¶é›†çš„æ•°æ®è¶Šæ¥è¶Šå¤šï¼Œæˆ‘ä»¬çš„Qè¡¨è¶‹å‘äºæ›´å¥½åœ°é€¼è¿‘é¢„æœŸæ”¶ç›Š)ï¼Œæˆ‘ä»¬ä¼šå˜å¾—æ›´åŠ è´ªå©ªå—ï¼Ÿ</p><p> This final idea is the concept behind the  ğœ€-greedy algorithm where we pick the action in the table that maximizes our expected return with  1 â€” ğœ€ + ğœ€/n and the values that donâ€™t maximize our return with  ğœ€/n probability. We want to explore more in the beginning (when our Q-table is unlikely to be a good representation of the rewards) and narrow our exploration as our approximations improves with a larger number of episodes. A method to achieve this is to have an ğœ€ that decays (becomes smaller) with increasing number of episodes but is limited by some bound so that our algorithm is never fully greedy such as  ğœ€ â† min(ğœ€ * ğœ€_decay^num_episodes, ğœ€_min_bound). This notion is often referred to as  greedy in the limit with infinite exploration  (GLIE). Using this algorithm and running for 1000 episodes we end up with the Q table below.</p><p>æœ€åä¸€ä¸ªæƒ³æ³•æ˜¯ğœ€-è´ªå©ªç®—æ³•èƒŒåçš„æ¦‚å¿µï¼Œæˆ‘ä»¬åœ¨è¡¨ä¸­é€‰æ‹©ä»¥1-ğœ€+ğœ€/næœ€å¤§åŒ–é¢„æœŸå›æŠ¥çš„æ“ä½œï¼Œä»¥åŠä¸ä»¥ğœ€/næ¦‚ç‡æœ€å¤§åŒ–å›æŠ¥çš„å€¼ã€‚æˆ‘ä»¬å¸Œæœ›åœ¨å¼€å§‹æ—¶æ¢ç´¢æ›´å¤š(å½“æˆ‘ä»¬çš„Qè¡¨ä¸å¤ªå¯èƒ½å¾ˆå¥½åœ°è¡¨ç¤ºå›æŠ¥æ—¶)ï¼Œå¹¶éšç€æˆ‘ä»¬çš„è¿‘ä¼¼éšç€å‰§é›†æ•°é‡çš„å¢åŠ è€Œæ”¹è¿›ï¼Œç¼©å°æˆ‘ä»¬çš„æ¢ç´¢èŒƒå›´ã€‚å®ç°è¿™ä¸€ç‚¹çš„ä¸€ç§æ–¹æ³•æ˜¯ä½¿ğœ€éšç€å‰§é›†æ•°é‡çš„å¢åŠ è€Œè¡°å‡(å˜å¾—æ›´å°)ï¼Œä½†å—åˆ°æŸäº›ç•Œé™çš„é™åˆ¶ï¼Œä»¥ä¾¿æˆ‘ä»¬çš„ç®—æ³•æ°¸è¿œä¸ä¼šå®Œå…¨è´ªå©ªï¼Œä¾‹å¦‚ğœ€â†MIN(ğœ€*ğœ€_Decen^num_epsidesï¼Œğœ€_MIN_Bound)ã€‚è¿™ä¸ªæ¦‚å¿µé€šå¸¸è¢«ç§°ä¸ºæ— é™æ¢ç´¢çš„è´ªå©ª(Glie)ã€‚ä½¿ç”¨è¿™ä¸ªç®—æ³•ï¼Œè¿è¡Œ1000é›†ï¼Œæˆ‘ä»¬æœ€ç»ˆå¾—åˆ°äº†ä¸‹é¢çš„Qè¡¨ã€‚</p><p>  Observe how choosing greedy actions based on this table is our optimal (blue) policy as shown earlier in the MDP figure.</p><p>è§‚å¯ŸåŸºäºæ­¤è¡¨é€‰æ‹©è´ªå©ªæ“ä½œå¦‚ä½•æˆä¸ºæˆ‘ä»¬çš„æœ€ä½³(è“è‰²)ç­–ç•¥ï¼Œå¦‚å‰é¢çš„MDPå›¾æ‰€ç¤ºã€‚</p><p>  We have derived a simple reinforcement learning algorithm based on Monte Carlo Control. However, what if the problem we are attempting to solve is not episodic but instead a continuing task (one that has no clear end)? In the next article we will explore Temporal-Difference (TD) Methods that are useful for this class of problems.</p><p>æˆ‘ä»¬æ¨å¯¼äº†ä¸€ç§ç®€å•çš„åŸºäºè’™ç‰¹å¡ç½—æ§åˆ¶çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ã€‚ç„¶è€Œï¼Œå¦‚æœæˆ‘ä»¬è¯•å›¾è§£å†³çš„é—®é¢˜ä¸æ˜¯é—´æ­‡æ€§çš„ï¼Œè€Œæ˜¯ä¸€ä¸ªæŒç»­æ€§çš„ä»»åŠ¡(ä¸€ä¸ªæ²¡æœ‰æ˜ç¡®ç»“æŸçš„ä»»åŠ¡)ï¼Œè¯¥æ€ä¹ˆåŠï¼Ÿåœ¨ä¸‹ä¸€ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ¢ç´¢å¯¹è¿™ç±»é—®é¢˜æœ‰ç”¨çš„æ—¶å·®(TD)æ–¹æ³•ã€‚</p><p>  Udacityâ€™s course in deep reinforcement learning provides a solid introduction to reinforcement learning.</p><p>Udacityçš„æ·±åº¦å¼ºåŒ–å­¦ä¹ è¯¾ç¨‹ä¸ºå¼ºåŒ–å­¦ä¹ æä¾›äº†åšå®çš„ä»‹ç»ã€‚</p><p> The scripts used to generate the Q table in the example can be found  here.</p><p>å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°ç¤ºä¾‹ä¸­ç”¨äºç”ŸæˆQè¡¨çš„è„šæœ¬ã€‚</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://medium.com/swlh/shallow-reinforcement-learning-3e8b59ff66c7">https://medium.com/swlh/shallow-reinforcement-learning-3e8b59ff66c7</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/å¼ºåŒ–/">#å¼ºåŒ–</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/çŠ¶æ€/">#çŠ¶æ€</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è®¾è®¡/">#è®¾è®¡</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/æ¸¸æˆ/">#æ¸¸æˆ</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/åˆ›æ„/">#åˆ›æ„</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è‹¹æœ/">#è‹¹æœ</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/æ‘„å½±/">#æ‘„å½±</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è½¯ä»¶/">#è½¯ä»¶</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/ç¾å›½/">#ç¾å›½</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/å›¾ç‰‡/">#å›¾ç‰‡</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/æ‰‹æœº/">#æ‰‹æœº</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è§†é¢‘/">#è§†é¢‘</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/å¹¿å‘Š/">#å¹¿å‘Š</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/å…è´¹/">#å…è´¹</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è°·æ­Œ/">#è°·æ­Œ</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/ç½‘ç«™/">#ç½‘ç«™</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/å¾®è½¯/">#å¾®è½¯</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/ä¸‹è½½/">#ä¸‹è½½</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/éŸ³ä¹/">#éŸ³ä¹</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/linux/">#linux</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/åšå®¢/">#åšå®¢</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/ç¨‹åº/">#ç¨‹åº</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è‰ºæœ¯/">#è‰ºæœ¯</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>