<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>ç†è§£åˆ†å¸ƒå¼å…±è¯†(2018)</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">ç†è§£åˆ†å¸ƒå¼å…±è¯†(2018)</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2020-11-07 18:11:57</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2020/11/da4972a06ab6e39fef37b4cc5fb1841e.jpg"><img src="http://img2.diglog.com/img/2020/11/da4972a06ab6e39fef37b4cc5fb1841e.jpg" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>Distributed systems can be difficult to understand, mainly because the knowledge surrounding them  is distributed. But donâ€™t worry, Iâ€™m well aware of the irony. While teaching myself distributed computing, I fell flat on my face many times. Now, after many trials and tribulations, Iâ€™m finally ready to explain the basics of distributed systems to you.</p><p>åˆ†å¸ƒå¼ç³»ç»Ÿå¯èƒ½å¾ˆéš¾ç†è§£ï¼Œä¸»è¦æ˜¯å› ä¸ºå®ƒä»¬å‘¨å›´çš„çŸ¥è¯†æ˜¯åˆ†å¸ƒå¼çš„ã€‚ä½†åˆ«æ‹…å¿ƒï¼Œæˆ‘å¾ˆæ¸…æ¥šå…¶ä¸­çš„è®½åˆºæ„å‘³ã€‚åœ¨è‡ªå­¦åˆ†å¸ƒå¼è®¡ç®—æ—¶ï¼Œæˆ‘å¤šæ¬¡æ‘”å€’åœ¨åœ°ã€‚ç°åœ¨ï¼Œåœ¨ç»å†äº†è®¸å¤šè€ƒéªŒå’Œç£¨éš¾ä¹‹åï¼Œæˆ‘ç»ˆäºå‡†å¤‡å¥½å‘ä½ ä»¬è§£é‡Šåˆ†å¸ƒå¼ç³»ç»Ÿçš„åŸºç¡€çŸ¥è¯†äº†ã€‚</p><p> Blockchains have forced engineers and scientists to re-examine and question firmly entrenched paradigms in distributed computing.</p><p>åŒºå—é“¾è¿«ä½¿å·¥ç¨‹å¸ˆå’Œç§‘å­¦å®¶é‡æ–°å®¡è§†å’Œè´¨ç–‘åˆ†å¸ƒå¼è®¡ç®—ä¸­æ ¹æ·±è’‚å›ºçš„èŒƒå¼ã€‚</p><p> I also want to discuss the profound effect that blockchain technology has had on the field. Blockchains have forced engineers and scientists to re-examine and question firmly entrenched paradigms in distributed computing. Perhaps no other technology has catalyzed progress faster in this area of study than blockchain.</p><p>æˆ‘è¿˜æƒ³è®¨è®ºä¸€ä¸‹åŒºå—é“¾æŠ€æœ¯å¯¹è¯¥é¢†åŸŸäº§ç”Ÿçš„æ·±åˆ»å½±å“ã€‚åŒºå—é“¾è¿«ä½¿å·¥ç¨‹å¸ˆå’Œç§‘å­¦å®¶é‡æ–°å®¡è§†å’Œè´¨ç–‘åˆ†å¸ƒå¼è®¡ç®—ä¸­æ ¹æ·±è’‚å›ºçš„èŒƒå¼ã€‚ä¹Ÿè®¸æ²¡æœ‰å…¶ä»–æŠ€æœ¯æ¯”åŒºå—é“¾æ›´å¿«åœ°æ¨åŠ¨äº†è¿™ä¸€ç ”ç©¶é¢†åŸŸçš„è¿›æ­¥ã€‚</p><p> Distributed systems are by no means new. Scientists and engineers have spent decades researching the subject. But what does blockchain have to do with them? Well, all the contributions that blockchain has made wouldnâ€™t have been possible if distributed systems hadnâ€™t existed first.</p><p>åˆ†å¸ƒå¼ç³»ç»Ÿç»ä¸æ˜¯ä»€ä¹ˆæ–°é²œäº‹ã€‚ç§‘å­¦å®¶å’Œå·¥ç¨‹å¸ˆèŠ±äº†å‡ åå¹´çš„æ—¶é—´æ¥ç ”ç©¶è¿™ä¸ªè¯¾é¢˜ã€‚ä½†æ˜¯åŒºå—é“¾å’Œä»–ä»¬æœ‰ä»€ä¹ˆå…³ç³»å‘¢ï¼Ÿå—¯ï¼Œå¦‚æœæ²¡æœ‰åˆ†å¸ƒå¼ç³»ç»Ÿï¼ŒåŒºå—é“¾åšå‡ºçš„æ‰€æœ‰è´¡çŒ®éƒ½æ˜¯ä¸å¯èƒ½çš„ã€‚</p><p> Essentially, a blockchain is a new type of distributed system. It started with the advent of  Bitcoin and has since made a lasting impact in the field of distributed computing. So, if you want to really know how blockchains work, a great grasp of the principles of distributed systems is essential.</p><p>ä»æœ¬è´¨ä¸Šè®²ï¼ŒåŒºå—é“¾æ˜¯ä¸€ç§æ–°å‹çš„åˆ†å¸ƒå¼ç³»ç»Ÿã€‚å®ƒå§‹äºæ¯”ç‰¹å¸çš„å‡ºç°ï¼Œæ­¤ååœ¨åˆ†å¸ƒå¼è®¡ç®—é¢†åŸŸäº§ç”Ÿäº†æŒä¹…çš„å½±å“ã€‚å› æ­¤ï¼Œå¦‚æœä½ æƒ³çœŸæ­£äº†è§£åŒºå—é“¾æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Œå¯¹åˆ†å¸ƒå¼ç³»ç»ŸåŸç†çš„æ·±åˆ»æŠŠæ¡æ˜¯å¿…ä¸å¯å°‘çš„ã€‚</p><p> Unfortunately, much of the literature on distributed computing is either difficult to comprehend or dispersed across way too many academic papers. To make matters more complex, there are hundreds of architectures, all of which serve different needs. Boiling this down into a simple-to-understand framework is quite difficult.</p><p>ä¸å¹¸çš„æ˜¯ï¼Œè®¸å¤šå…³äºåˆ†å¸ƒå¼è®¡ç®—çš„æ–‡çŒ®è¦ä¹ˆéš¾ä»¥ç†è§£ï¼Œè¦ä¹ˆåˆ†æ•£åœ¨å¤ªå¤šçš„å­¦æœ¯è®ºæ–‡ä¸­ã€‚æ›´å¤æ‚çš„æ˜¯ï¼Œæœ‰æ•°ç™¾ç§æ¶æ„ï¼Œæ‰€æœ‰è¿™äº›æ¶æ„éƒ½æœåŠ¡äºä¸åŒçš„éœ€æ±‚ã€‚å°†å…¶å½’ç»“ä¸ºä¸€ä¸ªç®€å•æ˜“æ‡‚çš„æ¡†æ¶æ˜¯ç›¸å½“å›°éš¾çš„ã€‚</p><p> Because the field is vast, I had to carefully choose what I could cover. I also had to make generalizations to mask some of the complexity. Please note, my goal is not to make you an expert in the field. Instead, I want to give you enough knowledge to jump-start your journey into distributed systems and consensus.</p><p>å› ä¸ºè¿™ä¸ªé¢†åŸŸå¾ˆå¹¿é˜”ï¼Œæˆ‘ä¸å¾—ä¸è°¨æ…åœ°é€‰æ‹©æˆ‘èƒ½è¦†ç›–çš„é¢†åŸŸã€‚æˆ‘è¿˜ä¸å¾—ä¸åšä¸€äº›æ¦‚æ‹¬ï¼Œä»¥æ©ç›–ä¸€äº›å¤æ‚æ€§ã€‚è¯·æ³¨æ„ï¼Œæˆ‘çš„ç›®æ ‡ä¸æ˜¯è®©ä½ æˆä¸ºè¯¥é¢†åŸŸçš„ä¸“å®¶ã€‚ç›¸åï¼Œæˆ‘æƒ³ç»™ä½ è¶³å¤Ÿçš„çŸ¥è¯†æ¥å¯åŠ¨ä½ çš„åˆ†å¸ƒå¼ç³»ç»Ÿå’Œå…±è¯†ä¹‹æ—…ã€‚</p><p>     A distributed system involves a set of distinct processes (e.g., computers) passing messages to one another and coordinating to accomplish a common objective (i.e., solving a computational problem).</p><p>åˆ†å¸ƒå¼ç³»ç»Ÿæ¶‰åŠä¸€ç»„ä¸åŒçš„è¿›ç¨‹(ä¾‹å¦‚ï¼Œè®¡ç®—æœº)å½¼æ­¤ä¼ é€’æ¶ˆæ¯å¹¶åè°ƒä»¥å®ç°å…±åŒç›®æ ‡(å³ï¼Œè§£å†³è®¡ç®—é—®é¢˜)ã€‚</p><p> A distributed system is a group of computers working together to achieve a unified goal.</p><p>åˆ†å¸ƒå¼ç³»ç»Ÿæ˜¯ä¸€ç»„ååŒå·¥ä½œä»¥å®ç°ç»Ÿä¸€ç›®æ ‡çš„è®¡ç®—æœºã€‚</p><p> Simply put, a distributed system is a group of computers working together to achieve a unified goal. And although the processes are separate, the system appears as a single computer to end-user(s).</p><p>ç®€å•åœ°è¯´ï¼Œåˆ†å¸ƒå¼ç³»ç»Ÿå°±æ˜¯ä¸€ç»„è®¡ç®—æœºä¸ºäº†å®ç°ç»Ÿä¸€çš„ç›®æ ‡è€ŒååŒå·¥ä½œã€‚è™½ç„¶è¿™ä¸¤ä¸ªè¿‡ç¨‹æ˜¯åˆ†å¼€çš„ï¼Œä½†å¯¹äºæœ€ç»ˆç”¨æˆ·æ¥è¯´ï¼Œç³»ç»Ÿçœ‹èµ·æ¥å°±åƒä¸€å°è®¡ç®—æœºã€‚</p><p> As I mentioned, there are hundreds of architectures for a distributed system. For example, a single computer can also be viewed as a distributed system: the central control unit, memory units, and input-output channels are separate processes collaborating to complete an objective.</p><p>æ­£å¦‚æˆ‘æåˆ°çš„ï¼Œåˆ†å¸ƒå¼ç³»ç»Ÿæœ‰æ•°ç™¾ç§æ¶æ„ã€‚ä¾‹å¦‚ï¼Œä¸€å°è®¡ç®—æœºä¹Ÿå¯ä»¥çœ‹ä½œæ˜¯ä¸€ä¸ªåˆ†å¸ƒå¼ç³»ç»Ÿï¼šä¸­å¤®æ§åˆ¶å•å…ƒã€å­˜å‚¨å•å…ƒå’Œè¾“å…¥è¾“å‡ºé€šé“æ˜¯ç›¸äº’åä½œä»¥å®ŒæˆæŸä¸€ç›®æ ‡çš„ç‹¬ç«‹è¿›ç¨‹ã€‚</p><p> In the case of an airplane, these discrete units work together to get you from Point A to Point B:</p><p>åœ¨é£æœºçš„æƒ…å†µä¸‹ï¼Œè¿™äº›ç‹¬ç«‹çš„å•å…ƒååŒå·¥ä½œï¼Œå°†æ‚¨ä»Aç‚¹å¸¦åˆ°Bç‚¹ï¼š</p><p>  In this post, weâ€™ll focus on distributed systems in which processes are spatially-separated computers.</p><p>åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†é‡ç‚¹ä»‹ç»åˆ†å¸ƒå¼ç³»ç»Ÿï¼Œåœ¨åˆ†å¸ƒå¼ç³»ç»Ÿä¸­ï¼Œè¿›ç¨‹æ˜¯ç©ºé—´ä¸Šåˆ†ç¦»çš„è®¡ç®—æœºã€‚</p><p>  Note: I may use the terms â€œnode,â€ â€œpeer,â€ â€œcomputer,â€ or â€œcomponentâ€ interchangeably with â€œprocess.â€ They all mean the same thing for the purposes of this post. Similarly, I may use the term â€œnetworkâ€ interchangeably with â€œsystem.â€</p><p>æ³¨æ„ï¼šæˆ‘å¯ä»¥å°†æœ¯è¯­â€œèŠ‚ç‚¹â€ã€â€œå¯¹ç­‰â€ã€â€œè®¡ç®—æœºâ€æˆ–â€œç»„ä»¶â€ä¸â€œè¿›ç¨‹â€äº’æ¢ä½¿ç”¨ã€‚å¯¹äºè¿™ç¯‡æ–‡ç« æ¥è¯´ï¼Œå®ƒä»¬çš„æ„æ€éƒ½æ˜¯ä¸€æ ·çš„ã€‚åŒæ ·ï¼Œæˆ‘å¯ä»¥å°†æœ¯è¯­â€œç½‘ç»œâ€ä¸â€œç³»ç»Ÿâ€äº’æ¢ä½¿ç”¨ã€‚</p><p>    The processes in the system operate concurrently, meaning multiple events occur simultaneously. In other words, each computer in the network executes events independently at the same time as other computers in the network.</p><p>ç³»ç»Ÿä¸­çš„è¿›ç¨‹åŒæ—¶è¿è¡Œï¼Œè¿™æ„å‘³ç€å¤šä¸ªäº‹ä»¶åŒæ—¶å‘ç”Ÿã€‚æ¢å¥è¯è¯´ï¼Œç½‘ç»œä¸­çš„æ¯å°è®¡ç®—æœºä¸ç½‘ç»œä¸­çš„å…¶ä»–è®¡ç®—æœºåŒæ—¶ç‹¬ç«‹åœ°æ‰§è¡Œäº‹ä»¶ã€‚</p><p>    For a distributed system to work, we need a way to determine the order of events. However, in a set of computers operating concurrently, it is sometimes impossible to say that one of two events occurred first, as computers are spatially separated. In other words, there is no single global clock that determines the sequence of events happening across all computers in the network.</p><p>è¦è®©åˆ†å¸ƒå¼ç³»ç»Ÿæ­£å¸¸å·¥ä½œï¼Œæˆ‘ä»¬éœ€è¦ä¸€ç§æ–¹æ³•æ¥ç¡®å®šäº‹ä»¶çš„é¡ºåºã€‚ç„¶è€Œï¼Œåœ¨åŒæ—¶è¿è¡Œçš„ä¸€ç»„è®¡ç®—æœºä¸­ï¼Œæœ‰æ—¶ä¸å¯èƒ½è¯´ä¸¤ä¸ªäº‹ä»¶ä¸­çš„ä¸€ä¸ªé¦–å…ˆå‘ç”Ÿï¼Œå› ä¸ºè®¡ç®—æœºåœ¨ç©ºé—´ä¸Šæ˜¯åˆ†å¼€çš„ã€‚æ¢å¥è¯è¯´ï¼Œä¸å­˜åœ¨å•ä¸€çš„å…¨å±€æ—¶é’Ÿæ¥å†³å®šç½‘ç»œä¸­æ‰€æœ‰è®¡ç®—æœºä¸Šå‘ç”Ÿçš„äº‹ä»¶çš„é¡ºåºã€‚</p><p> In the paper â€œ Time, Clocks and Ordering of Events in a Distributed System,â€ Leslie Lamport shows how we can deduce whether one event happens before another by remembering the following factors:</p><p>åœ¨ã€Šåˆ†å¸ƒå¼ç³»ç»Ÿä¸­çš„æ—¶é—´ã€æ—¶é’Ÿå’Œäº‹ä»¶é¡ºåºã€‹ä¸€æ–‡ä¸­ï¼ŒLeslie Lamportå±•ç¤ºäº†æˆ‘ä»¬å¦‚ä½•é€šè¿‡è®°ä½ä»¥ä¸‹å› ç´ æ¥æ¨æ–­ä¸€ä¸ªäº‹ä»¶æ˜¯å¦å‘ç”Ÿåœ¨å¦ä¸€ä¸ªäº‹ä»¶ä¹‹å‰ï¼š</p><p>  By determining which event happens before another, we can get a  partial ordering of events in the system. Lamportâ€™s paper describes an algorithm which requires each computer to hear from every other computer in the system. In this way, events can be  totally ordered based on this partial ordering.</p><p>é€šè¿‡ç¡®å®šå“ªä¸ªäº‹ä»¶åœ¨å¦ä¸€ä¸ªäº‹ä»¶ä¹‹å‰å‘ç”Ÿï¼Œæˆ‘ä»¬å¯ä»¥è·å¾—ç³»ç»Ÿä¸­äº‹ä»¶çš„éƒ¨åˆ†æ’åºã€‚å…°æ³¢ç‰¹çš„è®ºæ–‡æè¿°äº†ä¸€ç§ç®—æ³•ï¼Œè¯¥ç®—æ³•è¦æ±‚æ¯å°è®¡ç®—æœºéƒ½èƒ½ç›‘å¬ç³»ç»Ÿä¸­æ‰€æœ‰å…¶ä»–è®¡ç®—æœºçš„ä¿¡æ¯ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¯ä»¥åŸºäºè¿™ç§éƒ¨åˆ†æ’åºå¯¹äº‹ä»¶è¿›è¡Œå®Œå…¨æ’åºã€‚</p><p> However, if we base the order entirely upon events heard by each individual computer, we can run into situations where this order differs from what a user external to the system perceives. Thus, the paper shows that the algorithm can still allow for anomalous behavior.</p><p>ä½†æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬å°†é¡ºåºå®Œå…¨åŸºäºæ¯å°è®¡ç®—æœºå¬åˆ°çš„äº‹ä»¶ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šé‡åˆ°è¿™æ ·çš„æƒ…å†µï¼šè¯¥é¡ºåºä¸ç³»ç»Ÿå¤–éƒ¨çš„ç”¨æˆ·æ‰€æ„ŸçŸ¥çš„ä¸åŒã€‚å› æ­¤ï¼Œæœ¬æ–‡è¡¨æ˜ï¼Œè¯¥ç®—æ³•ä»ç„¶å¯ä»¥å…è®¸å¼‚å¸¸è¡Œä¸ºã€‚</p><p> Finally, Lamport discusses how such anomalies can be prevented by using properly synchronized physical clocks.</p><p>æœ€åï¼Œå…°æ³¢ç‰¹è®¨è®ºäº†å¦‚ä½•é€šè¿‡ä½¿ç”¨é€‚å½“åŒæ­¥çš„ç‰©ç†æ—¶é’Ÿæ¥é˜²æ­¢æ­¤ç±»å¼‚å¸¸ã€‚</p><p> But waitâ€Šâ€”â€Šthereâ€™s a huge caveat: coordinating otherwise independent clocks is a very complex computer science problem. Even if you initially set a bunch of clocks accurately, the clocks will begin to differ after some amount of time. This is due to â€œ clock drift,â€ a phenomenon in which clocks count time at slightly different rates.</p><p>ä½†ç­‰ç­‰ï¼Œâ€Š-â€Šï¼Œæœ‰ä¸€ä¸ªå¾ˆå¤§çš„è­¦å‘Šï¼šåè°ƒåŸæœ¬ç‹¬ç«‹çš„æ—¶é’Ÿæ˜¯ä¸€ä¸ªéå¸¸å¤æ‚çš„è®¡ç®—æœºç§‘å­¦é—®é¢˜ã€‚å³ä½¿æ‚¨æœ€åˆç²¾ç¡®åœ°è®¾ç½®äº†ä¸€ç»„æ—¶é’Ÿï¼Œæ—¶é’Ÿåœ¨ä¸€æ®µæ—¶é—´åä¹Ÿä¼šå¼€å§‹ä¸åŒã€‚è¿™æ˜¯ç”±äºâ€œæ—¶é’Ÿæ¼‚ç§»â€ç°è±¡é€ æˆçš„ï¼Œå³æ—¶é’Ÿä»¥ç•¥æœ‰ä¸åŒçš„é€Ÿç‡è®¡æ—¶ã€‚</p><p> Essentially, Lamportâ€™s paper demonstrates that time and order of events are fundamental obstacles in a system of distributed computers that are spatially separated.</p><p>ä»æœ¬è´¨ä¸Šè®²ï¼Œå…°æ³¢ç‰¹çš„è®ºæ–‡è¯æ˜ï¼Œåœ¨ç©ºé—´ä¸Šåˆ†æ•£çš„åˆ†å¸ƒå¼è®¡ç®—æœºç³»ç»Ÿä¸­ï¼Œäº‹ä»¶çš„æ—¶é—´å’Œé¡ºåºæ˜¯æ ¹æœ¬çš„éšœç¢ã€‚</p><p>  A critical aspect of understanding distributed systems is acknowledging that components in a distributed system are faulty. This is why itâ€™s called â€œfault-tolerant distributed computing.â€</p><p>ç†è§£åˆ†å¸ƒå¼ç³»ç»Ÿçš„ä¸€ä¸ªå…³é”®æ–¹é¢æ˜¯æ‰¿è®¤åˆ†å¸ƒå¼ç³»ç»Ÿä¸­çš„ç»„ä»¶æœ‰æ•…éšœã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆå®ƒè¢«ç§°ä¸ºâ€œå®¹é”™åˆ†å¸ƒå¼è®¡ç®—â€ã€‚</p><p> Itâ€™s impossible to have a system free of faults. Real systems are subject to a number of possible flaws or defects, whether thatâ€™s a process crashing; messages being lost, distorted, or duplicated; a network partition delaying or dropping messages; or even a process going completely haywire and sending messages according to some malevolent plan.</p><p>ä¸€ä¸ªæ²¡æœ‰æ•…éšœçš„ç³»ç»Ÿæ˜¯ä¸å¯èƒ½çš„ã€‚çœŸæ­£çš„ç³»ç»Ÿå¯èƒ½å­˜åœ¨è®¸å¤šç¼ºé™·æˆ–ç¼ºé™·ï¼Œæ— è®ºæ˜¯è¿›ç¨‹å´©æºƒï¼›æ¶ˆæ¯ä¸¢å¤±ã€æ‰­æ›²æˆ–é‡å¤ï¼›ç½‘ç»œåˆ†åŒºå»¶è¿Ÿæˆ–ä¸¢å¼ƒæ¶ˆæ¯ï¼›ç”šè‡³æ˜¯è¿›ç¨‹å®Œå…¨æ··ä¹±å¹¶æ ¹æ®æŸç§æ¶æ„è®¡åˆ’å‘é€æ¶ˆæ¯ã€‚</p><p>   Omission: The component sends a message but it is not received by the other nodes (e.g., the message was dropped).</p><p>çœç•¥ï¼šè¯¥ç»„ä»¶å‘é€ä¸€æ¡æ¶ˆæ¯ï¼Œä½†å…¶ä»–èŠ‚ç‚¹æ²¡æœ‰æ¥æ”¶åˆ°è¯¥æ¶ˆæ¯(ä¾‹å¦‚ï¼Œè¯¥æ¶ˆæ¯è¢«ä¸¢å¼ƒ)ã€‚</p><p> Byzantine: The component behaves arbitrarily. This type of fault is irrelevant in controlled environments (e.g., Google or Amazon data centers) where there is presumably no malicious behavior. Instead, these faults occur in whatâ€™s known as an â€œadversarial context.â€ Basically, when a decentralized set of independent actors serve as nodes in the network, these actors may choose to act in a â€œByzantineâ€ manner. This means they maliciously choose to alter, block, or not send messages at all.</p><p>æ‹œå åº­ï¼šç»„ä»¶çš„è¡Œä¸ºæ˜¯ä»»æ„çš„ã€‚è¿™ç§ç±»å‹çš„æ•…éšœåœ¨å¯èƒ½æ²¡æœ‰æ¶æ„è¡Œä¸ºçš„å—æ§ç¯å¢ƒ(ä¾‹å¦‚Googleæˆ–Amazonæ•°æ®ä¸­å¿ƒ)ä¸­æ˜¯æ— å…³ç´§è¦çš„ã€‚ç›¸åï¼Œè¿™äº›é”™è¯¯å‘ç”Ÿåœ¨æ‰€è°“çš„â€œå¯¹æŠ—æ€§ç¯å¢ƒâ€ä¸­ã€‚åŸºæœ¬ä¸Šï¼Œå½“ä¸€ç»„åˆ†æ•£çš„ç‹¬ç«‹è¡Œä¸ºè€…å……å½“ç½‘ç»œä¸­çš„èŠ‚ç‚¹æ—¶ï¼Œè¿™äº›è¡Œä¸ºè€…å¯èƒ½ä¼šé€‰æ‹©ä»¥â€œæ‹œå åº­å¼â€çš„æ–¹å¼è¡Œäº‹ã€‚è¿™æ„å‘³ç€ä»–ä»¬æ¶æ„é€‰æ‹©æ›´æ”¹ã€é˜»æ­¢æˆ–æ ¹æœ¬ä¸å‘é€æ¶ˆæ¯ã€‚</p><p> With this in mind, the aim is to design protocols that allow a system with faulty components to still achieve the common goal and provide a useful service.</p><p>è€ƒè™‘åˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯è®¾è®¡åè®®ï¼Œä½¿å…·æœ‰æ•…éšœç»„ä»¶çš„ç³»ç»Ÿä»èƒ½å®ç°å…±åŒç›®æ ‡å¹¶æä¾›æœ‰ç”¨çš„æœåŠ¡ã€‚</p><p> Given that every system has faults, a core consideration we must make when building a distributed system is whether it can survive even when its parts deviate from normal behavior, whether thatâ€™s due to non-malicious behaviors (i.e., crash-fail or omission faults) or malicious behavior (i.e., Byzantine faults).</p><p>é‰´äºæ¯ä¸ªç³»ç»Ÿéƒ½æœ‰æ•…éšœï¼Œåœ¨æ„å»ºåˆ†å¸ƒå¼ç³»ç»Ÿæ—¶ï¼Œæˆ‘ä»¬å¿…é¡»è€ƒè™‘çš„ä¸€ä¸ªæ ¸å¿ƒé—®é¢˜æ˜¯ï¼Œæ— è®ºæ˜¯ç”±äºéæ¶æ„è¡Œä¸º(å³å´©æºƒå¤±è´¥æˆ–é—æ¼æ•…éšœ)è¿˜æ˜¯æ¶æ„è¡Œä¸º(å³æ‹œå åº­æ•…éšœ)ï¼Œåœ¨æ„å»ºåˆ†å¸ƒå¼ç³»ç»Ÿæ—¶ï¼Œå³ä½¿å…¶éƒ¨åˆ†åç¦»æ­£å¸¸è¡Œä¸ºï¼Œå®ƒæ˜¯å¦ä»èƒ½å­˜æ´»ã€‚</p><p> Broadly speaking, there are two types of models to consider when making a distributed system:</p><p>ä¸€èˆ¬è€Œè¨€ï¼Œåœ¨æ„å»ºåˆ†å¸ƒå¼ç³»ç»Ÿæ—¶æœ‰ä¸¤ç§ç±»å‹çš„æ¨¡å‹éœ€è¦è€ƒè™‘ï¼š</p><p>  In a simple fault-tolerant system, we assume that all parts of the system do one of two things: they either follow the protocol exactly or they fail. This type of system should definitely be able to handle nodes going offline or failing. But it doesnâ€™t have to worry about nodes exhibiting arbitrary or malicious behavior.</p><p>åœ¨ä¸€ä¸ªç®€å•çš„å®¹é”™ç³»ç»Ÿä¸­ï¼Œæˆ‘ä»¬å‡è®¾ç³»ç»Ÿçš„æ‰€æœ‰éƒ¨åˆ†æ‰§è¡Œä»¥ä¸‹ä¸¤ç§æ“ä½œä¹‹ä¸€ï¼šå®ƒä»¬è¦ä¹ˆå®Œå…¨éµå¾ªåè®®ï¼Œè¦ä¹ˆå¤±è´¥ã€‚è¿™ç§ç±»å‹çš„ç³»ç»Ÿåº”è¯¥ç»å¯¹èƒ½å¤Ÿå¤„ç†èŠ‚ç‚¹è„±æœºæˆ–æ•…éšœã€‚ä½†å®ƒä¸å¿…æ‹…å¿ƒèŠ‚ç‚¹è¡¨ç°å‡ºä»»æ„æˆ–æ¶æ„è¡Œä¸ºã€‚</p><p>  A simple fault-tolerant system is not very useful in an uncontrolled environment. In a decentralized system that has nodes controlled by independent actors communicating on the open, permissionless internet, we also need to design for nodes that choose to be malicious or â€œByzantine.â€ Therefore, in a Byzantine fault-tolerant system, we assume nodes can fail or be malicious.</p><p>ç®€å•çš„å®¹é”™ç³»ç»Ÿåœ¨ä¸å—æ§åˆ¶çš„ç¯å¢ƒä¸­ç”¨å¤„ä¸å¤§ã€‚åœ¨ä¸€ä¸ªåˆ†æ•£çš„ç³»ç»Ÿä¸­ï¼ŒèŠ‚ç‚¹ç”±åœ¨å¼€æ”¾ã€æœªç»è®¸å¯çš„äº’è”ç½‘ä¸Šé€šä¿¡çš„ç‹¬ç«‹å‚ä¸è€…æ§åˆ¶ï¼Œæˆ‘ä»¬è¿˜éœ€è¦ä¸ºé€‰æ‹©æ¶æ„æˆ–â€œæ‹œå åº­â€çš„èŠ‚ç‚¹è¿›è¡Œè®¾è®¡ã€‚å› æ­¤ï¼Œåœ¨æ‹œå åº­å®¹é”™ç³»ç»Ÿä¸­ï¼Œæˆ‘ä»¬å‡è®¾èŠ‚ç‚¹å¯èƒ½å‡ºç°æ•…éšœæˆ–æ¶æ„ã€‚</p><p>  Despite the fact that most real systems are designed to withstand Byzantine failures,  some experts argue that these designs are too general and donâ€™t take into account â€œrationalâ€ failures, wherein nodes can deviate if it is in their self-interest to do so. In other words, nodes can be both honest and dishonest, depending on incentives. If the incentives are high enough, then even the majority of nodes might act dishonestly.</p><p>å°½ç®¡å¤§å¤šæ•°çœŸå®ç³»ç»Ÿçš„è®¾è®¡éƒ½æ˜¯ä¸ºäº†æ‰¿å—æ‹œå åº­å¼çš„æ•…éšœï¼Œä½†ä¸€äº›ä¸“å®¶è®¤ä¸ºï¼Œè¿™äº›è®¾è®¡è¿‡äºç¬¼ç»Ÿï¼Œæ²¡æœ‰è€ƒè™‘åˆ°â€œç†æ€§â€æ•…éšœï¼Œå³èŠ‚ç‚¹å¦‚æœè¿™æ ·åšç¬¦åˆè‡ªèº«åˆ©ç›Šï¼Œå°±å¯èƒ½å‡ºç°åå·®ã€‚æ¢å¥è¯è¯´ï¼ŒèŠ‚ç‚¹å¯ä»¥æ˜¯è¯šå®çš„ï¼Œä¹Ÿå¯ä»¥æ˜¯ä¸è¯šå®çš„ï¼Œè¿™å–å†³äºæ¿€åŠ±ã€‚å¦‚æœæ¿€åŠ±è¶³å¤Ÿé«˜ï¼Œé‚£ä¹ˆå³ä½¿å¤§å¤šæ•°èŠ‚ç‚¹ä¹Ÿå¯èƒ½ä¼šåšå‡ºä¸è¯šå®çš„è¡Œä¸ºã€‚</p><p> More formally, this is defined as the BAR modelâ€Šâ€”â€Šone that specifies for both Byzantine and rational failures. The BAR model assumes three types of actors:</p><p>æ›´æ­£å¼åœ°è¯´ï¼Œè¿™è¢«å®šä¹‰ä¸ºé…’å§æ¨¡å‹â€Š-â€Šï¼Œå®ƒåŒæ—¶è§„å®šäº†æ‹œå åº­å¼å’Œç†æ€§å¼çš„å¤±è´¥ã€‚é…’å§æ¨¡å‹å‡å®šæœ‰ä¸‰ç§ç±»å‹çš„å‚ä¸è€…ï¼š</p><p>   As I noted earlier, computers in a distributed system communicate and coordinate by â€œmessage passingâ€ between one or more other computers. Messages can be passed using any messaging protocol, whether thatâ€™s HTTP, RPC, or a custom protocol built for the specific implementation. There are two types of message-passing environments:</p><p>æ­£å¦‚æˆ‘å‰é¢æåˆ°çš„ï¼Œåˆ†å¸ƒå¼ç³»ç»Ÿä¸­çš„è®¡ç®—æœºé€šè¿‡ä¸€å°æˆ–å¤šå°å…¶ä»–è®¡ç®—æœºä¹‹é—´çš„â€œæ¶ˆæ¯ä¼ é€’â€è¿›è¡Œé€šä¿¡å’Œåè°ƒã€‚å¯ä»¥ä½¿ç”¨ä»»ä½•æ¶ˆæ¯ä¼ é€’åè®®ä¼ é€’æ¶ˆæ¯ï¼Œæ— è®ºæ˜¯HTTPã€RPCè¿˜æ˜¯ä¸ºç‰¹å®šå®ç°æ„å»ºçš„è‡ªå®šä¹‰åè®®ã€‚æœ‰ä¸¤ç§ç±»å‹çš„æ¶ˆæ¯ä¼ é€’ç¯å¢ƒï¼š</p><p>  In a synchronous system, it is assumed that messages will be delivered within some fixed, known amount of time.</p><p>åœ¨åŒæ­¥ç³»ç»Ÿä¸­ï¼Œå‡å®šæ¶ˆæ¯å°†åœ¨ä¸€æ®µå›ºå®šçš„å·²çŸ¥æ—¶é—´å†…ä¼ é€’ã€‚</p><p> Synchronous message passing is conceptually less complex because users have a guarantee: when they send a message, the receiving component will get it within a certain time frame. This allows users to model their protocol with a fixed upper bound of how long the message will take to reach its destination.</p><p>åŒæ­¥æ¶ˆæ¯ä¼ é€’åœ¨æ¦‚å¿µä¸Šä¸é‚£ä¹ˆå¤æ‚ï¼Œå› ä¸ºç”¨æˆ·æœ‰ä¸€ä¸ªä¿è¯ï¼šå½“ä»–ä»¬å‘é€æ¶ˆæ¯æ—¶ï¼Œæ¥æ”¶ç»„ä»¶å°†åœ¨ç‰¹å®šçš„æ—¶é—´èŒƒå›´å†…æ”¶åˆ°è¯¥æ¶ˆæ¯ã€‚è¿™å…è®¸ç”¨æˆ·ç”¨æ¶ˆæ¯åˆ°è¾¾ç›®çš„åœ°æ‰€éœ€æ—¶é—´çš„å›ºå®šä¸Šé™æ¥å»ºæ¨¡ä»–ä»¬çš„åè®®ã€‚</p><p> However, this type of environment is not very practical in a real-world distributed system where computers can crash or go offline and messages can be dropped, duplicated, delayed, or received out of order.</p><p>ç„¶è€Œï¼Œè¿™ç§ç±»å‹çš„ç¯å¢ƒåœ¨ç°å®ä¸–ç•Œçš„åˆ†å¸ƒå¼ç³»ç»Ÿä¸­å¹¶ä¸æ˜¯å¾ˆå®ç”¨ï¼Œåœ¨ç°å®ä¸–ç•Œä¸­ï¼Œè®¡ç®—æœºå¯èƒ½ä¼šå´©æºƒæˆ–è„±æœºï¼Œæ¶ˆæ¯å¯èƒ½ä¼šè¢«ä¸¢å¼ƒã€å¤åˆ¶ã€å»¶è¿Ÿæˆ–æ— åºæ¥æ”¶ã€‚</p><p>  In an asynchronous message-passing system, it is assumed that a network may delay messages infinitely, duplicate them, or deliver them out of order. In other words, there is no fixed upper bound on how long a message will take to be received.</p><p>åœ¨å¼‚æ­¥æ¶ˆæ¯ä¼ é€’ç³»ç»Ÿä¸­ï¼Œå‡è®¾ç½‘ç»œå¯èƒ½æ— é™å»¶è¿Ÿæ¶ˆæ¯ã€å¤åˆ¶æ¶ˆæ¯æˆ–æ— åºå‘é€æ¶ˆæ¯ã€‚æ¢å¥è¯è¯´ï¼Œæ¥æ”¶ä¸€æ¡æ¶ˆæ¯éœ€è¦å¤šé•¿æ—¶é—´æ²¡æœ‰å›ºå®šçš„ä¸Šé™ã€‚</p><p>    Next, weâ€™ll focus on understanding what it means to achieve â€œconsensusâ€ in a distributed system. But first, itâ€™s important to reiterate what we alluded to earlier: there are hundreds of hardware and software architectures used for distributed computing.</p><p>æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†é‡ç‚¹äº†è§£åœ¨åˆ†å¸ƒå¼ç³»ç»Ÿä¸­è¾¾æˆâ€œå…±è¯†â€æ„å‘³ç€ä»€ä¹ˆã€‚ä½†é¦–å…ˆï¼Œé‡è¦çš„æ˜¯è¦é‡ç”³æˆ‘ä»¬å‰é¢æåˆ°çš„ï¼šç”¨äºåˆ†å¸ƒå¼è®¡ç®—çš„ç¡¬ä»¶å’Œè½¯ä»¶ä½“ç³»ç»“æ„æœ‰æ•°ç™¾ç§ã€‚</p><p>   A replicated state machine is a deterministic state machine that is replicated across many computers but functions as a single state machine. Any of these computers may be faulty, but the state machine will still function.</p><p>å¤åˆ¶çŠ¶æ€æœºæ˜¯ç¡®å®šæ€§çŠ¶æ€æœºï¼Œå¯è·¨å¤šå°è®¡ç®—æœºå¤åˆ¶ï¼Œä½†å……å½“å•ä¸ªçŠ¶æ€æœºã€‚è¿™äº›è®¡ç®—æœºä¸­çš„ä»»ä½•ä¸€ä¸ªéƒ½å¯èƒ½å‡ºç°æ•…éšœï¼Œä½†çŠ¶æ€æœºä»å°†è¿è¡Œã€‚</p><p>  In a replicated state machine, if a transaction is valid, a set of inputs will cause the state of the system to transition to the next state. A transaction is an atomic operation on a database. This means the operations either complete in full or never complete at all. The set of transactions maintained in a replicated state machine is known as a â€œtransaction log.â€</p><p>åœ¨å¤åˆ¶çŠ¶æ€æœºä¸­ï¼Œå¦‚æœäº‹åŠ¡æœ‰æ•ˆï¼Œä¸€ç»„è¾“å…¥å°†å¯¼è‡´ç³»ç»ŸçŠ¶æ€è½¬æ¢åˆ°ä¸‹ä¸€ä¸ªçŠ¶æ€ã€‚äº‹åŠ¡æ˜¯å¯¹æ•°æ®åº“çš„åŸå­æ“ä½œã€‚è¿™æ„å‘³ç€è¿™äº›æ“ä½œè¦ä¹ˆå®Œå…¨å®Œæˆï¼Œè¦ä¹ˆæ°¸è¿œä¸ä¼šå®Œæˆã€‚åœ¨å¤åˆ¶çŠ¶æ€æœºä¸­ç»´æŠ¤çš„ä¸€ç»„äº‹åŠ¡ç§°ä¸ºâ€œäº‹åŠ¡æ—¥å¿—â€ã€‚</p><p> The logic for transitioning from one valid state to the next is called the â€œstate transition logic.â€</p><p>ä»ä¸€ä¸ªæœ‰æ•ˆçŠ¶æ€è½¬æ¢åˆ°ä¸‹ä¸€ä¸ªæœ‰æ•ˆçŠ¶æ€çš„é€»è¾‘ç§°ä¸ºâ€œçŠ¶æ€è½¬æ¢é€»è¾‘â€ã€‚</p><p>  In other words, a replicated state machine is a set of distributed computers that all start with the same initial value. For each state transition, each of the processes decides on the next value. Reaching â€œconsensusâ€ means that all the computers must collectively agree on the output of this value.</p><p>æ¢å¥è¯è¯´ï¼Œå¤åˆ¶çŠ¶æ€æœºæ˜¯ä¸€ç»„åˆ†å¸ƒå¼è®¡ç®—æœºï¼Œå®ƒä»¬éƒ½ä»¥ç›¸åŒçš„åˆå§‹å€¼å¼€å§‹ã€‚å¯¹äºæ¯ä¸ªçŠ¶æ€è½¬æ¢ï¼Œæ¯ä¸ªè¿›ç¨‹éƒ½å†³å®šä¸‹ä¸€ä¸ªå€¼ã€‚è¾¾æˆâ€œå…±è¯†â€æ„å‘³ç€æ‰€æœ‰çš„è®¡ç®—æœºå¿…é¡»é›†ä½“åŒæ„è¿™ä¸ªå€¼çš„è¾“å‡ºã€‚</p><p> In turn, this maintains a consistent transaction log across  every computer in the system (i.e., they â€œachieve a common goalâ€). The replicated state machine must continually accept new transactions into this log (i.e., â€œprovide a useful serviceâ€). It must do so despite the fact that:</p><p>åè¿‡æ¥ï¼Œè¿™ä¼šåœ¨ç³»ç»Ÿä¸­çš„æ¯å°è®¡ç®—æœºä¸Šç»´æŠ¤ä¸€è‡´çš„äº‹åŠ¡æ—¥å¿—(å³ï¼Œå®ƒä»¬â€œå®ç°äº†å…±åŒçš„ç›®æ ‡â€)ã€‚å¤åˆ¶çš„çŠ¶æ€æœºå¿…é¡»ä¸æ–­åœ°å°†æ–°äº‹åŠ¡æ¥æ”¶åˆ°è¯¥æ—¥å¿—ä¸­(å³ï¼Œâ€œæä¾›æœ‰ç”¨çš„æœåŠ¡â€)ã€‚å®ƒå¿…é¡»è¿™æ ·åšï¼Œå°½ç®¡äº‹å®æ˜¯ï¼š</p><p> The network is not reliable and messages may fail to deliver, be delayed, or be out of order.</p><p>ç½‘ç»œä¸å¯é ï¼Œæ¶ˆæ¯å¯èƒ½æ— æ³•ä¼ é€’ã€å»¶è¿Ÿæˆ–å‡ºç°æ•…éšœã€‚</p><p>      Note: Different algorithms have different variations of the conditions above. For example, some divide the   Agreement  property into   Consistency  and   Totality . Some have a concept of   Validity  or   Integrity  or   Efficiency . However, such nuances are beyond the scope of this post.</p><p>æ³¨ï¼šä¸åŒçš„ç®—æ³•å¯¹ä¸Šè¿°æ¡ä»¶æœ‰ä¸åŒçš„å˜åŒ–ã€‚ä¾‹å¦‚ï¼Œæœ‰äº›äººå°†åè®®å±æ€§åˆ†ä¸ºä¸€è‡´æ€§å’Œæ•´ä½“æ€§ã€‚æœ‰äº›äººæœ‰æœ‰æ•ˆæ€§ã€å®Œæ•´æ€§æˆ–æ•ˆç‡çš„æ¦‚å¿µã€‚ç„¶è€Œï¼Œè¿™äº›ç»†å¾®å·®åˆ«è¶…å‡ºäº†æœ¬æ–‡çš„è®¨è®ºèŒƒå›´ã€‚</p><p>  Learners, other processes in the system which learn the final values that are decided upon.</p><p>å­¦ä¹ è€…ï¼Œç³»ç»Ÿä¸­å­¦ä¹ æœ€ç»ˆå†³å®šå€¼çš„å…¶ä»–è¿‡ç¨‹ã€‚</p><p>     The non-faulty processes listen to the value being proposed by the leader, validate it, and propose it as the next valid value.</p><p>æ— æ•…éšœæµç¨‹ç›‘å¬é¢†å¯¼æå‡ºçš„å€¼ï¼Œå¯¹å…¶è¿›è¡ŒéªŒè¯ï¼Œå¹¶å°†å…¶ä½œä¸ºä¸‹ä¸€ä¸ªæœ‰æ•ˆå€¼æå‡ºã€‚</p><p>  The non-faulty processes must come to a consensus on a single correct output value. If it receives a threshold number of identical votes which satisfy some criteria, then the processes will decide on that value.</p><p>éæ•…éšœæµç¨‹å¿…é¡»å°±å•ä¸ªæ­£ç¡®çš„è¾“å‡ºå€¼è¾¾æˆå…±è¯†ã€‚å¦‚æœå®ƒæ”¶åˆ°æ»¡è¶³æŸäº›æ ‡å‡†çš„ç›¸åŒé€‰ç¥¨çš„é˜ˆå€¼æ•°é‡ï¼Œåˆ™è¿›ç¨‹å°†å†³å®šè¯¥å€¼ã€‚</p><p>        Nonetheless, if we can use this generic process to build an algorithm that guarantees the general conditions defined above, then we have a distributed system which is able to achieve consensus.</p><p>å°½ç®¡å¦‚æ­¤ï¼Œå¦‚æœæˆ‘ä»¬èƒ½å¤Ÿä½¿ç”¨è¿™ä¸ªé€šç”¨è¿‡ç¨‹æ¥æ„å»ºä¸€ä¸ªç®—æ³•æ¥ä¿è¯ä¸Šé¢å®šä¹‰çš„ä¸€èˆ¬æ¡ä»¶ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±æ‹¥æœ‰äº†ä¸€ä¸ªèƒ½å¤Ÿè¾¾æˆå…±è¯†çš„åˆ†å¸ƒå¼ç³»ç»Ÿã€‚</p><p>       Reaching consensus in a synchronous environment is possible because we can make assumptions about the maximum time it takes for messages to get delivered. Thus, in this type of system, we can allow the different nodes in the system to take turns proposing new transactions, poll for a majority vote, and skip any node if it doesnâ€™t offer a proposal within the maximum time limit.</p><p>åœ¨åŒæ­¥ç¯å¢ƒä¸­è¾¾æˆå…±è¯†æ˜¯å¯èƒ½çš„ï¼Œå› ä¸ºæˆ‘ä»¬å¯ä»¥å‡è®¾æ¶ˆæ¯ä¼ é€’æ‰€éœ€çš„æœ€é•¿æ—¶é—´ã€‚å› æ­¤ï¼Œåœ¨è¿™ç§ç±»å‹çš„ç³»ç»Ÿä¸­ï¼Œæˆ‘ä»¬å¯ä»¥å…è®¸ç³»ç»Ÿä¸­çš„ä¸åŒèŠ‚ç‚¹è½®æµæå‡ºæ–°çš„äº‹åŠ¡ï¼Œè½®è¯¢å¤šæ•°ç¥¨ï¼Œå¹¶è·³è¿‡ä»»ä½•åœ¨æœ€å¤§æ—¶é—´é™åˆ¶å†…æ²¡æœ‰æå‡ºå»ºè®®çš„èŠ‚ç‚¹ã€‚</p><p> But, as noted earlier, assuming we are operating in synchronous environments is not practical outside of controlled environments where message latency is predictable, such as data centers which have synchronized atomic clocks.</p><p>ä½†æ˜¯ï¼Œå¦‚å‰æ‰€è¿°ï¼Œå‡è®¾æˆ‘ä»¬åœ¨åŒæ­¥ç¯å¢ƒä¸­æ“ä½œï¼Œåœ¨æ¶ˆæ¯å»¶è¿Ÿå¯é¢„æµ‹çš„å—æ§ç¯å¢ƒä¹‹å¤–æ˜¯ä¸ç°å®çš„ï¼Œä¾‹å¦‚å…·æœ‰åŒæ­¥åŸå­æ—¶é’Ÿçš„æ•°æ®ä¸­å¿ƒã€‚</p><p> In reality, most environments donâ€™t allow us to make the synchronous assumption. So we must design for asynchronous environments.</p><p>å®é™…ä¸Šï¼Œå¤§å¤šæ•°ç¯å¢ƒéƒ½ä¸å…è®¸æˆ‘ä»¬åšå‡ºåŒæ­¥å‡è®¾ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¿…é¡»é’ˆå¯¹å¼‚æ­¥ç¯å¢ƒè¿›è¡Œè®¾è®¡ã€‚</p><p> If we cannot assume a maximum message delivery time in an asynchronous environment, then achieving termination is much harder, if not impossible. Remember, one of the conditions that must be met to achieve consensus is â€œtermination,â€ which means every non-faulty node  must decide on some output value.</p><p>å¦‚æœæˆ‘ä»¬ä¸èƒ½å‡è®¾å¼‚æ­¥ç¯å¢ƒä¸­çš„æœ€å¤§æ¶ˆæ¯ä¼ é€’æ—¶é—´ï¼Œé‚£ä¹ˆå®ç°ç»ˆæ­¢å°±ä¼šå›°éš¾å¾—å¤šï¼Œå¦‚æœä¸æ˜¯ä¸å¯èƒ½çš„è¯ã€‚è¯·è®°ä½ï¼Œè¦è¾¾æˆå…±è¯†ï¼Œå¿…é¡»æ»¡è¶³çš„æ¡ä»¶ä¹‹ä¸€æ˜¯â€œç»ˆæ­¢â€ï¼Œè¿™æ„å‘³ç€æ¯ä¸ªæœªå‡ºé”™çš„èŠ‚ç‚¹éƒ½å¿…é¡»å†³å®šæŸä¸ªè¾“å‡ºå€¼ã€‚</p><p> This is formally known as the â€œFLP impossibility result.â€ How did it get this name? Well, Iâ€™m glad you asked!</p><p>è¿™è¢«æ­£å¼ç§°ä¸ºâ€œFLPä¸å¯èƒ½ç»“æœâ€ã€‚å®ƒæ˜¯æ€ä¹ˆå¾—åçš„ï¼Ÿå—¯ï¼Œæˆ‘å¾ˆé«˜å…´ä½ è¿™ä¹ˆé—®ï¼</p><p> Even a single faulty process makes it impossible to reach consensus among deterministic asynchronous processes.</p><p>å³ä½¿æ˜¯ä¸€ä¸ªå•ä¸€çš„æ•…éšœæµç¨‹ï¼Œä¹Ÿä¸å¯èƒ½åœ¨ç¡®å®šæ€§çš„å¼‚æ­¥æµç¨‹ä¹‹é—´è¾¾æˆå…±è¯†ã€‚</p><p> In their 1985 paper â€œ Impossibility of Distributed Consensus with One Faulty Process,â€ researchers Fischer, Lynch, and Paterson (aka FLP) show how even a single faulty process makes it impossible to reach consensus among deterministic asynchronous processes. Basically, because processes can fail at unpredictable times, itâ€™s also possible for them to fail at the exact opportune time that prevents consensus from occurring.</p><p>ç ”ç©¶äººå‘˜Fischerã€Lynchå’ŒPaterson(åˆåFLP)åœ¨1985å¹´çš„è®ºæ–‡ã€Šä¸€ä¸ªæœ‰æ•…éšœçš„è¿‡ç¨‹ä¸­ä¸å¯èƒ½è¾¾æˆåˆ†å¸ƒå¼å…±è¯†ã€‹ä¸­å±•ç¤ºäº†ï¼Œå³ä½¿æ˜¯ä¸€ä¸ªæœ‰æ•…éšœçš„è¿‡ç¨‹ä¹Ÿä¸å¯èƒ½åœ¨ç¡®å®šæ€§çš„å¼‚æ­¥è¿‡ç¨‹ä¸­è¾¾æˆå…±è¯†ã€‚åŸºæœ¬ä¸Šï¼Œå› ä¸ºæµç¨‹å¯èƒ½åœ¨ä¸å¯é¢„æµ‹çš„æ—¶é—´å¤±è´¥ï¼Œæ‰€ä»¥å®ƒä»¬ä¹Ÿæœ‰å¯èƒ½åœ¨é˜»æ­¢è¾¾æˆå…±è¯†çš„ç¡®åˆ‡æ—¶é—´å¤±è´¥ã€‚</p><p>  This result was a huge bummer for the distributed computing space. Nonetheless, scientists continued to push forward to find ways to circumvent FLP impossibility.</p><p>è¿™ä¸€ç»“æœå¯¹åˆ†å¸ƒå¼è®¡ç®—ç©ºé—´æ¥è¯´æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ«æŠ˜ã€‚å°½ç®¡å¦‚æ­¤ï¼Œç§‘å­¦å®¶ä»¬ä»åœ¨ç»§ç»­åŠªåŠ›å¯»æ‰¾ç»•è¿‡FLPä¸å¯èƒ½çš„æ–¹æ³•ã€‚</p><p>      Letâ€™s revisit our impossibility result. Hereâ€™s another way to think about it: the FLP impossibility result essentially shows that, if we cannot make progress in a system, then we cannot reach consensus. In other words, if messages are asynchronously delivered, termination cannot be guaranteed. Recall that termination is a required condition that means every non-faulty node must eventually decide on some output value.</p><p>è®©æˆ‘ä»¬é‡æ–°å®¡è§†ä¸€ä¸‹æˆ‘ä»¬ä¸å¯èƒ½çš„ç»“æœã€‚è¿™é‡Œæœ‰å¦ä¸€ç§æ€è€ƒæ–¹å¼ï¼šFLPä¸å¯èƒ½çš„ç»“æœæœ¬è´¨ä¸Šè¡¨æ˜ï¼Œå¦‚æœæˆ‘ä»¬ä¸èƒ½åœ¨ä¸€ä¸ªä½“ç³»ä¸­å–å¾—è¿›å±•ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±ä¸èƒ½è¾¾æˆå…±è¯†ã€‚æ¢å¥è¯è¯´ï¼Œå¦‚æœæ¶ˆæ¯æ˜¯å¼‚æ­¥ä¼ é€’çš„ï¼Œåˆ™æ— æ³•ä¿è¯ç»ˆæ­¢ã€‚å›æƒ³ä¸€ä¸‹ï¼Œç»ˆæ­¢æ˜¯å¿…éœ€çš„æ¡ä»¶ï¼Œè¿™æ„å‘³ç€æ¯ä¸ªéæ•…éšœèŠ‚ç‚¹æœ€ç»ˆéƒ½å¿…é¡»å†³å®šæŸä¸ªè¾“å‡ºå€¼ã€‚</p><p> But how can we guarantee every non-faulty process will decide on a value if we donâ€™t know when a message will be delivered due to asynchronous networks?</p><p>ä½†æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬ä¸çŸ¥é“æ¶ˆæ¯å°†åœ¨ä½•æ—¶ç”±äºå¼‚æ­¥ç½‘ç»œä¼ é€’ï¼Œæˆ‘ä»¬å¦‚ä½•ä¿è¯æ¯ä¸ªæ²¡æœ‰é”™è¯¯çš„è¿›ç¨‹éƒ½ä¼šå†³å®šä¸€ä¸ªå€¼å‘¢ï¼Ÿ</p><p> To be clear, the finding does not state that consensus is unreachable. Rather, due to asynchrony, consensus cannot be reached in a fixed time. Saying that consensus is â€œimpossibleâ€ simply means that consensus is â€œnot always possible.â€ Itâ€™s a subtle but crucial detail.</p><p>éœ€è¦æ˜ç¡®çš„æ˜¯ï¼Œè¿™ä¸€å‘ç°å¹¶ä¸æ˜¯è¯´æ— æ³•è¾¾æˆå…±è¯†ã€‚ç›¸åï¼Œç”±äºä¸åŒæ­¥ï¼Œä¸å¯èƒ½åœ¨å›ºå®šçš„æ—¶é—´å†…è¾¾æˆå…±è¯†ã€‚è¯´å…±è¯†æ˜¯â€œä¸å¯èƒ½çš„â€ï¼Œåªæ˜¯æ„å‘³ç€å…±è¯†â€œå¹¶ä¸æ€»æ˜¯å¯èƒ½çš„â€ã€‚è¿™æ˜¯ä¸€ä¸ªå¾®å¦™ä½†è‡³å…³é‡è¦çš„ç»†èŠ‚ã€‚</p><p> One way to circumvent this is to use timeouts. If no progress is being made on deciding the next value, we wait until a timeout, then start the steps all over again. As weâ€™re about to see, this is what consensus algorithms like Paxos and Raft essentially did.</p><p>è§„é¿æ­¤é—®é¢˜çš„ä¸€ç§æ–¹æ³•æ˜¯ä½¿ç”¨è¶…æ—¶ã€‚å¦‚æœåœ¨å†³å®šä¸‹ä¸€ä¸ªå€¼æ–¹é¢æ²¡æœ‰å–å¾—ä»»ä½•è¿›å±•ï¼Œæˆ‘ä»¬å°†ç­‰åˆ°è¶…æ—¶ï¼Œç„¶åé‡æ–°å¼€å§‹è¿™äº›æ­¥éª¤ã€‚æ­£å¦‚æˆ‘ä»¬å³å°†çœ‹åˆ°çš„ï¼Œè¿™å°±æ˜¯åƒPaxoså’ŒRAFTè¿™æ ·çš„å…±è¯†ç®—æ³•æ‰€åšçš„äº‹æƒ…ã€‚</p><p>  Introduced in the 1990s,  Paxos was the first real-world, practical, fault-tolerant consensus algorithm. Itâ€™s one of the first widely adopted consensus algorithms to be proven correct by Leslie Lamport and has been used by global internet companies like Google and Amazon to build distributed services.</p><p>Paxosäº20ä¸–çºª90å¹´ä»£æ¨å‡ºï¼Œæ˜¯ç¬¬ä¸€ä¸ªç°å®ä¸–ç•Œä¸­å®ç”¨çš„å®¹é”™ä¸€è‡´æ€§ç®—æ³•ã€‚è¿™æ˜¯è±æ–¯åˆ©Â·å…°æ³¢ç‰¹(Leslie Lamport)è¯æ˜æ˜¯æ­£ç¡®çš„é¦–æ‰¹è¢«å¹¿æ³›é‡‡ç”¨çš„å…±è¯†ç®—æ³•ä¹‹ä¸€ï¼Œå¹¶å·²è¢«è°·æ­Œ(Google)å’Œäºšé©¬é€Š(Amazon)ç­‰å…¨çƒäº’è”ç½‘å…¬å¸ç”¨æ¥æ„å»ºåˆ†å¸ƒå¼æœåŠ¡ã€‚</p><p>   The proposer chooses a new proposal version number (n) and sends a â€œprepare requestâ€ to the acceptors.</p><p>æå‡ºè€…é€‰æ‹©æ–°çš„ææ¡ˆç‰ˆæœ¬å·(N)ï¼Œå¹¶å‘æ¥å—è€…å‘é€â€œå‡†å¤‡è¯·æ±‚â€ã€‚</p><p> If acceptors receive a prepare request (â€œprepare,â€ n) with n greater than that of any prepare request they had already responded to, the acceptors send out (â€œack,â€ n, nâ€™, vâ€™) or (â€œack,â€ n, ^ , ^).</p><p>å¦‚æœæ¥å—è€…æ¥æ”¶åˆ°æ¯”ä»–ä»¬å·²ç»å“åº”çš„ä»»ä½•å‡†å¤‡è¯·æ±‚çš„nå¤§çš„å‡†å¤‡è¯·æ±‚(â€œPREPAREï¼Œâ€n)ï¼Œæ¥å—è€…å°±å‘å‡º(â€œackï¼Œâ€nï¼Œnâ€˜ï¼Œvâ€™)æˆ–(â€œackï¼Œâ€nï¼Œ^ï¼Œ^)ã€‚</p><p> Acceptors respond with a promise not to accept any more proposals numbered less than n.</p><p>æ¥å—è€…çš„å›åº”æ˜¯æ‰¿è¯ºä¸å†æ¥å—ä»»ä½•ç¼–å·å°äºnçš„ææ¡ˆã€‚</p><p> Acceptors suggest the value (v) of the highest-number proposal that they have accepted, if any. Or else, they respond with ^.</p><p>æ¥å—è€…å»ºè®®ä»–ä»¬æ‰€æ¥å—çš„æœ€é«˜ç¼–å·ææ¡ˆçš„å€¼(V)(å¦‚æœæœ‰çš„è¯)ã€‚å¦åˆ™ï¼Œä»–ä»¬ä¼šå›å¤^ã€‚</p><p>  If the proposer receives responses from a majority of the acceptors, then it can issue an accept request (â€œaccept,â€ n, v) with number n and value v.</p><p>å¦‚æœæå‡ºè€…æ”¶åˆ°æ¥è‡ªå¤§å¤šæ•°æ¥å—è€…çš„å“åº”ï¼Œåˆ™å®ƒå¯ä»¥å‘å‡ºæ•°å­—ä¸ºnã€å€¼ä¸ºvçš„æ¥å—è¯·æ±‚(â€œAcceptâ€ï¼Œnï¼Œv)ã€‚</p><p>   If the acceptor receives an accept request (â€œaccept,â€ n, v), it accepts the proposal unless it has already responded to a prepare request with a number greater than n.</p><p>å¦‚æœæ¥å—è€…æ¥æ”¶åˆ°æ¥å—è¯·æ±‚(â€œAcceptï¼Œâ€nï¼Œv)ï¼Œåˆ™å®ƒæ¥å—è¯¥æè®®ï¼Œé™¤éå®ƒå·²ç»ç”¨å¤§äºnçš„æ•°å­—å“åº”äº†å‡†å¤‡è¯·æ±‚ã€‚</p><p>  Whenever an acceptor accepts a proposal, it responds to all learners (â€œaccept,â€ n, v).</p><p>å½“æ¥å—è€…æ¥å—ä¸€ä¸ªæè®®æ—¶ï¼Œå®ƒä¼šå›åº”æ‰€æœ‰çš„å­¦ä¹ è€…(â€œæ¥å—â€ï¼Œnï¼Œv)ã€‚</p><p> Learners receive (â€œaccept,â€ n, v) from a majority of acceptors, decide v, and send (â€œdecide,â€ v) to all other learners.</p><p>å­¦ä¹ è€…ä»å¤§å¤šæ•°æ¥å—è€…é‚£é‡Œæ¥æ”¶(â€œæ¥å—â€ï¼Œnï¼Œv)ï¼Œå†³å®švï¼Œç„¶åå‘é€(â€œå†³å®šï¼Œâ€v)ç»™æ‰€æœ‰å…¶ä»–å­¦ä¹ è€…ã€‚</p><p>  Phew! Confused yet? I know that was a quite a lot of information to digest.</p><p>å“Ÿï¼å›°æƒ‘äº†å—ï¼Ÿæˆ‘çŸ¥é“è¿™æ˜¯ä¸€ä¸ªç›¸å½“å¤šçš„ä¿¡æ¯éœ€è¦æ¶ˆåŒ–ã€‚</p><p>  As we now know, every distributed system has faults. In this algorithm, if a proposer failed (e.g., because there was an omission fault), then decisions could be delayed. Paxos dealt with this by starting with a new version number in Phase 1, even if previous attempts never ended.</p><p>æ­£å¦‚æˆ‘ä»¬ç°åœ¨æ‰€çŸ¥é“çš„ï¼Œæ¯ä¸ªåˆ†å¸ƒå¼ç³»ç»Ÿéƒ½æœ‰æ•…éšœã€‚åœ¨è¯¥ç®—æ³•ä¸­ï¼Œå¦‚æœæè®®è€…å¤±è´¥(ä¾‹å¦‚ï¼Œå› ä¸ºå­˜åœ¨é—æ¼é”™è¯¯)ï¼Œé‚£ä¹ˆå†³ç­–å¯èƒ½ä¼šè¢«æ¨è¿Ÿã€‚Paxosé€šè¿‡åœ¨ç¬¬ä¸€é˜¶æ®µä½¿ç”¨ä¸€ä¸ªæ–°çš„ç‰ˆæœ¬å·æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå³ä½¿ä¹‹å‰çš„å°è¯•ä»æœªç»“æŸã€‚</p><p> I wonâ€™t go into details, but the process to get back to normal operations in such cases was quite complex since processes were expected to step in and drive the resolution process forward.</p><p>æˆ‘ä¸ä¼šè¯¦ç»†ä»‹ç»ï¼Œä½†åœ¨è¿™ç§æƒ…å†µä¸‹æ¢å¤æ­£å¸¸æ“ä½œçš„è¿‡ç¨‹ç›¸å½“å¤æ‚ï¼Œå› ä¸ºé¢„è®¡è¿‡ç¨‹ä¼šä»‹å…¥å¹¶æ¨åŠ¨è§£å†³è¿‡ç¨‹å‘å‰å‘å±•ã€‚</p><p> The main reason Paxos is so hard to understand is that many of its implementation details are left open to the readerâ€™s interpretation: How do we know when a proposer is failing? Do we use synchronous clocks to set a timeout period for deciding when a proposer is failing and we need to move on to the next rank? ğŸ¤·â€</p><p>Paxoså¦‚æ­¤éš¾ä»¥ç†è§£çš„ä¸»è¦åŸå› æ˜¯ï¼Œå®ƒçš„è®¸å¤šå®ç°ç»†èŠ‚éƒ½æœ‰å¾…è¯»è€…çš„è§£è¯»ï¼šæˆ‘ä»¬å¦‚ä½•çŸ¥é“ææ¡ˆä½•æ—¶å¤±è´¥ï¼Ÿæˆ‘ä»¬æ˜¯å¦ä½¿ç”¨åŒæ­¥æ—¶é’Ÿæ¥è®¾ç½®è¶…æ—¶æ—¶æ®µï¼Œä»¥å†³å®šä½•æ—¶ææ¡ˆå¤±è´¥ï¼Œæˆ‘ä»¬éœ€è¦è¿›å…¥ä¸‹ä¸€ä¸ªçº§åˆ«ï¼ŸğŸ¤·â€ã€‚</p><p> In favor of offering flexibility in implementation, several specifications in key areas are left open-ended. Things like leader election, failure detection, and log management are vaguely or  completely undefined.</p><p>ä¸ºäº†åœ¨å®ç°ä¸Šæä¾›çµæ´»æ€§ï¼Œå…³é”®é¢†åŸŸä¸­çš„å‡ ä¸ªè§„èŒƒæ˜¯ä¸é™æˆå‘˜åé¢çš„ã€‚é¢†å¯¼äººé€‰ä¸¾ã€æ•…éšœæ£€æµ‹å’Œæ—¥å¿—ç®¡ç†ç­‰å†…å®¹æ¨¡ç³Šæˆ–å®Œå…¨æ²¡æœ‰å®šä¹‰ã€‚</p><p> This design choice ended up becoming one of the biggest downsides of Paxos. Itâ€™s not only incredibly difficult to understand but difficult to implement as well. In turn, this made the field of distributed systems incredibly hard to navigate.</p><p>è¿™ç§è®¾è®¡é€‰æ‹©æœ€ç»ˆæˆä¸ºPaxosæœ€å¤§çš„ç¼ºç‚¹ä¹‹ä¸€ã€‚å®ƒä¸ä»…ä»¤äººéš¾ä»¥ç½®ä¿¡åœ°éš¾ä»¥ç†è§£ï¼Œè€Œä¸”ä¹Ÿéš¾ä»¥å®ç°ã€‚åè¿‡æ¥ï¼Œè¿™ä½¿å¾—åˆ†å¸ƒå¼ç³»ç»Ÿé¢†åŸŸçš„å¯¼èˆªå˜å¾—å¼‚å¸¸å›°éš¾ã€‚</p><p>  In Paxos, although timeouts are not explicit in the algorithm, when it comes to the actual implementation, electing a new proposer after some timeout period is necessary to achieve termination. Otherwise, we couldnâ€™t guarantee that acceptors would output the next value, and the system could come to a halt.</p><p>åœ¨Paxosä¸­ï¼Œè™½ç„¶è¶…æ—¶åœ¨ç®—æ³•ä¸­å¹¶ä¸æ˜ç¡®ï¼Œä½†å½“æ¶‰åŠåˆ°å®é™…å®ç°æ—¶ï¼Œéœ€è¦åœ¨è¶…æ—¶ä¸€æ®µæ—¶é—´åé€‰ä¸¾æ–°çš„æåäººæ¥å®ç°ç»ˆæ­¢ã€‚å¦åˆ™ï¼Œæˆ‘ä»¬ä¸èƒ½ä¿è¯æ¥å—è€…ä¼šè¾“å‡ºä¸‹ä¸€ä¸ªå€¼ï¼Œç³»ç»Ÿå¯èƒ½ä¼šåœæ­¢ã€‚</p><p>  In 2013, Ongaro and Ousterhout published a new consensus algorithm for a replicated state machine called  Raft, where the core goal was understandability (unlike Paxos).</p><p>2013å¹´ï¼ŒOngaroå’ŒOusterhoutå‘å¸ƒäº†ä¸€ç§åä¸ºRAFTçš„å¤åˆ¶çŠ¶æ€æœºçš„æ–°å…±è¯†ç®—æ³•ï¼Œå…¶æ ¸å¿ƒç›®æ ‡æ˜¯å¯ç†è§£æ€§(ä¸Paxosä¸åŒ)ã€‚</p><p> One important new thing we learned from Raft is the concept of using a shared timeout to deal with termination. In Raft, if you crash and restart, you wait at least one timeout period before trying to get yourself declared a leader, and you are guaranteed to make progress.</p><p>æˆ‘ä»¬ä»RAFTå­¦åˆ°çš„ä¸€ä¸ªé‡è¦çš„æ–°ä¸œè¥¿æ˜¯ä½¿ç”¨å…±äº«è¶…æ—¶æ¥å¤„ç†ç»ˆæ­¢çš„æ¦‚å¿µã€‚åœ¨RAFTä¸­ï¼Œå¦‚æœä½ å´©æºƒå¹¶é‡æ–°å¯åŠ¨ï¼Œä½ è‡³å°‘è¦ç­‰å¾…ä¸€æ®µæš‚åœæ—¶é—´ï¼Œç„¶åæ‰èƒ½è¯•å›¾è®©è‡ªå·±è¢«å®£å¸ƒä¸ºé¢†å¯¼è€…ï¼Œä½ è‚¯å®šä¼šå–å¾—è¿›å±•ã€‚</p><p>  While traditional consensus algorithms (such as Paxos and Raft) are able to thrive in asynchronous environments using some level of synchrony assumptions (i.e. timeouts), they are not Byzantine fault-tolerant. They are only crash fault-tolerant.</p><p>è™½ç„¶ä¼ ç»Ÿçš„ä¸€è‡´æ€§ç®—æ³•(å¦‚Paxoså’ŒRAFT)èƒ½å¤Ÿåœ¨ä½¿ç”¨æŸç§ç¨‹åº¦çš„åŒæ­¥å‡è®¾(å³è¶…æ—¶)çš„å¼‚æ­¥ç¯å¢ƒä¸­èŒå£®æˆé•¿ï¼Œä½†å®ƒä»¬å¹¶ä¸å…·æœ‰æ‹œå åº­å¼çš„å®¹é”™èƒ½åŠ›ã€‚å®ƒä»¬åªå…·æœ‰å´©æºƒå®¹é”™èƒ½åŠ›ã€‚</p><p> Crash-faults are easier to handle because we can model the process as either working or crashedâ€Šâ€”â€Š0 or 1. The processes canâ€™t act maliciously and lie. Therefore, in a crash fault-tolerant system, a distributed system can be built where a simple majority is enough to reach a consensus.</p><p>å´©æºƒæ•…éšœæ›´å®¹æ˜“å¤„ç†ï¼Œå› ä¸ºæˆ‘ä»¬å¯ä»¥å°†è¿›ç¨‹å»ºæ¨¡ä¸ºæ­£åœ¨å·¥ä½œæˆ–å´©æºƒçš„â€Š-â€Š0æˆ–1ã€‚è¿›ç¨‹ä¸èƒ½æ¶æ„æ“ä½œå’Œæ’’è°ã€‚å› æ­¤ï¼Œåœ¨å´©æºƒå®¹é”™ç³»ç»Ÿä¸­ï¼Œå¯ä»¥æ„å»ºä¸€ä¸ªç®€å•å¤šæ•°å°±è¶³ä»¥è¾¾æˆå…±è¯†çš„åˆ†å¸ƒå¼ç³»ç»Ÿã€‚</p><p> In an open and decentralized system (such as public blockchains), users have no control over the nodes in the network. Instead, each node makes decisions toward its individual goals, which may conflict with those of other nodes.</p><p>åœ¨å¼€æ”¾å»ä¸­å¿ƒåŒ–çš„ç³»ç»Ÿ(å¦‚å…¬å…±åŒºå—é“¾)ä¸­ï¼Œç”¨æˆ·å¯¹ç½‘ç»œä¸­çš„èŠ‚ç‚¹æ²¡æœ‰æ§åˆ¶æƒã€‚å–è€Œä»£ä¹‹çš„æ˜¯ï¼Œæ¯ä¸ªèŠ‚ç‚¹é’ˆå¯¹å…¶å„è‡ªçš„ç›®æ ‡åšå‡ºå†³ç­–ï¼Œè¿™å¯èƒ½ä¸å…¶ä»–èŠ‚ç‚¹çš„å†³ç­–ç›¸å†²çªã€‚</p><p> In a Byzantine system where nodes have different incentives and can lie, coordinate, or act arbitrarily, you cannot assume a simple majority is enough to reach consensus. Half or more of the supposedly honest nodes can coordinate with each other to lie.</p><p>åœ¨æ‹œå åº­å¼çš„ç³»ç»Ÿä¸­ï¼ŒèŠ‚ç‚¹æœ‰ä¸åŒçš„åŠ¨æœºï¼Œå¯ä»¥ä»»æ„æ’’è°ã€åè°ƒæˆ–è¡ŒåŠ¨ï¼Œä½ ä¸èƒ½å‡è®¾ç®€å•çš„å¤šæ•°å°±è¶³ä»¥è¾¾æˆå…±è¯†ã€‚ä¸€åŠæˆ–æ›´å¤šè¢«è®¤ä¸ºè¯šå®çš„èŠ‚ç‚¹å¯ä»¥ç›¸äº’åè°ƒä»¥æ’’è°ã€‚</p><p> For example, if an elected leader is Byzantine and maintains strong network connections to other nodes, it can compromise the system. Recall how we said we must model our system to either tolerate simple faults or Byzantine faults. Raft and Paxos are simple fault-tolerant but not Byzantine fault-tolerant. They are not designed to tolerate malicious behavior.</p><p>ä¾‹å¦‚ï¼Œå¦‚æœä¸€ä½å½“é€‰çš„é¢†å¯¼äººæ˜¯æ‹œå åº­å¼çš„ï¼Œå¹¶ä¸”ä¸å…¶ä»–èŠ‚ç‚¹ä¿æŒç€å¼ºå¤§çš„ç½‘ç»œè¿æ¥ï¼Œè¿™å¯èƒ½ä¼šå±åŠç³»ç»Ÿã€‚å›æƒ³ä¸€ä¸‹ï¼Œæˆ‘ä»¬æ›¾è¯´è¿‡ï¼Œæˆ‘ä»¬å¿…é¡»å¯¹æˆ‘ä»¬çš„ç³»ç»Ÿè¿›è¡Œå»ºæ¨¡ï¼Œè¦ä¹ˆå®¹å¿ç®€å•çš„é”™è¯¯ï¼Œè¦ä¹ˆå®¹å¿æ‹œå åº­å¼çš„é”™è¯¯ã€‚RAFTå’ŒPaxosæ˜¯ç®€å•çš„å®¹é”™ï¼Œä½†ä¸æ˜¯æ‹œå åº­å¼çš„å®¹é”™ã€‚å®ƒä»¬çš„è®¾è®¡ä¸æ˜¯ä¸ºäº†å®¹å¿æ¶æ„è¡Œä¸ºã€‚</p><p>  Trying to build a reliable computer system that can handle processes that provide conflicting information is formally known as the â€œ Byzantine Generalâ€™s Proble</p><p>è¯•å›¾å»ºç«‹ä¸€ä¸ªå¯é çš„è®¡ç®—æœºç³»ç»Ÿæ¥å¤„ç†æä¾›ç›¸äº’å†²çªçš„ä¿¡æ¯çš„è¿‡ç¨‹è¢«æ­£å¼åœ°ç§°ä¸ºâ€œæ‹œå åº­å°†å†›çš„é—®é¢˜â€</p><p>......</p><p>.</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://www.preethikasireddy.com/post/lets-take-a-crack-at-understanding-distributed-consensus">https://www.preethikasireddy.com/post/lets-take-a-crack-at-understanding-distributed-consensus</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/åˆ†å¸ƒå¼/">#åˆ†å¸ƒå¼</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/å¯èƒ½/">#å¯èƒ½</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è®¾è®¡/">#è®¾è®¡</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/æ¸¸æˆ/">#æ¸¸æˆ</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/åˆ›æ„/">#åˆ›æ„</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è‹¹æœ/">#è‹¹æœ</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/æ‘„å½±/">#æ‘„å½±</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è½¯ä»¶/">#è½¯ä»¶</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/ç¾å›½/">#ç¾å›½</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/å›¾ç‰‡/">#å›¾ç‰‡</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/æ‰‹æœº/">#æ‰‹æœº</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è§†é¢‘/">#è§†é¢‘</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/å¹¿å‘Š/">#å¹¿å‘Š</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/å…è´¹/">#å…è´¹</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è°·æ­Œ/">#è°·æ­Œ</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/ç½‘ç«™/">#ç½‘ç«™</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/å¾®è½¯/">#å¾®è½¯</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/ä¸‹è½½/">#ä¸‹è½½</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/éŸ³ä¹/">#éŸ³ä¹</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/linux/">#linux</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/åšå®¢/">#åšå®¢</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/ç¨‹åº/">#ç¨‹åº</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è‰ºæœ¯/">#è‰ºæœ¯</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>