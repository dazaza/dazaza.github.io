<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>éšæœºçŸ©é˜µå…¥é—¨ï¼šåˆ†æ­¥æŒ‡å—Getting Started with Random Matrices: A Step-by-Step Guide</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Getting Started with Random Matrices: A Step-by-Step Guide<br/>éšæœºçŸ©é˜µå…¥é—¨ï¼šåˆ†æ­¥æŒ‡å—</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2020-11-21 23:23:08</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2020/11/750c984af0ffdd34a987bb62e6ab009b.png"><img src="http://img2.diglog.com/img/2020/11/750c984af0ffdd34a987bb62e6ab009b.png" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>In the Deep Learning (DL) age, more and more people have encountered and used (knowingly or not) random matrices. Most of the time this use is limited to the initialization of the networks weights, that can be accomplished with a single line of code in your favorite DL framework. However, Random Matrices have a rich mathematical theory with far reaching applications in physics, network theory, machine learning, finance, etc. This fascinating range of applications also means that each field often developed its dedicated terminology to describe the same mathematical concept, often with confusing consequences. The aim of this article(s) is to introduce Random Matrix Theory (RMT) from a physicist&#39;s perspective while trying to relate different tools and terminology, especially across the math and physics literature. Although you do need to know basic calculus, I will try to introduce new concepts and give references in case you would like to go deeper.</p><p>åœ¨æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰æ—¶ä»£ï¼Œè¶Šæ¥è¶Šå¤šçš„äººé‡åˆ°å¹¶ä½¿ç”¨ï¼ˆå·²çŸ¥æˆ–ä¸çŸ¥é“ï¼‰éšæœºçŸ©é˜µã€‚åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œè¿™ç§ä½¿ç”¨ä»…é™äºç½‘ç»œæƒé‡çš„åˆå§‹åŒ–ï¼Œè¿™å¯ä»¥é€šè¿‡æ‚¨å–œæ¬¢çš„DLæ¡†æ¶ä¸­çš„å•è¡Œä»£ç æ¥å®Œæˆã€‚ä½†æ˜¯ï¼ŒéšæœºçŸ©é˜µå…·æœ‰ä¸°å¯Œçš„æ•°å­¦ç†è®ºï¼Œåœ¨ç‰©ç†å­¦ï¼Œç½‘ç»œç†è®ºï¼Œæœºå™¨å­¦ä¹ ï¼Œé‡‘èç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨ã€‚è¿™ç§å¼•äººå…¥èƒœçš„åº”ç”¨èŒƒå›´è¿˜æ„å‘³ç€æ¯ä¸ªé¢†åŸŸéƒ½ç»å¸¸å¼€å‘å…¶ä¸“ç”¨æœ¯è¯­æ¥æè¿°ç›¸åŒçš„æ•°å­¦æ¦‚å¿µï¼Œé€šå¸¸ä»¤äººå›°æƒ‘çš„åæœã€‚æœ¬æ–‡çš„ç›®çš„æ˜¯ä»ç‰©ç†å­¦å®¶çš„è§’åº¦ä»‹ç»éšæœºçŸ©é˜µç†è®ºï¼ˆRMTï¼‰ï¼ŒåŒæ—¶å°è¯•è”ç³»ä¸åŒçš„å·¥å…·å’Œæœ¯è¯­ï¼Œå°¤å…¶æ˜¯æ•´ä¸ªæ•°å­¦å’Œç‰©ç†å­¦æ–‡çŒ®ã€‚å°½ç®¡æ‚¨ç¡®å®éœ€è¦äº†è§£åŸºæœ¬çš„æ¼”ç®—ï¼Œä½†æ˜¯æˆ‘å°†å°è¯•ä»‹ç»æ–°çš„æ¦‚å¿µå¹¶æä¾›å‚è€ƒï¼Œä»¥å¤‡æ‚¨æƒ³è¿›ä¸€æ­¥äº†è§£ã€‚</p><p> This post is complemented by a numerical analysis you can find in this  GitHub repository in Jupyter notebook format. Some of the content of the notebook is also presented in the last section.</p><p> è¿™ç¯‡æ–‡ç« è¿˜è¾…ä»¥æ•°å€¼åˆ†æï¼Œæ‚¨å¯ä»¥åœ¨æ­¤GitHubå­˜å‚¨åº“ä¸­æ‰¾åˆ°Jupyterç¬”è®°æœ¬æ ¼å¼çš„æ•°å€¼åˆ†æã€‚æœ€åä¸€éƒ¨åˆ†è¿˜ä»‹ç»äº†ç¬”è®°æœ¬çš„ä¸€äº›å†…å®¹ã€‚</p><p>  Historically, random matrices were introduced in statistics by John Wishart (1928), and saw their first appearance in physics when Eugene Wigner [1] used them to model the nuclei of heavy atoms (1955). You may know that at the microscopic level, where quantum mechanical effects are dominant, the energy spectrum of an atom is not continuous, but comes in discrete energy levels. Evaluating these levels for complex atoms comes at an expensive numerical cost on modern computers, but it was basically impossible to evaluate at the time Wigner was looking into the problem. Instead, he conjectured that these spectra could be akin to the eigenvalues of a random matrix, whose properties should only depend on its symmetry class (more on this later). This is an example of universality: complex systems that differ at some microscopic scale, tend to have similar properties at a â€œmacroscopic oneâ€. Moving from &#34;micro to macro&#34;, many details are washed out from the description of a system, and the remaining  relevant degrees of freedom tend to be common to a larger class of systems that share some particular symmetry. This idea of symmetry classes in RMT was further developed by Freeman Dyson [2]. There are three classical RM ensembles:  orthogonal,  unitary and  symplectic; Dyson refer to them as the  threefold way. If all of this sounds a bit obscure do not worry, we will come back to this later on.</p><p>  å†å²ä¸Šï¼Œçº¦ç¿°Â·æ€€æ²™ç‰¹ï¼ˆJohn Wishartï¼‰ï¼ˆ1928ï¼‰åœ¨ç»Ÿè®¡å­¦ä¸­å¼•å…¥äº†éšæœºçŸ©é˜µï¼Œå½“å°¤é‡‘Â·ç»´æ ¼çº³ï¼ˆEugene Wignerï¼‰[1]ä½¿ç”¨éšæœºçŸ©é˜µå»ºæ¨¡é‡åŸå­çš„æ ¸æ—¶ï¼ˆ1955ï¼‰ï¼ŒéšæœºçŸ©é˜µé¦–æ¬¡å‡ºç°åœ¨ç‰©ç†å­¦ä¸­ã€‚æ‚¨å¯èƒ½çŸ¥é“ï¼Œåœ¨é‡å­åŠ›å­¦ä½œç”¨å ä¸»å¯¼çš„å¾®è§‚æ°´å¹³ä¸Šï¼ŒåŸå­çš„èƒ½è°±ä¸æ˜¯è¿ç»­çš„ï¼Œè€Œæ˜¯å¤„äºç¦»æ•£çš„èƒ½çº§ã€‚åœ¨ç°ä»£è®¡ç®—æœºä¸Šï¼Œå¯¹å¤æ‚åŸå­çš„è¿™äº›èƒ½çº§è¿›è¡Œè¯„ä¼°çš„ä»£ä»·æ˜¯æ˜‚è´µçš„ï¼Œä½†æ˜¯åœ¨ç»´æ ¼çº³ç ”ç©¶é—®é¢˜æ—¶ï¼ŒåŸºæœ¬ä¸Šä¸å¯èƒ½è¿›è¡Œè¯„ä¼°ã€‚å–è€Œä»£ä¹‹çš„æ˜¯ï¼Œä»–æ¨æµ‹è¿™äº›å…‰è°±å¯èƒ½ç±»ä¼¼äºéšæœºçŸ©é˜µçš„ç‰¹å¾å€¼ï¼Œè€ŒéšæœºçŸ©é˜µçš„ç‰¹æ€§åº”ä»…å–å†³äºå…¶å¯¹ç§°æ€§ç±»åˆ«ï¼ˆç¨åå†è®¨è®ºï¼‰ã€‚è¿™æ˜¯æ™®éæ€§çš„ä¸€ä¸ªä¾‹å­ï¼šåœ¨æŸäº›å¾®è§‚å°ºåº¦ä¸Šä¸åŒçš„å¤æ‚ç³»ç»Ÿåœ¨â€œå®è§‚â€ä¸Šå¾€å¾€å…·æœ‰ç›¸ä¼¼çš„ç‰¹æ€§ã€‚ä»â€œå¾®è§‚â€åˆ°â€œå®è§‚â€ï¼Œè®¸å¤šç»†èŠ‚éƒ½ä»ç³»ç»Ÿæè¿°ä¸­å‰”é™¤æ‰äº†ï¼Œå‰©ä½™çš„ç›¸å…³è‡ªç”±åº¦å¾€å¾€æ˜¯å…±äº«æŸäº›ç‰¹å®šå¯¹ç§°æ€§çš„è¾ƒå¤§å‹ç³»ç»Ÿæ‰€å…±æœ‰çš„ã€‚ Freeman Dyson [2]è¿›ä¸€æ­¥å‘å±•äº†RMTä¸­å¯¹ç§°ç±»çš„æƒ³æ³•ã€‚æœ‰ä¸‰ç§ç»å…¸çš„RMåˆå¥ï¼šæ­£äº¤ï¼Œunitå’Œè¾›ã€‚æˆ´æ£®å°†å®ƒä»¬ç§°ä¸ºä¸‰é‡æ–¹å¼ã€‚å¦‚æœè¿™ä¸€åˆ‡å¬èµ·æ¥æœ‰äº›æ™¦æ¶©ï¼Œè¯·æ”¾å¿ƒï¼Œæˆ‘ä»¬ç¨åä¼šå†è®²ã€‚</p><p>  Before diving deeper into RMT, let me recap some basic concepts of standard matrices. The purpose of this section is to define terminology, introduce the basic tools we will work with and refresh your memory in case you forgot some of these concepts. In doing so I will make no attempt for a thorough explanation, but I will assume some basic knowledge of calculus and linear algebra. Consider an NÃ—N matrix  M; the typical problem setup is:</p><p>  åœ¨æ·±å…¥ç ”ç©¶RMTä¹‹å‰ï¼Œè®©æˆ‘å›é¡¾ä¸€ä¸‹æ ‡å‡†çŸ©é˜µçš„ä¸€äº›åŸºæœ¬æ¦‚å¿µã€‚æœ¬éƒ¨åˆ†çš„ç›®çš„æ˜¯å®šä¹‰æœ¯è¯­ï¼Œä»‹ç»æˆ‘ä»¬å°†ä½¿ç”¨çš„åŸºæœ¬å·¥å…·ï¼Œå¹¶åœ¨æ‚¨å¿˜è®°å…¶ä¸­ä¸€äº›æ¦‚å¿µæ—¶åˆ·æ–°æ‚¨çš„è®°å¿†ã€‚è¿™æ ·åšæ—¶ï¼Œæˆ‘å°†ä¸åšä»»ä½•è¯¦å°½çš„è§£é‡Šï¼Œä½†æˆ‘å°†å‡è®¾ä¸€äº›å¾®ç§¯åˆ†å’Œçº¿æ€§ä»£æ•°çš„åŸºæœ¬çŸ¥è¯†ã€‚è€ƒè™‘ä¸€ä¸ªNÃ—NçŸ©é˜µMï¼›å…¸å‹çš„é—®é¢˜è®¾ç½®æ˜¯ï¼š</p><p>  where  x and  b are two vectors: the former is the variable we want to solve for and the latter a â€œsourceâ€ term. You can think about this formula as a compact way to represent a system of coupled linear equations. A typical problem in linear algebra is to find the eigenvalues and eigenvectors of this matrix by finding the roots of the  characteristic equation:</p><p>  å…¶ä¸­xå’Œbæ˜¯ä¸¤ä¸ªå‘é‡ï¼šå‰è€…æ˜¯æˆ‘ä»¬è¦æ±‚è§£çš„å˜é‡ï¼Œåè€…æ˜¯â€œæºâ€é¡¹ã€‚æ‚¨å¯ä»¥å°†æ­¤å…¬å¼è§†ä¸ºè¡¨ç¤ºè€¦åˆçº¿æ€§æ–¹ç¨‹ç»„çš„ç´§å‡‘æ–¹å¼ã€‚çº¿æ€§ä»£æ•°ä¸­çš„ä¸€ä¸ªå…¸å‹é—®é¢˜æ˜¯é€šè¿‡æ‰¾åˆ°ç‰¹å¾æ–¹ç¨‹çš„æ ¹æ¥æ‰¾åˆ°è¯¥çŸ©é˜µçš„ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡ï¼š</p><p>  where Î» are the eigenvalues and  I is the identity matrix (in the equations I will use a hat to denote a matrix). From here, we can find the matrix  U that diagonalizes  M, i.e.  M =  U ğš²  U^(-1), where ğš² =  diag (Î»1, Î»2, â€¦, Î»N). Note that  M could have a rectangular shape, i.e. be an M Ã— N matrix, in which case the so called  singular values need to be evaluated; although these rectangular matrices are not very popular in physics, they are prominent in Machine learning (input data, weights, etcâ€¦) or finance, but more on this in a follow up post. The N eigenvalues of  M form its â€œspectrumâ€, containing many important information about the system. For example, as mentioned in the introduction, they can describe the energy levels of a quantum mechanical system.</p><p>å…¶ä¸­Î»æ˜¯ç‰¹å¾å€¼ï¼ŒIæ˜¯å•ä½çŸ©é˜µï¼ˆåœ¨ç­‰å¼ä¸­ï¼Œæˆ‘å°†ç”¨å¸½å­æ¥è¡¨ç¤ºçŸ©é˜µï¼‰ã€‚ä»è¿™é‡Œï¼Œæˆ‘ä»¬å¯ä»¥æ‰¾åˆ°å¯¹è§’åŒ–Mçš„çŸ©é˜µUï¼Œå³M = U ğš² U ^ï¼ˆ-1ï¼‰ï¼Œå…¶ä¸­ğš² = diagï¼ˆÎ»1ï¼ŒÎ»2ï¼Œ...ï¼ŒÎ»Nï¼‰ã€‚æ³¨æ„ï¼ŒMå¯ä»¥å…·æœ‰çŸ©å½¢å½¢çŠ¶ï¼Œå³æ˜¯MÃ—NçŸ©é˜µï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œéœ€è¦è¯„ä¼°æ‰€è°“çš„å¥‡å¼‚å€¼ï¼›å°½ç®¡è¿™äº›çŸ©å½¢çŸ©é˜µåœ¨ç‰©ç†ä¸Šä¸æ˜¯å¾ˆæµè¡Œï¼Œä½†å®ƒä»¬åœ¨æœºå™¨å­¦ä¹ ï¼ˆè¾“å…¥æ•°æ®ï¼Œæƒé‡ç­‰ï¼‰æˆ–é‡‘èå­¦ä¸­å¾ˆçªå‡ºï¼Œä½†åœ¨åç»­æ–‡ç« ä¸­ä¼šä»‹ç»æ›´å¤šã€‚ Mçš„Nä¸ªç‰¹å¾å€¼å½¢æˆå…¶â€œè°±â€ï¼Œå…¶ä¸­åŒ…å«æœ‰å…³ç³»ç»Ÿçš„è®¸å¤šé‡è¦ä¿¡æ¯ã€‚ä¾‹å¦‚ï¼Œå¦‚å¼•è¨€ä¸­æ‰€è¿°ï¼Œå®ƒä»¬å¯ä»¥æè¿°é‡å­åŠ›å­¦ç³»ç»Ÿçš„èƒ½çº§ã€‚</p><p> It is often convenient to evaluate the spectrum of  M using the  resolvent method; this method has the advantage of generalizing quite easily to infinite dimensional spaces, and can be used to solve non-homogenous, linear or non-linear differential equations. In this latter context, the resolvent is known as the Greenâ€™s function, and it is one of the central objects to evaluate when doing calculations in quantum field theory, see e.g. Ref. [4] for a general introduction. Going back to Eq.[1], we want to solve  x =  M^(-1)  b. This inverse operator is represented by the resolvent:</p><p> ä½¿ç”¨åˆ†è¾¨æ–¹æ³•è¯„ä¼°Mçš„å…‰è°±é€šå¸¸å¾ˆæ–¹ä¾¿ã€‚è¯¥æ–¹æ³•çš„ä¼˜ç‚¹æ˜¯å¯ä»¥å¾ˆå®¹æ˜“åœ°å°†å…¶æ¨å¹¿åˆ°æ— é™ç»´ç©ºé—´ï¼Œå¹¶ä¸”å¯ä»¥ç”¨äºæ±‚è§£éé½æ¬¡ï¼Œçº¿æ€§æˆ–éçº¿æ€§å¾®åˆ†æ–¹ç¨‹ã€‚åœ¨åä¸€ç§æƒ…å†µä¸‹ï¼Œå¯åˆ†è¾¨ç‰©è¢«ç§°ä¸ºæ ¼æ—å‡½æ•°ï¼Œå®ƒæ˜¯åœ¨é‡å­åœºè®ºä¸­è¿›è¡Œè®¡ç®—æ—¶è¦è¯„ä¼°çš„ä¸»è¦å¯¹è±¡ä¹‹ä¸€ï¼Œä¾‹å¦‚å‚è€ƒ[4]ä½œä¸€èˆ¬ä»‹ç»ã€‚å›åˆ°ç­‰å¼[1]ï¼Œæˆ‘ä»¬æƒ³æ±‚è§£x = M ^ï¼ˆ-1ï¼‰bã€‚æ­¤é€†è¿ç®—ç¬¦ç”±è§£æå™¨è¡¨ç¤ºï¼š</p><p>  The idea is that the resolvent has poles (i.e. singularities) located at the real eigenvalues of  M. We can then use the toolbox of complex analysis by defining z= Î» + i Îµ, such that the singularities are moved away from the real axis by an infinitesimal amount Îµ. This procedure is generally known as  regularization. The (normalized) trace of the resolvent is known as the  Stieltjes transform of  M in the mathematical literature, and it is a very familiar object for physicists:</p><p>  è¿™ä¸ªæƒ³æ³•æ˜¯ï¼Œåˆ†è§£ä½“çš„æç‚¹ï¼ˆå³å¥‡ç‚¹ï¼‰ä½äºMçš„çœŸå®ç‰¹å¾å€¼å¤„ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å®šä¹‰z =Î»+ iÎµæ¥ä½¿ç”¨å¤æ‚åˆ†æçš„å·¥å…·ç®±ï¼Œä»è€Œä½¿å¥‡ç‚¹è¿œç¦»å®è½´æ— é™å°Îµã€‚æ­¤è¿‡ç¨‹é€šå¸¸ç§°ä¸ºæ­£åˆ™åŒ–ã€‚è§£æç‰©çš„ï¼ˆå½’ä¸€åŒ–ï¼‰è¿¹çº¿åœ¨æ•°å­¦æ–‡çŒ®ä¸­è¢«ç§°ä¸ºMçš„Stieltjeså˜æ¢ï¼Œå®ƒæ˜¯ç‰©ç†å­¦å®¶éå¸¸ç†Ÿæ‚‰çš„å¯¹è±¡ï¼š</p><p>  where we have defined the eigenvalues distribution Ï(Î»), also known as the density of states in quantum mechanics. Here Î´(.) is the Dirac delta function, see Ref.[5]. Our final task is to evaluate the eigenvalue distribution of the resolvent and hence the matrix  M. The last bit of information we need is how to link the eigenvalue distribution to the trace of the resolvent, that is by using the Plemelj formula, see Ref. [4]:</p><p>  åœ¨è¿™é‡Œæˆ‘ä»¬å®šä¹‰äº†ç‰¹å¾å€¼åˆ†å¸ƒÏï¼ˆÎ»ï¼‰ï¼Œä¹Ÿç§°ä¸ºé‡å­åŠ›å­¦ä¸­çš„çŠ¶æ€å¯†åº¦ã€‚æ­¤å¤„çš„Î´ï¼ˆã€‚ï¼‰æ˜¯ç‹„æ‹‰å…‹Î´å‡½æ•°ï¼Œè¯·å‚è§å‚è€ƒæ–‡çŒ®[5]ã€‚æˆ‘ä»¬çš„æœ€ç»ˆä»»åŠ¡æ˜¯è¯„ä¼°åˆ†è§£ç‰©çš„ç‰¹å¾å€¼åˆ†å¸ƒï¼Œä»è€Œè¯„ä¼°çŸ©é˜µMã€‚æˆ‘ä»¬éœ€è¦çš„æœ€åä¸€ç‚¹ä¿¡æ¯æ˜¯å¦‚ä½•ä½¿ç”¨Plemeljå…¬å¼å°†ç‰¹å¾å€¼åˆ†å¸ƒä¸åˆ†è§£ç‰©çš„è½¨è¿¹è”ç³»èµ·æ¥ã€‚ [4]ï¼š</p><p>  In the random case, the entries of  M will be sampled from a distribution, so the above object needs to be averaged over all possible realizations of  M. Once the average of Tr[G(z)] is evaluated, we will analytically continue the result, i.e. use z â†’ Î» + i Îµ, take the Imaginary part and than the limit Îµ â†’ 0^(+) at the end (the + sign means that we are approaching 0 from the positive real axis). In the next section we revise few key concept from probability theory that will be useful to accomplish our task.</p><p>  åœ¨éšæœºæƒ…å†µä¸‹ï¼Œå°†ä»åˆ†å¸ƒä¸­é‡‡æ ·Mçš„é¡¹ï¼Œå› æ­¤éœ€è¦åœ¨Mçš„æ‰€æœ‰å¯èƒ½å®ç°ä¸Šå¯¹ä¸Šè¿°å¯¹è±¡æ±‚å¹³å‡ã€‚ä¸€æ—¦å¯¹Tr [Gï¼ˆzï¼‰]çš„å¹³å‡å€¼æ±‚å€¼ï¼Œæˆ‘ä»¬å°†ç»§ç»­åˆ†æç»“æœï¼Œå³ä½¿ç”¨zâ†’Î»+ iÎµï¼Œå–è™šéƒ¨ï¼Œç„¶ååœ¨æé™Îµâ†’0 ^ï¼ˆ+ï¼‰å¤„ç»“æŸï¼ˆ+å·è¡¨ç¤ºæˆ‘ä»¬ä»æ­£å®è½´æ¥è¿‘0ï¼‰ã€‚åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ä»æ¦‚ç‡è®ºä¸­ä¿®æ”¹ä¸€äº›å…³é”®æ¦‚å¿µï¼Œè¿™äº›æ¦‚å¿µå°†å¯¹å®Œæˆæˆ‘ä»¬çš„ä»»åŠ¡æœ‰ç”¨ã€‚</p><p>  This section is meant to refresh few concepts from probability that will be useful to understand the next section. I assume you are familiar with the concept of a random variable x. Depending on its domain, x can assume a series of values depending on the underlying probability distribution P(x). Say x denotes the position in space of an object, than P(x) will be the probability of the object to be at position x. A random matrix  X is an extension of this concept; you may think about it as a collection or random variables. We may have two objects located at positions x1 and x2; in general, the two positions may not be independent, so we need now a joint probability distribution to describe our problem. If the two objects are independent then:</p><p>æœ¬éƒ¨åˆ†æ—¨åœ¨ä»æ¦‚ç‡ä¸­åˆ·æ–°ä¸€äº›æ¦‚å¿µï¼Œè¿™äº›æ¦‚å¿µå°†æœ‰åŠ©äºç†è§£ä¸‹ä¸€éƒ¨åˆ†ã€‚æˆ‘å‡è®¾æ‚¨ç†Ÿæ‚‰éšæœºå˜é‡xçš„æ¦‚å¿µã€‚æ ¹æ®å…¶åŸŸï¼Œxå¯ä»¥æ ¹æ®åŸºç¡€æ¦‚ç‡åˆ†å¸ƒPï¼ˆxï¼‰å‡è®¾ä¸€ç³»åˆ—å€¼ã€‚è¯´xè¡¨ç¤ºç‰©ä½“åœ¨ç©ºé—´ä¸­çš„ä½ç½®ï¼Œåˆ™Pï¼ˆxï¼‰å°†æ˜¯ç‰©ä½“ä½äºä½ç½®xçš„æ¦‚ç‡ã€‚éšæœºçŸ©é˜µXæ˜¯è¯¥æ¦‚å¿µçš„æ‰©å±•ï¼›æ‚¨å¯ä»¥å°†å…¶è§†ä¸ºé›†åˆæˆ–éšæœºå˜é‡ã€‚æˆ‘ä»¬å¯èƒ½æœ‰ä¸¤ä¸ªå¯¹è±¡ä½äºx1å’Œx2ä½ç½®ï¼›é€šå¸¸ï¼Œè¿™ä¸¤ä¸ªèŒä½å¯èƒ½ä¸æ˜¯ç‹¬ç«‹çš„ï¼Œå› æ­¤æˆ‘ä»¬ç°åœ¨éœ€è¦ä¸€ä¸ªè”åˆæ¦‚ç‡åˆ†å¸ƒæ¥æè¿°æˆ‘ä»¬çš„é—®é¢˜ã€‚å¦‚æœä¸¤ä¸ªå¯¹è±¡æ˜¯ç‹¬ç«‹çš„ï¼Œåˆ™ï¼š</p><p>  that is one of the fundamental properties satisfied by the probabilities of two independent random processes. Think about the most common of the examples, rolling a dice. The probability that say number 3 comes out in one roll is 1/6; the probability that we get two consecutive 3 is 1/6 Ã— 1/6 and so on. This is not true if the two events are correlated, i.e. if they can influence each other. So the positions of the two objects may not be independent, but once we observe one, we can ask what is the probability that the second object is in a certain position  given the fact we observed it in another; mathematically this is denoted as:</p><p>  è¿™æ˜¯ä¸¤ä¸ªç‹¬ç«‹éšæœºè¿‡ç¨‹çš„æ¦‚ç‡æ‰€æ»¡è¶³çš„åŸºæœ¬ç‰¹æ€§ä¹‹ä¸€ã€‚æƒ³æƒ³æœ€å¸¸è§çš„ä¾‹å­ï¼Œæ·éª°å­ã€‚ä¸€å£è¯´å‡º3ä¸ªæ•°å­—çš„æ¦‚ç‡ä¸º1/6ï¼›æˆ‘ä»¬è¿ç»­å¾—åˆ°ä¸¤ä¸ª3çš„æ¦‚ç‡æ˜¯1/6Ã—1/6ï¼Œä¾æ­¤ç±»æ¨ã€‚å¦‚æœä¸¤ä¸ªäº‹ä»¶æ˜¯ç›¸å…³çš„ï¼Œå³å®ƒä»¬æ˜¯å¦å¯ä»¥ç›¸äº’å½±å“ï¼Œåˆ™æƒ…å†µå¹¶éå¦‚æ­¤ã€‚å› æ­¤ï¼Œä¸¤ä¸ªç‰©ä½“çš„ä½ç½®å¯èƒ½ä¸æ˜¯ç‹¬ç«‹çš„ï¼Œä½†æ˜¯ä¸€æ—¦è§‚å¯Ÿåˆ°ä¸€ä¸ªç‰©ä½“ï¼Œè€ƒè™‘åˆ°æˆ‘ä»¬åœ¨å¦ä¸€ä¸ªç‰©ä½“ä¸­è§‚å¯Ÿåˆ°çš„äº‹å®ï¼Œæˆ‘ä»¬å¯ä»¥é—®ç¬¬äºŒä¸ªç‰©ä½“å¤„äºæŸä¸ªä½ç½®çš„æ¦‚ç‡æ˜¯å¤šå°‘ï¼Ÿåœ¨æ•°å­¦ä¸Šï¼Œè¿™è¡¨ç¤ºä¸ºï¼š</p><p>  where the | expresses the conditional statement. To see how this is connected to random matrices, let me show an explicit representation of the two above equations, assuming that the random matrix  X is Gaussian but bearing in mind that those two relations above are valid in general. To be explicit,  X is a 2 Ã— 2 matrix with unitary variance:</p><p>  å“ªé‡Œ|è¡¨ç¤ºæ¡ä»¶è¯­å¥ã€‚ä¸ºäº†äº†è§£è¿™ä¸éšæœºçŸ©é˜µçš„å…³ç³»ï¼Œè®©æˆ‘å±•ç¤ºä¸Šè¿°ä¸¤ä¸ªæ–¹ç¨‹çš„æ˜¾å¼è¡¨ç¤ºï¼Œå‡è®¾éšæœºçŸ©é˜µXä¸ºé«˜æ–¯ï¼Œä½†è¦è®°ä½ä¸Šè¿°ä¸¤ä¸ªå…³ç³»é€šå¸¸æ˜¯æœ‰æ•ˆçš„ã€‚æ˜ç¡®åœ°è¯´ï¼ŒXæ˜¯ä¸€ä¸ª2Ã—2çš„å…·æœ‰ä¸€å…ƒæ–¹å·®çš„çŸ©é˜µï¼š</p><p>  Note that I am using Tr as a shorthand for a sum over all indices; this is not the usual definition of the trace (sum over diagonal elements) but it is often used in the physics literature. From Eq.(8) we see that even if the single elements of the RM are i.i.d. (independently, individually distributed), the RM itself encompasses a certain degree of correlations due to the presence of the off-diagonal terms. If the latter are zero, we clearly have:</p><p>  è¯·æ³¨æ„ï¼Œæˆ‘ä½¿ç”¨Trä½œä¸ºæ‰€æœ‰ç´¢å¼•æ€»å’Œçš„ç®€å†™ï¼›è¿™ä¸æ˜¯é€šå¸¸çš„è½¨è¿¹å®šä¹‰ï¼ˆå¯¹è§’å…ƒç´ çš„æ€»å’Œï¼‰ï¼Œä½†æ˜¯åœ¨ç‰©ç†å­¦æ–‡çŒ®ä¸­ç»å¸¸ä½¿ç”¨ã€‚ä»å¼ï¼ˆ8ï¼‰å¯ä»¥çœ‹å‡ºï¼Œå³ä½¿RMçš„å•ä¸ªå…ƒç´ æ˜¯i.i.d. ï¼ˆç‹¬ç«‹åœ°ï¼Œç‹¬ç«‹åœ°åˆ†å¸ƒï¼‰ï¼Œç”±äºéå¯¹è§’é¡¹çš„å­˜åœ¨ï¼ŒRMæœ¬èº«åŒ…å«ä¸€å®šç¨‹åº¦çš„ç›¸å…³æ€§ã€‚å¦‚æœåè€…ä¸ºé›¶ï¼Œæˆ‘ä»¬æ˜¾ç„¶å…·æœ‰ï¼š</p><p>  On the other hand, we can rewrite the matrix distribution in terms of its elements as a conditional probability:</p><p>  å¦ä¸€æ–¹é¢ï¼Œæˆ‘ä»¬å¯ä»¥å°†çŸ©é˜µåˆ†å¸ƒçš„å…ƒç´ é‡å†™ä¸ºæ¡ä»¶æ¦‚ç‡ï¼š</p><p>  The second objects I need to briefly introduce is the moment and cumulant generating function. The former is defined as:</p><p>æˆ‘éœ€è¦ç®€è¦ä»‹ç»çš„ç¬¬äºŒä¸ªå¯¹è±¡æ˜¯çŸ©å’Œç´¯ç§¯é‡ç”Ÿæˆå‡½æ•°ã€‚å‰è€…å®šä¹‰ä¸ºï¼š</p><p>  where x is a random variable, P(x) its probability distribution and Î¾ a parameter used to generate all the moments. In physics, this parameter goes under different names, depending on which field you are working in, but usually goes under the name of  source term. Formally, you can obtain all the moments of the function by repeatedly differentiating with respect to Î¾:</p><p>  å…¶ä¸­xæ˜¯éšæœºå˜é‡ï¼ŒPï¼ˆxï¼‰æ˜¯å…¶æ¦‚ç‡åˆ†å¸ƒï¼ŒÎ¾æ˜¯ç”¨äºç”Ÿæˆæ‰€æœ‰çŸ©çš„å‚æ•°ã€‚åœ¨ç‰©ç†å­¦ä¸­ï¼Œæ­¤å‚æ•°ä½¿ç”¨ä¸åŒçš„åç§°ï¼Œå…·ä½“å–å†³äºæ‚¨åœ¨å“ªä¸ªé¢†åŸŸå·¥ä½œï¼Œä½†é€šå¸¸ä½¿ç”¨æºæœ¯è¯­çš„åç§°ã€‚å½¢å¼ä¸Šï¼Œæ‚¨å¯ä»¥é€šè¿‡åå¤åœ°å¯¹Î¾æ±‚å¾®åˆ†æ¥è·å¾—å‡½æ•°çš„æ‰€æœ‰çŸ©ï¼š</p><p>  However, it is often more useful to work with the cumulants, as they often simplify calculations; The cumulant generating function is defined as:</p><p>  ä½†æ˜¯ï¼Œä½¿ç”¨ç´¯ç§¯é‡é€šå¸¸ä¼šæ›´æœ‰ç”¨ï¼Œå› ä¸ºå®ƒä»¬ç»å¸¸ç®€åŒ–è®¡ç®—ã€‚ç´¯ç§¯é‡ç”Ÿæˆå‡½æ•°å®šä¹‰ä¸ºï¼š</p><p>  How does this connect with RM or statistical mechanics? The central object of statistical mechanics is the partition function of the Boltzmann distribution (see below). In order to obtain average physical quantities we introduce a source term Î¾ and differentiate with respect to it. However, as we need a properly normalized quantity, we take the average of the logarithm of the partition function, that is the cumulant generating function defined above. You can generalize this concept to matrices or even functionals, as it is done in quantum field theory â€¦ but this is beyond the scope of this article:)</p><p>  è¿™å¦‚ä½•ä¸RMæˆ–ç»Ÿè®¡æœºåˆ¶è”ç³»èµ·æ¥ï¼Ÿç»Ÿè®¡åŠ›å­¦çš„ä¸»è¦ç›®æ ‡æ˜¯ç»è€³å…¹æ›¼åˆ†å¸ƒçš„åˆ†é…å‡½æ•°ï¼ˆè§ä¸‹æ–‡ï¼‰ã€‚ä¸ºäº†è·å¾—å¹³å‡ç‰©ç†é‡ï¼Œæˆ‘ä»¬å¼•å…¥æºé¡¹Î¾å¹¶å¯¹å…¶è¿›è¡Œå¾®åˆ†ã€‚ä½†æ˜¯ï¼Œç”±äºæˆ‘ä»¬éœ€è¦é€‚å½“å½’ä¸€åŒ–çš„æ•°é‡ï¼Œå› æ­¤æˆ‘ä»¬å–åˆ†åŒºå‡½æ•°ï¼ˆå³ä¸Šé¢å®šä¹‰çš„ç´¯ç§¯é‡ç”Ÿæˆå‡½æ•°ï¼‰çš„å¯¹æ•°çš„å¹³å‡å€¼ã€‚æ‚¨å¯ä»¥å°†æ­¤æ¦‚å¿µæ¦‚æ‹¬ä¸ºçŸ©é˜µï¼Œç”šè‡³å¯ä»¥æ³›å‡½ï¼Œå°±åƒé‡å­åœºè®ºä¸­æ‰€åšçš„é‚£æ ·â€¦â€¦ä½†è¿™è¶…å‡ºäº†æœ¬æ–‡çš„èŒƒå›´ï¼šï¼‰</p><p>  In this section I will give a â€œdirtyâ€ evaluation of the eigenvalue density. Although this evaluation gives the correct result, it hides several subtle points I will only briefly mention for now. I will follow a statistical mechanics approach, mostly focused on the calculation side. Also, we will focus for now only on the simplest of the RMT ensembles, the Gaussian Orthogonal Ensemble, or GOE for simplicity. Matrices belonging to this ensemble all have elements that are Gaussian variables; the other condition is the symmetry class under which these matrices are invariant, rotational invariance. Let be  O an orthogonal matrix, then  O^T =  O^{-1}, where the superscript â€œTâ€ stands for the transpose operation. This matrix acts on the elements of a matrix  M and rotates them. For example, in two and three dimensions these matrices have the known form:</p><p>  åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘å°†å¯¹ç‰¹å¾å€¼å¯†åº¦è¿›è¡Œâ€œè‚®è„â€è¯„ä¼°ã€‚å°½ç®¡æ­¤è¯„ä¼°ç»™å‡ºäº†æ­£ç¡®çš„ç»“æœï¼Œä½†å®ƒéšè—äº†æˆ‘ç°åœ¨ä»…ç®€è¦æåŠçš„å‡ ä¸ªç»†å¾®ä¹‹å¤„ã€‚æˆ‘å°†éµå¾ªç»Ÿè®¡åŠ›å­¦æ–¹æ³•ï¼Œä¸»è¦ä¾§é‡äºè®¡ç®—æ–¹é¢ã€‚æ­¤å¤–ï¼Œä¸ºç®€å•èµ·è§ï¼Œæˆ‘ä»¬ç°åœ¨ä»…å…³æ³¨æœ€ç®€å•çš„RMTä¹å›¢ï¼Œé«˜æ–¯æ­£äº¤ä¹å›¢æˆ–GOEã€‚å±äºè¯¥æ•´ä½“çš„çŸ©é˜µéƒ½å…·æœ‰ä½œä¸ºé«˜æ–¯å˜é‡çš„å…ƒç´ ï¼›å¦ä¸€ä¸ªæ¡ä»¶æ˜¯å¯¹ç§°ç±»åˆ«ï¼Œåœ¨è¿™äº›ç±»åˆ«ä¸‹è¿™äº›çŸ©é˜µæ˜¯ä¸å˜çš„ï¼Œæ—‹è½¬ä¸å˜çš„ã€‚è®¾Oä¸ºæ­£äº¤çŸ©é˜µï¼Œåˆ™O ^ T = O ^ {-1}ï¼Œå…¶ä¸­ä¸Šæ ‡â€œ Tâ€ä»£è¡¨è½¬ç½®è¿ç®—ã€‚è¯¥çŸ©é˜µä½œç”¨äºçŸ©é˜µMçš„å…ƒç´ å¹¶æ—‹è½¬å®ƒä»¬ã€‚ä¾‹å¦‚ï¼Œåœ¨äºŒç»´å’Œä¸‰ç»´ä¸­ï¼Œè¿™äº›çŸ©é˜µå…·æœ‰å·²çŸ¥å½¢å¼ï¼š</p><p>  where the suffix â€œzâ€ in the three dimensional matrix means that rotations are performed around the z axis.</p><p>å…¶ä¸­ä¸‰ç»´çŸ©é˜µä¸­çš„åç¼€â€œ zâ€è¡¨ç¤ºå›´ç»•zè½´æ‰§è¡Œæ—‹è½¬ã€‚</p><p> What does it mean for  M to be rotationally invariant? The rotated matrix $  Mâ€™ =  O  M  O^T and  M have the same eigenvalues but rotated eigenvectors  w=  O  v. So the spectrum of  Mâ€™ and  M is the same. The first step in the calculation consists in manipulating the trace of the resolvent as follows:</p><p> Mæ—‹è½¬ä¸å˜æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿæ—‹è½¬åçš„çŸ©é˜µ$ Mâ€™= O M O ^ Tå’ŒMå…·æœ‰ç›¸åŒçš„ç‰¹å¾å€¼ï¼Œä½†æ˜¯æ—‹è½¬åçš„ç‰¹å¾å‘é‡w = O vã€‚å› æ­¤Mâ€™å’ŒMçš„é¢‘è°±ç›¸åŒã€‚è®¡ç®—çš„ç¬¬ä¸€æ­¥æ˜¯æŒ‰å¦‚ä¸‹æ–¹å¼å¤„ç†è§£æç‰©çš„è½¨è¿¹ï¼š</p><p>  Looking at the above expression you may think we have taken a step behind instead of forward! The idea is, however, to rewrite the trace of the resolvent as a more familiar object in statistical mechanics. Consider the following m-dimensional Gaussian Matrix integral:</p><p>  æŸ¥çœ‹ä»¥ä¸Šè¡¨è¾¾å¼ï¼Œæ‚¨å¯èƒ½ä¼šè®¤ä¸ºæˆ‘ä»¬å·²ç»è½åè€Œä¸æ˜¯å‰è¿›äº†ï¼ä½†æ˜¯ï¼Œè¯¥æƒ³æ³•æ˜¯å°†è§£æç¨‹åºçš„è½¨è¿¹é‡å†™ä¸ºç»Ÿè®¡åŠ›å­¦ä¸­æ›´ç†Ÿæ‚‰çš„å¯¹è±¡ã€‚è€ƒè™‘ä»¥ä¸‹mç»´é«˜æ–¯çŸ©é˜µç§¯åˆ†ï¼š</p><p>    where Z now has the form of a statistical mechanics partition function for a specific realization of  M and the measure ğ’Ÿ  ğœ‘ = âˆ dğœ‘/(2 ğœ‹)^(N/2), where the product runs over  i=(1,2â€¦,N). So far, we have just performed some formal manipulations to massage our initial expression in a form that can be treated using the tools of statistical mechanics. We can also identify F_N(z) =- 1/N log Z(z) as the free energy of the system (with inverse temperature Î²=1/T); note that from a probability point of view, this is the (average) cumulant generating function defined in Eq.(14). At this point we need to average the trace of the resolvent over the different realizations of  M using the rotationally invariant, Gaussian probability distribution:</p><p>    å…¶ä¸­ï¼ŒZç°åœ¨å…·æœ‰é’ˆå¯¹Mçš„ç‰¹å®šå®ç°çš„ç»Ÿè®¡åŠ›å­¦åˆ’åˆ†å‡½æ•°çš„å½¢å¼ï¼Œå¹¶ä¸”åº¦é‡ğ’Ÿ= d //ï¼ˆ2ï¼‰^ï¼ˆN / 2ï¼‰ï¼Œå…¶ä¸­ä¹˜ç§¯åœ¨i =ï¼ˆ1,2 ... ï¼ŒNï¼‰ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬åˆšåˆšè¿›è¡Œäº†ä¸€äº›å½¢å¼ä¸Šçš„æ“ä½œï¼Œä»¥ä¸€ç§å¯ä»¥ä½¿ç”¨ç»Ÿè®¡åŠ›å­¦å·¥å…·è¿›è¡Œå¤„ç†çš„å½¢å¼æ¥ä¿®é¥°æˆ‘ä»¬çš„åˆå§‹è¡¨è¾¾ã€‚æˆ‘ä»¬è¿˜å¯ä»¥å°†F_Nï¼ˆzï¼‰=-1 / N log Zï¼ˆzï¼‰æ ‡è¯†ä¸ºç³»ç»Ÿçš„è‡ªç”±èƒ½ï¼ˆé€†æ¸©åº¦Î²= 1 / Tï¼‰ï¼›æ³¨æ„ï¼Œä»æ¦‚ç‡è§’åº¦æ¥çœ‹ï¼Œè¿™æ˜¯å…¬å¼ï¼ˆ14ï¼‰ä¸­å®šä¹‰çš„ï¼ˆå¹³å‡ï¼‰ç´¯ç§¯é‡ç”Ÿæˆå‡½æ•°ã€‚åœ¨è¿™ä¸€ç‚¹ä¸Šï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨æ—‹è½¬ä¸å˜çš„é«˜æ–¯æ¦‚ç‡åˆ†å¸ƒå¯¹Mçš„ä¸åŒå®ç°ä¸Šçš„åˆ†è§£å‰‚è½¨è¿¹æ±‚å¹³å‡ï¼š</p><p>  where the first term on the right hand side is a normalization constant and we have used a unit standard deviation scaled by N as Ïƒ = 1/âˆšN and zero mean. This brings us to the first (and only for this post) subtlety I mentioned before: when we take the expectation value of the trace of the resolvent in Eq.(17), we need to evaluate &lt; log Z_M &gt;_M, where &lt; . &gt;_M denotes the expectation w.r.t.  M. The problem is: how do we evaluate the expectation value of the logarithm of a function and what is its meaning? A detailed answer to this question will be given in a follow up post, for the moment let me state few facts here without proof:</p><p>  å…¶ä¸­å³è¾¹çš„ç¬¬ä¸€é¡¹æ˜¯å½’ä¸€åŒ–å¸¸æ•°ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†æŒ‰Nç¼©æ”¾çš„å•ä½æ ‡å‡†åå·®ï¼ŒÏƒ= 1 /âˆšNï¼Œå‡å€¼ä¸ºé›¶ã€‚è¿™å°†æˆ‘ä»¬å¸¦åˆ°äº†æˆ‘ä¹‹å‰æåˆ°çš„ç¬¬ä¸€ä¸ªï¼ˆå¹¶ä¸”ä»…é’ˆå¯¹æ­¤å¸–å­ï¼‰å¾®å¦™ä¹‹å¤„ï¼šå½“æˆ‘ä»¬å–ç­‰å¼ï¼ˆ17ï¼‰ä¸­çš„è§£æç‰©è½¨è¿¹çš„æœŸæœ›å€¼æ—¶ï¼Œæˆ‘ä»¬éœ€è¦è¯„ä¼° _Mï¼Œå…¶ä¸­ _Mè¡¨ç¤ºæœŸæœ›w.r.t. M.é—®é¢˜æ˜¯ï¼šæˆ‘ä»¬å¦‚ä½•è¯„ä¼°å‡½æ•°å¯¹æ•°çš„æœŸæœ›å€¼ï¼Œå®ƒçš„å«ä¹‰æ˜¯ä»€ä¹ˆï¼Ÿæœ‰å…³è¯¥é—®é¢˜çš„è¯¦ç»†ç­”æ¡ˆå°†åœ¨åç»­å¸–å­ä¸­ç»™å‡ºï¼Œæ­¤åˆ»ï¼Œè®©æˆ‘åœ¨è¿™é‡Œé™ˆè¿°ä¸€äº›æ²¡æœ‰è¯æ®çš„äº‹å®ï¼š</p><p> â€¢ &lt; log Z_M &gt;_M and log &lt;Z_M &gt;_M are known respectively as the quenched and annealed averages.</p><p>â€¢ _Må’Œlog  _Måˆ†åˆ«ç§°ä¸ºæ·¬ç«å’Œé€€ç«å¹³å‡å€¼ã€‚</p><p> For classical Random Matrix ensembles, it turns out that &lt;log Z_M &gt;_M = log &lt; Z_M &gt;_M$; the results presented here does give the right answer within a simpler calculation scheme. However, it is not clear why these two type of averages should be equal (or different) for now.</p><p> å¯¹äºç»å…¸çš„éšæœºçŸ©é˜µåˆå¥ï¼Œç»“æœæ˜¯ _M = log  _M $;åœ¨ç®€å•çš„è®¡ç®—æ–¹æ¡ˆä¸­ï¼Œæ­¤å¤„ç»™å‡ºçš„ç»“æœç¡®å®ç»™å‡ºäº†æ­£ç¡®çš„ç­”æ¡ˆã€‚ä½†æ˜¯ï¼Œç›®å‰å°šä¸æ¸…æ¥šä¸ºä»€ä¹ˆè¿™ä¸¤ç§å¹³å‡å€¼åº”è¯¥ç›¸ç­‰ï¼ˆæˆ–ä¸åŒï¼‰ã€‚</p><p> In what follows we evaluate the annealed average of the partition function. In order to get all pre-factors right, is it convenient to write everything in components and separate the diagonal from the off-diagonal part. In doing so, we use the fact that for a symmetric matrix  X we can separate the sum over the elements as:</p><p> æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è¯„ä¼°åˆ†é…å‡½æ•°çš„é€€ç«å¹³å‡å€¼ã€‚ä¸ºäº†ä½¿æ‰€æœ‰å‰ç½®å› å­æ­£ç¡®ï¼Œå°†æ‰€æœ‰å†…å®¹ç¼–å†™åœ¨ç»„ä»¶ä¸­å¹¶å°†å¯¹è§’çº¿ä¸éå¯¹è§’çº¿éƒ¨åˆ†åˆ†å¼€æ˜¯å¦æ–¹ä¾¿ã€‚åœ¨æ­¤è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ä»¥ä¸‹äº‹å®ï¼šå¯¹äºå¯¹ç§°çŸ©é˜µXï¼Œæˆ‘ä»¬å¯ä»¥å°†å…ƒç´ ä¸Šçš„å’Œåˆ†ç¦»ä¸ºï¼š</p><p>  where the second summation spans only the upper triangular part of the matrix and the factor of 2 accounts for the fact that the matrix elements in the lower triangular part are identical. Using this property in both the probability distribution and the resolvent, we can take the expectation value of the diagonal and off-diagonal elements separately and then sum back the two components as follows:</p><p>  å…¶ä¸­ç¬¬äºŒä¸ªæ±‚å’Œä»…è·¨è¶ŠçŸ©é˜µçš„ä¸Šä¸‰è§’éƒ¨åˆ†ï¼Œä¸”å› å­2è§£é‡Šäº†ä¸‹ä¸‰è§’éƒ¨åˆ†ä¸­çš„çŸ©é˜µå…ƒç´ ç›¸åŒçš„äº‹å®ã€‚åœ¨æ¦‚ç‡åˆ†å¸ƒå’Œåˆ†è§£å˜é‡ä¸­éƒ½ä½¿ç”¨æ­¤å±æ€§ï¼Œæˆ‘ä»¬å¯ä»¥åˆ†åˆ«è·å–å¯¹è§’çº¿å…ƒç´ å’Œéå¯¹è§’çº¿å…ƒç´ çš„æœŸæœ›å€¼ï¼Œç„¶åå°†ä¸¤ä¸ªåˆ†é‡æ±‚å’Œå¦‚ä¸‹ï¼š</p><p>  Pay attention to the newly generated non-gaussian term in the exponent proportional to ğœ‘â´; In a physical theory, this term would appear to describe interactions of some sort, e.g. a density-density interaction between â€œspinsâ€ at site  i and  j. More in general, this term describes how the fluctuations of ğœ‘ (the variance) at site  i are correlated to those at site  j. In order to decouple this term we introduce an additional parameter that plays also the role of the order parameter in statistical mechanics. You may think about this as a change of variable in the integral or a generalized Hubbard-Stratonovich transformation in condensed matter theory. The idea is to use the delta function to enforce the scalar â€œconstraintâ€:</p><p>  æ³¨æ„ä¸proportionalæˆæ¯”ä¾‹çš„æŒ‡æ•°ä¸­æ–°ç”Ÿæˆçš„éé«˜æ–¯é¡¹ï¼›åœ¨ç‰©ç†ç†è®ºä¸­ï¼Œè¯¥æœ¯è¯­ä¼¼ä¹ç”¨æ¥æè¿°æŸç§ç›¸äº’ä½œç”¨ï¼Œä¾‹å¦‚ï¼šç«™ç‚¹iå’Œjå¤„â€œæ—‹è½¬â€ä¹‹é—´çš„å¯†åº¦-å¯†åº¦ç›¸äº’ä½œç”¨ã€‚æ›´ä¸€èˆ¬è€Œè¨€ï¼Œè¯¥æœ¯è¯­æè¿°äº†ç«™ç‚¹iå¤„çš„ğœ‘ï¼ˆæ–¹å·®ï¼‰æ³¢åŠ¨ä¸ç«™ç‚¹jå¤„çš„æ³¢åŠ¨å¦‚ä½•ç›¸å…³ã€‚ä¸ºäº†ä½¿è¯¥æœ¯è¯­è§£è€¦ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªé™„åŠ å‚æ•°ï¼Œè¯¥å‚æ•°åœ¨ç»Ÿè®¡åŠ›å­¦ä¸­ä¹Ÿèµ·ç€é˜¶æ•°å‚æ•°çš„ä½œç”¨ã€‚æ‚¨å¯èƒ½ä¼šè®¤ä¸ºè¿™æ˜¯å‡èšæ€ç†è®ºä¸­ç§¯åˆ†æˆ–å¹¿ä¹‰Hubbard-Stratonovichå˜æ¢ä¸­å˜é‡çš„å˜åŒ–ã€‚è¿™ä¸ªæƒ³æ³•æ˜¯ä½¿ç”¨deltaå‡½æ•°æ¥æ‰§è¡Œæ ‡é‡â€œçº¦æŸâ€ï¼š</p><p>  Before jumping into the calculation, let us understand what this parameter means. Certainly, it redefines a quadratic variable as a linear one. Its meaning however, is to measure the fluctuations of ğœ‘ at site  i; clearly, this is not a simple change of variable! Order parameters are central objects in statistical mechanics and quantum field theory, as they are used to study transitions across different phases of the system.</p><p>åœ¨è¿›è¡Œè®¡ç®—ä¹‹å‰ï¼Œè®©æˆ‘ä»¬äº†è§£è¯¥å‚æ•°çš„å«ä¹‰ã€‚å½“ç„¶ï¼Œå®ƒå°†äºŒæ¬¡å˜é‡é‡æ–°å®šä¹‰ä¸ºçº¿æ€§å˜é‡ã€‚ä½†æ˜¯ï¼Œå®ƒçš„å«ä¹‰æ˜¯æµ‹é‡ç«™ç‚¹iå¤„çš„æ³¢åŠ¨ã€‚æ˜¾ç„¶ï¼Œè¿™ä¸æ˜¯ç®€å•çš„å˜é‡æ›´æ”¹ï¼é¡ºåºå‚æ•°æ˜¯ç»Ÿè®¡åŠ›å­¦å’Œé‡å­åœºè®ºä¸­çš„ä¸­å¿ƒå¯¹è±¡ï¼Œå› ä¸ºå®ƒä»¬ç”¨äºç ”ç©¶ç³»ç»Ÿä¸åŒé˜¶æ®µä¹‹é—´çš„è·ƒè¿ã€‚</p><p>  This definitely looks better than our initial expression! In the second line of Eq.(23) we have used the Fourier representation of the Dirac-delta and introduced in this way the scalar Lagrange multiplier Î¾ [6]. This looks like a neat trick, but it is an incredibly useful and far reaching way of enforcing constraints in statistical mechanics and quantum field theory, where in the latter the delta becomes a functional that can be used, e.g. to regularize Gauge theories [7]! Before proceeding, it is convenient to redefine the Lagrange multiplier as follows: Î· = 2 i Î¾/N and perform the Gaussian integral over ğœ‘:</p><p>  è¿™è‚¯å®šæ¯”æˆ‘ä»¬çš„æœ€åˆè¡¨è¾¾æ›´å¥½ï¼åœ¨ç­‰å¼ï¼ˆ23ï¼‰çš„ç¬¬äºŒè¡Œä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ç‹„æ‹‰å…‹ä¸‰è§’æ´²çš„å‚…ç«‹å¶è¡¨ç¤ºï¼Œå¹¶ä»¥æ­¤æ–¹å¼å¼•å…¥äº†æ ‡é‡æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°Î¾[6]ã€‚è¿™çœ‹èµ·æ¥åƒæ˜¯ä¸€ä¸ªå·§å¦™çš„æŠ€å·§ï¼Œä½†æ˜¯å®ƒæ˜¯åœ¨ç»Ÿè®¡åŠ›å­¦å’Œé‡å­åœºè®ºä¸­å¼ºåˆ¶æ‰§è¡Œçº¦æŸçš„ä¸€ç§éå¸¸æœ‰ç”¨ä¸”æ„ä¹‰æ·±è¿œçš„æ–¹æ³•ï¼Œå…¶ä¸­åœ¨åè€…ä¸­ï¼ŒÎ´æˆä¸ºå¯ä»¥ä½¿ç”¨çš„å‡½æ•°ï¼Œä¾‹å¦‚è§„èŒƒé‡è§„ç†è®º[7]ï¼åœ¨ç»§ç»­ä¹‹å‰ï¼Œå¯ä»¥æ–¹ä¾¿åœ°æŒ‰ä»¥ä¸‹æ–¹å¼é‡æ–°å®šä¹‰æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°ï¼šÎ·= 2 iÎ¾/ Nå¹¶åœ¨ğœ‘ä¸Šæ‰§è¡Œé«˜æ–¯ç§¯åˆ†ï¼š</p><p>  In case you wonder, the log Î· term comes from re-exponentiating the result of the Gaussian integral over ğœ‘. We are interested in solving these two integrals in the limit of large N, i.e. we are looking for a continuous approximation to the eigenvalues density. In this limit, the evaluation of the two integrals becomes particularly simple, as we can use the steepest descent method [8]. If you are familiar with the concept of mean-field theory in physics, the steepest descent gives you exactly this result (for the moment just keep this in mind, I will come back on this in another post). The idea is to evaluate the integral in the stationary points of â„’(Î·, q):</p><p>  å¦‚æœæ‚¨æƒ³çŸ¥é“ï¼Œå¯¹æ•°Î·é¡¹æ¥è‡ªå¯¹ä¸Šçš„é«˜æ–¯ç§¯åˆ†çš„ç»“æœæ±‚å¹‚ã€‚æˆ‘ä»¬æœ‰å…´è¶£åœ¨å¤§Nçš„æé™ä¸­æ±‚è§£è¿™ä¸¤ä¸ªç§¯åˆ†ï¼Œå³æˆ‘ä»¬æ­£åœ¨å¯»æ‰¾ç‰¹å¾å€¼å¯†åº¦çš„è¿ç»­è¿‘ä¼¼å€¼ã€‚åœ¨è¿™ä¸ªæé™å†…ï¼Œä¸¤ä¸ªç§¯åˆ†çš„æ±‚å€¼å˜å¾—ç‰¹åˆ«ç®€å•ï¼Œå› ä¸ºæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æœ€é€Ÿä¸‹é™æ³•[8]ã€‚å¦‚æœæ‚¨ç†Ÿæ‚‰ç‰©ç†å­¦ä¸­çš„å‡åœºç†è®ºçš„æ¦‚å¿µï¼Œé‚£ä¹ˆæœ€é™¡å³­çš„ä¸‹é™å°†ä¸ºæ‚¨æä¾›å‡†ç¡®çš„ç»“æœï¼ˆç›®å‰è¯·è®°ä½è¿™ä¸€ç‚¹ï¼Œæˆ‘å°†åœ¨å¦ä¸€ç¯‡æ–‡ç« ä¸­å†æ¬¡ä»‹ç»ï¼‰ã€‚è¿™ä¸ªæƒ³æ³•æ˜¯è¦è¯„ä¼°â„’ï¼ˆÎ·ï¼Œqï¼‰çš„å›ºå®šç‚¹çš„ç§¯åˆ†ï¼š</p><p>    As it is often the case in physics, only one of the two solutions makes sense; in this case we will select the solution giving a finite result. We can now use q* to evaluate the result of the integral and finally the trace of the resolvent in Eq.(15):</p><p>    å°±åƒç‰©ç†å­¦ä¸­ç»å¸¸å‘ç”Ÿçš„é‚£æ ·ï¼Œåªæœ‰ä¸¤ç§è§£å†³æ–¹æ¡ˆä¹‹ä¸€æ‰æœ‰æ„ä¹‰ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°†é€‰æ‹©ç»™å‡ºæœ‰é™ç»“æœçš„è§£ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨q *è¯„ä¼°ç§¯åˆ†çš„ç»“æœï¼Œæœ€åè¯„ä¼°æ–¹ç¨‹å¼ï¼ˆ15ï¼‰ä¸­çš„å¯åˆ†è¾¨ç‰©è½¨è¿¹ï¼š</p><p>  Note that the factor of N in the definition of the trace of the resolvent cancels out with our solution and we obtain a finite and N independent (remember this is a continuous approximation) result. In order to obtain the eigenvalue density, we need to perform first the analytic continuation z â†’ Î» + i Îµ, take the Imaginary part and then the limit Îµ â†’ 0^(+). Note that the order of these operations is important! The tricky part is the imaginary part appearing in the square root; rather than having isolated poles, the square root has a  branch-cut in the complex plane. A neat and simple way to take care of this problem without using advanced complex analysis is given in Ref. [3] by means of the following identity:</p><p>  è¯·æ³¨æ„ï¼Œè§£æå™¨è¿¹çº¿çš„å®šä¹‰ä¸­çš„Nå› å­è¢«æˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆæŠµæ¶ˆï¼Œæˆ‘ä»¬è·å¾—äº†ä¸€ä¸ªæœ‰é™ä¸”ä¸Næ— å…³çš„ç»“æœï¼ˆè¯·è®°ä½ï¼Œè¿™æ˜¯ä¸€ä¸ªè¿ç»­è¿‘ä¼¼ï¼‰ã€‚ä¸ºäº†è·å¾—ç‰¹å¾å€¼å¯†åº¦ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦æ‰§è¡Œè§£æè¿ç»­zâ†’Î»+ iÎµï¼Œå–è™šéƒ¨ï¼Œç„¶åå–æé™Îµâ†’0 ^ï¼ˆ+ï¼‰ã€‚è¯·æ³¨æ„ï¼Œè¿™äº›æ“ä½œçš„é¡ºåºå¾ˆé‡è¦ï¼æ£˜æ‰‹çš„éƒ¨åˆ†æ˜¯å‡ºç°åœ¨å¹³æ–¹æ ¹ä¸­çš„è™šéƒ¨ã€‚å¹³æ–¹æ ¹ä¸æ˜¯å­¤ç«‹çš„æç‚¹ï¼Œè€Œæ˜¯åœ¨å¤å¹³é¢ä¸Šå…·æœ‰åˆ†æ”¯åˆ‡å£ã€‚å‚è€ƒæ–‡çŒ®ä¸­ç»™å‡ºäº†ä¸€ç§æ— éœ€ä½¿ç”¨é«˜çº§å¤æ‚åˆ†æå³å¯è§£å†³æ­¤é—®é¢˜çš„ç®€æ´æ–¹æ³•ã€‚ [3]é€šè¿‡ä»¥ä¸‹èº«ä»½ï¼š</p><p>    Using this result back in the definition of the eigenvalue density, and taking the Îµ â†’ 0^(+) limit, we obtain:</p><p>å°†è¯¥ç»“æœé‡æ–°ç”¨äºç‰¹å¾å€¼å¯†åº¦çš„å®šä¹‰ä¸­ï¼Œå¹¶å–Îµâ†’0 ^ï¼ˆ+ï¼‰æé™ï¼Œæˆ‘ä»¬å¾—åˆ°ï¼š</p><p>  From a direst inspection you can see that if |Î»| &gt; 2 the spectrum is zero (this is a real function!), while is it finite for |Î»|&lt;2. As the spectrum is defined as a positive quantity, the sign of the solution should be chosen as + for positive Î» and - otherwise. We finally obtain the celebrated Wigner semicircle law:</p><p>  ä»é—´æ¥æ£€æŸ¥ä¸­ï¼Œæ‚¨å¯ä»¥çœ‹åˆ°|Î»| > 2ï¼Œé¢‘è°±ä¸ºé›¶ï¼ˆè¿™æ˜¯ä¸€ä¸ªå®å‡½æ•°ï¼ï¼‰ï¼Œè€Œå¯¹äº|Î»| <2åˆ™æ˜¯æœ‰é™çš„ã€‚ç”±äºå°†å…‰è°±å®šä¹‰ä¸ºæ­£æ•°ï¼Œå¯¹äºæ­£æ•°Î»ï¼Œåº”å°†æº¶æ¶²çš„ç¬¦å·é€‰æ‹©ä¸º+ï¼Œå¦åˆ™é€‰æ‹©-ã€‚æˆ‘ä»¬ç»ˆäºè·å¾—äº†è‘—åçš„ç»´æ ¼çº³åŠåœ†å®šå¾‹ï¼š</p><p>  So much sweat for such a little formula :) In the section I will plot the above expression against an explicit numerical evaluation.</p><p>  å¯¹äºè¿™ä¹ˆå°çš„å…¬å¼ï¼ŒçœŸæ˜¯å¤ªè´¹åŠ›äº†ï¼š)åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘å°†æŠŠä¸Šé¢çš„è¡¨è¾¾å¼ä¸æ˜ç¡®çš„æ•°å€¼è¯„ä¼°ä½œå›¾ã€‚</p><p>  To conclude, we check results against a direct numerical evaluation. Details can be found in this G ithub repository, here I will go through the main steps only.</p><p>  æ€»è€Œè¨€ä¹‹ï¼Œæˆ‘ä»¬æ ¹æ®ç›´æ¥çš„æ•°å€¼è¯„ä¼°æ¥æ£€æŸ¥ç»“æœã€‚å¯ä»¥åœ¨æ­¤G ithubå­˜å‚¨åº“ä¸­æ‰¾åˆ°è¯¦ç»†ä¿¡æ¯ï¼Œè¿™é‡Œæˆ‘å°†ä»…ä»‹ç»ä¸»è¦æ­¥éª¤ã€‚</p><p> At the time Wigner solved this problem, it was probably prohibitive to check results numerically if not via lengthy processes on some gym-room size computer. Nowadays you can evaluate explicitly these expressions on your laptop and check they agree with the analytical results.</p><p> å½“ç»´æ ¼çº³è§£å†³äº†è¿™ä¸ªé—®é¢˜æ—¶ï¼Œå¦‚æœä¸é€šè¿‡ä¸€äº›å¥èº«æˆ¿å¤§å°çš„è®¡ç®—æœºè¿›è¡Œå†—é•¿çš„è¿‡ç¨‹ï¼Œå¯èƒ½æ— æ³•è¿›è¡Œæ•°å­—æ£€æŸ¥ç»“æœã€‚å¦‚ä»Šï¼Œæ‚¨å¯ä»¥åœ¨ç¬”è®°æœ¬ç”µè„‘ä¸Šæ˜¾å¼è¯„ä¼°è¿™äº›è¡¨è¾¾å¼ï¼Œå¹¶æ£€æŸ¥å®ƒä»¬æ˜¯å¦ä¸åˆ†æç»“æœä¸€è‡´ã€‚</p><p> One of the problem you face when running simulations is the finite size of the system. Remember that Eq.(31) has been obtained in the limit of large N. But how large is large? How big our simulation needs to be in order to agree with the analytical formula? To answer this question, let us write a function to perform the all evaluation:</p><p>è¿è¡Œæ¨¡æ‹Ÿæ—¶é¢ä¸´çš„é—®é¢˜ä¹‹ä¸€æ˜¯ç³»ç»Ÿçš„å¤§å°æœ‰é™ã€‚è¯·è®°ä½ï¼Œæ–¹ç¨‹ï¼ˆ31ï¼‰æ˜¯åœ¨å¤§Nçš„æé™ä¸­è·å¾—çš„ã€‚ä½†æ˜¯ï¼Œå¤§æœ‰å¤šå°‘å‘¢ï¼Ÿä¸ºäº†ä¸è§£æå…¬å¼ä¸€è‡´ï¼Œæˆ‘ä»¬çš„ä»¿çœŸéœ€è¦å¤šå¤§ï¼Ÿä¸ºäº†å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œè®©æˆ‘ä»¬ç¼–å†™ä¸€ä¸ªå‡½æ•°æ¥æ‰§è¡Œæ‰€æœ‰è¯„ä¼°ï¼š</p><p>   Generate an NÃ—N matrix instance X by sampling from the normal distribution with mean zero and variance Ïƒ = 1/âˆšN.</p><p>   é€šè¿‡ä»å‡å€¼ä¸ºé›¶ä¸”æ–¹å·®Ïƒ= 1 /âˆšNçš„æ­£æ€åˆ†å¸ƒè¿›è¡Œé‡‡æ ·æ¥ç”ŸæˆNÃ—NçŸ©é˜µå®ä¾‹Xã€‚</p><p>  Get the eigenvalues using  numpy.eigvalsh(Xs) note that this is a numerically optimized method to obtain eigenvalues of symmetric matrices.</p><p>  ä½¿ç”¨numpy.eigvalshï¼ˆXsï¼‰è·å¾—ç‰¹å¾å€¼è¯·æ³¨æ„ï¼Œè¿™æ˜¯è·å¾—å¯¹ç§°çŸ©é˜µç‰¹å¾å€¼çš„æ•°å€¼ä¼˜åŒ–æ–¹æ³•ã€‚</p><p>   You can see two things here: one is that the tails of the numerical distrbution exceeds the domain [-2,2] of the bulk semicircle. And two is that the bins do not agree too well with the analytical expression. We can reperat the evaluation for N=500:</p><p>   æ‚¨å¯ä»¥åœ¨æ­¤å¤„çœ‹åˆ°ä¸¤ä»¶äº‹ï¼šä¸€æ˜¯æ•°å­—åˆ†å¸ƒçš„å°¾éƒ¨è¶…è¿‡äº†æ•´ä½“åŠåœ†çš„åŸŸ[-2,2]ã€‚äºŒæ˜¯åƒåœ¾ç®±ä¸è§£æè¡¨è¾¾å¼ä¸å¤ªå»åˆã€‚æˆ‘ä»¬å¯ä»¥å¯¹N = 500è¿›è¡Œè¯„ä¼°ï¼š</p><p>  You can see that the results gets definitely better. We can evaluate the error between the analytical and numerical values as a function of the matrix size. Here I compute the Mean Square Root error (MSRE):</p><p>  æ‚¨å¯ä»¥çœ‹åˆ°ç»“æœè‚¯å®šä¼šæ›´å¥½ã€‚æˆ‘ä»¬å¯ä»¥æ ¹æ®çŸ©é˜µå¤§å°è¯„ä¼°è§£æå€¼å’Œæ•°å€¼ä¹‹é—´çš„è¯¯å·®ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘è®¡ç®—å‡æ–¹æ ¹è¯¯å·®ï¼ˆMSREï¼‰ï¼š</p><p>  In the above equation, the first eigenvalue density is the one evaluated numerically, while the second is the expression we have evaluated analytically, while S is the number of samples. In the plot below I separate the tails from the bulk error:</p><p>åœ¨ä¸Šé¢çš„æ–¹ç¨‹ä¸­ï¼Œç¬¬ä¸€ä¸ªç‰¹å¾å€¼å¯†åº¦æ˜¯ç”¨æ•°å­—è®¡ç®—çš„ä¸€ä¸ªï¼Œè€Œç¬¬äºŒä¸ªç‰¹å¾å€¼å¯†åº¦æ˜¯æˆ‘ä»¬ç»è¿‡åˆ†æè¯„ä¼°çš„è¡¨è¾¾å¼ï¼Œè€ŒSæ˜¯æ ·æœ¬æ•°ã€‚åœ¨ä¸‹é¢çš„å›¾ä¸­ï¼Œæˆ‘å°†å°¾éƒ¨é”™è¯¯ä¸æ‰¹é‡é”™è¯¯åˆ†å¼€ï¼š</p><p>  You can see how the error goes down to ~ 0 (you can add more points to get a better result here) as the matrix size increases, providing in this way a nice numerical validation of our calculation above.</p><p>  æ‚¨å¯ä»¥çœ‹åˆ°éšç€çŸ©é˜µå¤§å°çš„å¢åŠ ï¼Œè¯¯å·®å¦‚ä½•é™ä½åˆ°ã€œ0ï¼ˆå¯ä»¥åœ¨æ­¤å¤„æ·»åŠ æ›´å¤šç‚¹ä»¥è·å¾—æ›´å¥½çš„ç»“æœï¼‰ï¼Œä»è€Œä»¥ä¸Šè¿°æ–¹å¼ä¸ºæˆ‘ä»¬çš„è®¡ç®—æä¾›äº†å¾ˆå¥½çš„æ•°å€¼éªŒè¯ã€‚</p><p>  In this article I presented a simple step-by-step introduction to Random Matrix theory, and in particular the Gaussian Orthogonal Ensemble. The calculation I presented is not the standard one you will find in books, the rea</p><p>  åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ç®€å•ä»‹ç»äº†éšæœºçŸ©é˜µç†è®ºï¼Œç‰¹åˆ«æ˜¯é«˜æ–¯æ­£äº¤åˆå¥ã€‚æˆ‘æå‡ºçš„è®¡ç®—æ–¹æ³•ä¸æ˜¯æ‚¨åœ¨ä¹¦æœ¬ä¸­æ‰¾åˆ°çš„æ ‡å‡†ï¼Œå®é™…</p><p>......</p><p>......</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://medium.com/cantors-paradise/getting-started-with-random-matrices-a-step-by-step-guide-81e5902384e">https://medium.com/cantors-paradise/getting-started-with-random-matrices-a-step-by-step-guide-81e5902384e</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/çŸ©é˜µ/">#çŸ©é˜µ</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/step/">#step</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è®¾è®¡/">#è®¾è®¡</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/åˆ›æ„/">#åˆ›æ„</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/æ‘„å½±/">#æ‘„å½±</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/å›¾ç‰‡/">#å›¾ç‰‡</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/æ¸¸æˆ/">#æ¸¸æˆ</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è½¯ä»¶/">#è½¯ä»¶</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è§†é¢‘/">#è§†é¢‘</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/æ‰‹æœº/">#æ‰‹æœº</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/å¹¿å‘Š/">#å¹¿å‘Š</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/ç½‘ç«™/">#ç½‘ç«™</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/å…è´¹/">#å…è´¹</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/ä¸‹è½½/">#ä¸‹è½½</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è‹¹æœ/">#è‹¹æœ</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/å¾®è½¯/">#å¾®è½¯</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/éŸ³ä¹/">#éŸ³ä¹</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/åšå®¢/">#åšå®¢</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/æ¶æ/">#æ¶æ</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è‰ºæœ¯/">#è‰ºæœ¯</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/å·¥å…·/">#å·¥å…·</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/åˆ†äº«/">#åˆ†äº«</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>