<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>è®©ä½ çš„ç¾¤é›†æ¸¸æ³³ï¼ˆ2020ï¼‰ Make your cluster SWIM (2020)</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Make your cluster SWIM (2020)<br/>è®©ä½ çš„ç¾¤é›†æ¸¸æ³³ï¼ˆ2020ï¼‰ </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-06-29 22:34:55</div><div class="page_narrow text-break page_content"><p>In this blog post we&#39;ll cover how systems form clusters, what clusters actually are and what are their responsibilities. We&#39;ll also present different protocols responsible to serve the needs of the clusters with a various tradeoffs associated with them.</p><p>åœ¨è¿™ä¸ªåšå®¢æ–‡ç« ä¸­æˆ‘ä»¬ï¼†ï¼ƒ39; llæ¶µç›–ç³»ç»Ÿå¦‚ä½•å½¢æˆé›†ç¾¤ï¼Œå®é™…ä¸Šæ˜¯ä»€ä¹ˆé›†ç¾¤ä»¥åŠä»–ä»¬çš„èŒè´£æ˜¯ä»€ä¹ˆã€‚æˆ‘ä»¬ï¼†ï¼ƒ39; LLè¿˜å‘ˆç°äº†ä¸åŒçš„åè®®ï¼Œä»¥æ»¡è¶³ä¸ä»–ä»¬ç›¸å…³çš„å„ç§æƒè¡¡çš„ç¾¤é›†çš„éœ€æ±‚ã€‚</p><p> To make this blog post a bit more practical, we&#39;ll also go in depth of one of the membership protocols - known as under SWIM acronym - from theoretical standpoint up to example implementation in F#.</p><p> ä¸ºäº†ä½¿è¿™ä¸ªåšå®¢å‘è¡¨ä¸€ç‚¹æ›´å®ç”¨ï¼Œæˆ‘ä»¬ä¹Ÿä¼šæ·±å…¥äº†è§£äº†ä¼—è®®å‘˜åè®®ä¹‹ä¸€ - ä»æ¸¸æ³³é¦–å­—æ¯ç¼©ç•¥è¯ä¸­çš„ä¸€ä¸ªéš¶å±åè®® - ä»ç†è®ºä¸Šçš„è§’åº¦æ¥çœ‹ï¼Œåœ¨Fï¼ƒä¸­å®ç°äº†ç¤ºä¾‹ã€‚</p><p>  From a user perspective, cluster is supposed to serve as an illusion of a &#34;single machine&#34; and keep it  safe from all of complexity coming from communicating with - usually dynamically changing - network of interconnected servers.</p><p>  ä»ç”¨æˆ·çš„è§’åº¦æ¥çœ‹ï¼Œç¾¤é›†åº”è¯¥ç”¨ä½œAï¼†ï¼ƒ34;å•æœºï¼†ï¼ƒ34çš„å¹»è§‰;å¹¶å°†å…¶å…äºä¸ - é€šå¸¸åŠ¨æ€å˜åŒ– - äº’è”æœåŠ¡å™¨ç½‘ç»œçš„æ‰€æœ‰å¤æ‚æ€§å®‰å…¨ã€‚</p><p> As it turns out, there&#39;s a multitude of protocols and responsibilities that services being a part of cluster need to solve to maintain that illusion. Going bottom-up, here are some of the questions our cluster usually needs to know how to answer for:</p><p> æ­£å¦‚äº‹å®è¯æ˜ï¼Œé‚£é‡Œçš„ä¸€ç³»åˆ—åè®®å’Œè´£ä»»æ˜¯ç¾¤é›†çš„ä¸€éƒ¨åˆ†éœ€è¦è§£å†³ä»¥ç»´æŒè¿™ç§å¹»è§‰ã€‚èµ°ä¸‹è‡ªä¸‹è€Œä¸Šï¼Œè¿™é‡Œæ˜¯æˆ‘ä»¬é›†ç¾¤é€šå¸¸éœ€è¦çŸ¥é“å¦‚ä½•å›ç­”çš„ä¸€äº›é—®é¢˜ï¼š</p><p> How to join the cluster? Usually when we have a new server node, which we want to join to the cluster, it needs to know how to communicate with another node that&#39;s already part of the cluster itself. Where can it find that information? It&#39;s the question that needs to be answered: The simplest trick is to provide a static list of contact points as a configuration parameter provided at the start of new service.</p><p> å¦‚ä½•åŠ å…¥ç¾¤é›†ï¼Ÿé€šå¸¸ï¼Œå½“æˆ‘ä»¬æœ‰ä¸€ä¸ªæˆ‘ä»¬æƒ³è¦åŠ å…¥ç¾¤é›†çš„æ–°æœåŠ¡å™¨èŠ‚ç‚¹æ—¶ï¼Œå®ƒéœ€è¦çŸ¥é“å¦‚ä½•ä¸å·²æœ‰ç¾¤é›†æœ¬èº«çš„ä¸€éƒ¨åˆ†çš„å¦ä¸€ä¸ªèŠ‚ç‚¹é€šä¿¡ã€‚å“ªé‡Œå¯ä»¥æ‰¾åˆ°è¿™ä¸ªä¿¡æ¯ï¼Ÿå®ƒï¼†ï¼ƒ39;æ˜¯éœ€è¦å›ç­”çš„é—®é¢˜ï¼šæœ€ç®€å•çš„è¯€çªæ˜¯ä¸ºæ–°æœåŠ¡å¼€å§‹æ—¶æä¾›çš„é…ç½®å‚æ•°æä¾›é™æ€çš„è”ç³»ç‚¹åˆ—è¡¨ã€‚</p><p> Another way is to use 3rd party service (like database,  Consul,  etcd or  ZooKeeper) to serve as a node registry. Our cluster usually doesn&#39;t live in vacuum, and sometimes we can reuse other already established services for our advantage.</p><p> å¦ä¸€ç§æ–¹æ³•æ˜¯ä½¿ç”¨ç¬¬ä¸‰æ–¹æœåŠ¡ï¼ˆå¦‚æ•°æ®åº“ï¼ŒScenulï¼Œetcdæˆ–zookeeperï¼‰ç”¨ä½œèŠ‚ç‚¹æ³¨å†Œè¡¨ã€‚æˆ‘ä»¬çš„é›†ç¾¤é€šå¸¸ä¸ä¼šç”Ÿæ´»åœ¨çœŸç©ºä¸­ï¼Œæœ‰æ—¶æˆ‘ä»¬å¯ä»¥é‡å¤ä½¿ç”¨å…¶ä»–å·²ç»å»ºç«‹çš„æœåŠ¡ä»¥è·å¾—ä¼˜åŠ¿ã€‚</p><p> In some situations, we could leverage capabilities of lower layers - like  Kubernetes DNS service or  mDNS - that are specific to a host environment to dynamically discover other devices living in the same network.</p><p> åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨ä½å±‚Kubernetes DNSæœåŠ¡æˆ–MDNSçš„åŠŸèƒ½ - ç‰¹å®šäºä¸»æœºç¯å¢ƒï¼Œä»¥åŠ¨æ€å‘ç°ç”Ÿæ´»åœ¨åŒä¸€ç½‘ç»œä¸­çš„å…¶ä»–è®¾å¤‡ã€‚ </p><p> How do we know what other nodes are part of the cluster? This is area of so called membership protocols on which we&#39;ll focus in second part of this blog post. In dynamic clusters it&#39;s usually done by keeping track of a active discovered nodes, then updating and gossiping them once a node joins/leaves the cluster. A lot of decisions here depend on deployment scenario - services forming a cluster within the same datacenter have different characteristics from eg. meshes of mobile devices. In cases of common backend services hosted in the same datacenter, each node keeps a full information about the state of the cluster. This is how protocols like  SWIM operate.</p><p>æˆ‘ä»¬å¦‚ä½•çŸ¥é“å…¶ä»–èŠ‚ç‚¹æ˜¯ç¾¤é›†çš„ä¸€éƒ¨åˆ†ï¼Ÿè¿™æ˜¯æ‰€è°“çš„ä¼šå‘˜åè®®çš„åŒºåŸŸï¼Œæˆ‘ä»¬ï¼†ï¼ƒ39; llç„¦ç‚¹åœ¨æœ¬åšå®¢æ–‡ç« çš„ç¬¬äºŒéƒ¨åˆ†ã€‚åœ¨åŠ¨æ€é›†ç¾¤ä¸­ï¼Œå®ƒé€šå¸¸é€šè¿‡è·Ÿè¸ªæ´»åŠ¨å‘ç°çš„èŠ‚ç‚¹æ¥å®Œæˆï¼Œç„¶ååœ¨èŠ‚ç‚¹è¿æ¥/ç¦»å¼€ç¾¤é›†æ—¶æ›´æ–°å’Œé—²ç½®å®ƒä»¬ã€‚è¿™é‡Œçš„è®¸å¤šå†³å®šä¾èµ–äºéƒ¨ç½²æ–¹æ¡ˆ - åœ¨åŒä¸€æ•°æ®ä¸­å¿ƒå†…å½¢æˆç¾¤é›†çš„æœåŠ¡å…·æœ‰ä¸åŒçš„ç‰¹å¾ï¼Œä»ä¾‹å¦‚egã€‚ç§»åŠ¨è®¾å¤‡çš„ç½‘æ ¼ã€‚åœ¨æ‰˜ç®¡åœ¨åŒä¸€æ•°æ®ä¸­å¿ƒæ‰˜ç®¡çš„å¸¸è§åç«¯æœåŠ¡çš„æƒ…å†µä¸‹ï¼Œæ¯ä¸ªèŠ‚ç‚¹éƒ½ä¼šä¿æŒæœ‰å…³ç¾¤é›†çŠ¶æ€çš„å®Œæ•´ä¿¡æ¯ã€‚è¿™å°±æ˜¯æ¸¸æ³³æ“ä½œçš„åè®®ã€‚</p><p> Other membership protocols (like  HyParView) enable to have only partial view on the cluster. This is preferable in cases when our cluster operates on much higher scale eg. thousands of nodes (usually most clustered services living in datacenters don&#39;t reach over dozens-to-hundreds of servers).</p><p> å…¶ä»–æˆå‘˜èµ„æ ¼åè®®ï¼ˆå¦‚hyparviewï¼‰å¯ç”¨åªèƒ½åœ¨ç¾¤é›†ä¸­æ˜ åƒã€‚å¦‚æœæˆ‘ä»¬çš„ç¾¤é›†åœ¨æ›´é«˜çš„å°ºåº¦ä¸Šæ“ä½œæ—¶ï¼Œåˆ™è¿™æ˜¯ä¼˜é€‰çš„ã€‚æˆåƒä¸Šä¸‡çš„èŠ‚ç‚¹ï¼ˆé€šå¸¸ç”Ÿæ´»åœ¨æ•°æ®ä¸­å¿ƒçš„å¤§å¤šæ•°é›†ç¾¤æœåŠ¡Donï¼†ï¼ƒ39; tè¾¾åˆ°å‡ åå¤šä¸ªæœåŠ¡å™¨ï¼‰ã€‚</p><p> How do we send messages from one node to another? This usually is also related to a membership protocol. Most datacenter-oriented systems can make conservative assumption that every node in the system can connect to every other node, forming (potentially) a fully connected mesh.</p><p> æˆ‘ä»¬å¦‚ä½•å°†æ¶ˆæ¯ä»ä¸€ä¸ªèŠ‚ç‚¹å‘é€åˆ°å¦ä¸€ä¸ªèŠ‚ç‚¹ï¼Ÿè¿™é€šå¸¸ä¹Ÿä¸éš¶å±åè®®æœ‰å…³ã€‚å¤§å¤šæ•°ä»¥æ•°æ®ä¸­å¿ƒä¸ºå¯¼å‘çš„ç³»ç»Ÿå¯ä»¥ä¿å®ˆå‡è®¾ç³»ç»Ÿä¸­æ¯ä¸ªèŠ‚ç‚¹éƒ½å¯ä»¥è¿æ¥åˆ°æ¯ä¸ªå…¶ä»–èŠ‚ç‚¹ï¼Œå½¢æˆï¼ˆæ½œåœ¨åœ°ï¼‰å®Œå…¨è¿æ¥çš„ç½‘æ ¼ã€‚</p><p> In other scenarios, we need to take into account that some of the nodes may not be able to connect to others - because of the underlying network characteristics. While the most common case of that is client-server architecture, when we&#39;re talking about cluster protocols, often more advanced scenarios need to be applied (as we mentioned, &#34;server&#34; itself is not a single entity).</p><p> åœ¨å…¶ä»–åœºæ™¯ä¸­ï¼Œæˆ‘ä»¬éœ€è¦è€ƒè™‘åˆ°æŸäº›èŠ‚ç‚¹å¯èƒ½æ— æ³•è¿æ¥åˆ°å…¶ä»–èŠ‚ç‚¹ - å› ä¸ºç½‘ç»œç‰¹å¾æ½œåœ¨çš„ç½‘ç»œç‰¹å¾ã€‚è™½ç„¶å½“æˆ‘ä»¬è°ˆè®ºç¾¤é›†åè®®æ—¶ï¼Œæœ€å¸¸è§çš„æƒ…å†µæ˜¯å®¢æˆ·æœåŠ¡å™¨æ¶æ„ï¼Œä½†å¸¸å¸¸éœ€è¦åº”ç”¨æ›´é«˜çº§çš„æ–¹æ¡ˆï¼ˆå¦‚æˆ‘ä»¬æ‰€æåˆ°çš„ï¼Œï¼†ï¼ƒ34;æœåŠ¡å™¨ï¼†ï¼ƒ34;æœ¬èº«ä¸æ˜¯å•ä¸€çš„å®ä½“ï¼‰ã€‚</p><p> How do we detect dead nodes? Also known as failure detection. The oldest known trick in that area is a simple exchange of PING ğŸ¡˜ ACK messages within expected timeout boundaries every now and then, or simply expecting every connection to send a heartbeat message within given time interval. Heartbeats are often implemented directly into transport layer (like TCP), sometimes we&#39;re able to piggyback failure detector directly on top of it. Downside of that solution is that while transport layer itself is responsive - because it&#39;s usually managed by underlying OS - our application layer could in fact be not (eg. because it hangs deadlocked indefinitely). Another thing is that temporal failure of network connection doesn&#39;t has to mean, that node won&#39;t try to restart it and continue to work.</p><p> æˆ‘ä»¬å¦‚ä½•æ£€æµ‹åˆ°æ­»åŒºï¼Ÿä¹Ÿç§°ä¸ºå¤±è´¥æ£€æµ‹ã€‚è¯¥é¢†åŸŸæœ€å¤è€çš„çŸ¥è¯†æ˜¯æ¯ç«‹å³åœ¨é¢„æœŸè¶…æ—¶è¾¹ç•Œå†…çš„PingğŸ¡˜ACKæ¶ˆæ¯çš„ç®€å•äº¤æ¢ï¼Œæˆ–è€…åªæ˜¯æœŸæœ›åœ¨ç»™å®šæ—¶é—´é—´éš”å†…å‘é€å¿ƒè·³æ¶ˆæ¯ã€‚å¿ƒè·³é€šå¸¸ç›´æ¥è¿›å…¥è¿è¾“å±‚ï¼ˆå¦‚TCPï¼‰ï¼Œæœ‰æ—¶æ˜¯æˆ‘ä»¬ï¼†ï¼ƒ39;é‡æ–°èƒ½å¤Ÿç›´æ¥æå¸¦å¤±è´¥æ¢æµ‹å™¨ã€‚è¯¥è§£å†³æ–¹æ¡ˆçš„ç¼ºç‚¹æ˜¯ï¼Œè™½ç„¶è¿è¾“å±‚æœ¬èº«æ˜¯å“åº”çš„ - å› ä¸ºå®ƒé€šå¸¸ç”±åº•å±‚æ“ä½œç³»ç»Ÿç®¡ç† - æˆ‘ä»¬çš„åº”ç”¨å±‚å®é™…ä¸Šä¸æ˜¯ï¼ˆä¾‹å¦‚ï¼Œå› ä¸ºå®ƒæ— é™æœŸåœ°æŒ‚èµ·äº†åƒµåŒ–çš„åƒµå±€ï¼‰ã€‚å¦ä¸€ä»¶äº‹æ˜¯ç½‘ç»œè¿æ¥çš„æ—¶é—´å¤±è´¥å¹¶ä¸æ˜¯æ„å‘³ç€ï¼Œé‚£ä¸ªèŠ‚ç‚¹èµ¢å¾—äº†ï¼†ï¼ƒ39; tè¯•ç€é‡æ–°å¯åŠ¨å¹¶ç»§ç»­å·¥ä½œã€‚</p><p> More often, membership protocols promote their own heartbeat algorithms. What&#39;s worth noticing here, missing heartbeat doesn&#39;t necessarily mean, that our node is dead. It could as well be overwhelmed with serving other incoming requests. Modern algorithms like  Phi accural failure dector or  Lifeguard (an extension to SWIM protocol mentioned above) take that behavior into account.</p><p> æ›´å¸¸è§çš„æ˜¯ï¼Œä¼šå‘˜åè®®ä¿ƒè¿›è‡ªå·±çš„å¿ƒè·³ç®—æ³•ã€‚ä»€ä¹ˆï¼†ï¼ƒ39;åœ¨è¿™é‡Œå€¼å¾—æ³¨æ„ï¼Œç¼ºå°‘å¿ƒè·³å¹¶ä¸ä¸€å®šæ„å‘³ç€ï¼Œæˆ‘ä»¬çš„èŠ‚ç‚¹å·²ç»æ­»äº¡ã€‚å®ƒä¹Ÿå¯èƒ½ä¸å ªé‡è´Ÿï¼Œæä¾›å…¶ä»–ä¼ å…¥è¯·æ±‚ã€‚åƒPhi Accuctuct Dectoræˆ–Lifeguardè¿™æ ·çš„ç°ä»£ç®—æ³•ï¼ˆä¸Šé¢æåˆ°çš„æ¸¸æ³³åè®®çš„æ‰©å±•ï¼‰è€ƒè™‘åˆ°è¿™ä¸€è¡Œä¸ºã€‚</p><p> These are the most basic questions cluster needs to know how to answer. We could call it a layer 0 of any cluster. The main part of this blog post will cover how to implement protocol addressing these issues. On top of it, there are many other higher level features, aiming to solve problems like:</p><p> è¿™äº›æ˜¯æœ€åŸºæœ¬çš„é—®é¢˜éœ€è¦çŸ¥é“å¦‚ä½•å›ç­”ã€‚æˆ‘ä»¬å¯ä»¥ç§°ä¹‹ä¸ºä»»ä½•ç¾¤é›†çš„ç¬¬0å±‚ã€‚æœ¬åšå®¢æ–‡ç« çš„ä¸»è¦éƒ¨åˆ†å°†æ¶µç›–å¦‚ä½•å®æ–½è§£å†³è¿™äº›é—®é¢˜çš„åè®®ã€‚åœ¨å®ƒä¹‹ä¸Šï¼Œè¿˜æœ‰è®¸å¤šå…¶ä»–æ›´é«˜çš„çº§åˆ«åŠŸèƒ½ï¼Œæ—¨åœ¨è§£å†³ä»¥ä¸‹é—®é¢˜ï¼š </p><p> How can we detect/respond to periodic network partitions? A problem often known as  split-brain scenario. It comes from basic observation that  it&#39;s not possible to differentiate dead node from unresponsive one over network boundary. It can lead to very risky situations, like splitting our cluster in two, each one believing, it&#39;s the only one alive and causing data inconsistency or even corruption. There&#39;s no one simple cure for that, that&#39;s why some systems (like  Akka.NET split brain resolvers) offer different strategies depending on the tradeoffs, that we care for.</p><p>æˆ‘ä»¬å¦‚ä½•æ£€æµ‹/å“åº”å®šæœŸç½‘ç»œåˆ†åŒºï¼Ÿé€šå¸¸è¢«ç§°ä¸ºåˆ†è£‚æ€§æƒ…æ™¯çš„é—®é¢˜ã€‚å®ƒæ¥è‡ªåŸºæœ¬è§‚å¯Ÿï¼Œå³å®ƒï¼†ï¼ƒ39;â– ä¸å¯èƒ½åœ¨ç½‘ç»œè¾¹ç•Œä¸­åŒºåˆ†æ­»åŒºã€‚å®ƒå¯ä»¥å¯¼è‡´é£é™©éå¸¸å±é™©çš„æƒ…å†µï¼Œæ¯”å¦‚å°†æˆ‘ä»¬çš„é›†ç¾¤åˆ†æˆä¸¤ä¸ªï¼Œæ¯ä¸ªäººéƒ½ç›¸ä¿¡ï¼Œå®ƒï¼†ï¼ƒ39;å”¯ä¸€ä¸€ä¸ªæ´»ç€ï¼Œå¯¼è‡´æ•°æ®ä¸ä¸€è‡´ç”šè‡³è…è´¥ã€‚æ²¡æœ‰ä¸€ä¸ªç®€å•çš„æ²»ç–—æ–¹æ³•ï¼Œé‚£å°±æ˜¯ä¸ºä»€ä¹ˆä¸€äº›ç³»ç»Ÿï¼ˆå¦‚akka.netåˆ†è£‚å¤§è„‘resolversï¼‰æ ¹æ®æˆ‘ä»¬å…³å¿ƒçš„æƒè¡¡æä¾›ä¸åŒçš„ç­–ç•¥ã€‚</p><p> How different nodes can reason and decide about the state in the cluster? This case is usually common in systems that are responsible for data management (like distributed databases). Since nodes must be able to serve incoming requests, sometimes they may possibly run into making conflicting decisions about the state of the system. Without going into too much details, two generic approaches two this problem are: Avoiding conflicts, which usually assumes that nodes must establish consensus about state of the system before committing to their decisions. It usually requires establishing (and maintaining) some leader among the nodes, synchronizing via quorum of nodes or combination of two. This is area of popular protocols such as  Raft,  ZAB or Paxos.</p><p> ä¸åŒçš„èŠ‚ç‚¹å¦‚ä½•æ¨ç†å’Œå†³å®šç¾¤é›†ä¸­çš„çŠ¶æ€ï¼Ÿè¿™ç§æƒ…å†µé€šå¸¸åœ¨è´Ÿè´£æ•°æ®ç®¡ç†ï¼ˆå¦‚åˆ†å¸ƒå¼æ•°æ®åº“ï¼‰çš„ç³»ç»Ÿä¸­æ˜¯å¸¸è§çš„ã€‚ç”±äºèŠ‚ç‚¹å¿…é¡»èƒ½å¤Ÿä¸ºä¼ å…¥è¯·æ±‚æä¾›æœåŠ¡ï¼Œå› æ­¤æœ‰æ—¶å®ƒä»¬å¯â€‹â€‹èƒ½ä¼šé‡åˆ°å¯¹ç³»ç»ŸçŠ¶æ€çš„å†²çªå†³ç­–ã€‚åœ¨æ²¡æœ‰è¿‡å¤šçš„ç»†èŠ‚ä¸­ï¼Œä¸¤ä¸ªé€šç”¨æ–¹æ³•ä¸¤ä¸ªé—®é¢˜æ˜¯ï¼šé¿å…å†²çªï¼Œè¿™é€šå¸¸å‡è®¾èŠ‚ç‚¹å¿…é¡»åœ¨æäº¤å†³ç­–ä¹‹å‰å»ºç«‹å…³äºç³»ç»ŸçŠ¶æ€çš„å…±è¯†ã€‚å®ƒé€šå¸¸éœ€è¦å»ºç«‹ï¼ˆå’Œç»´æŠ¤ï¼‰èŠ‚ç‚¹ä¹‹é—´çš„ä¸€äº›é¢†å¯¼è€…ï¼Œé€šè¿‡ä»²è£èŠ‚ç‚¹æˆ–ä¸¤è€…ç»„åˆåŒæ­¥ã€‚è¿™æ˜¯ç­ï¼ŒZABæˆ–Paxoç­‰æµè¡Œåè®®çš„åŒºåŸŸã€‚</p><p> Resolving conflicts which accepts the possibility of state conflict to appear - mostly as a tradeoff happening in face of long latencies or periodically unreachable servers - but enriches it with enough metadata so that all nodes individually can reach the same conclusion about the result state without need of consensus. This is a dominant area of Conflict Free Replicated Data Types, which you could read about eg. in a  blog post series published here.</p><p> è§£å†³æ¥å—å›½å®¶å†²çªçš„å¯èƒ½æ€§å‡ºç°çš„å†²çª - ä¸»è¦æ˜¯åœ¨é•¿æœŸå»¶è¿Ÿæˆ–å‘¨æœŸæ€§åœ°å‘ç”Ÿçš„æƒè¡¡æˆ–å®šæœŸæ— æ³•è®¿é—®çš„æœåŠ¡å™¨ä¸­è¿›è¡Œçš„æƒè¡¡ - ä½†æ˜¯ä»¥è¶³å¤Ÿçš„å…ƒæ•°æ®ä¸°å¯Œï¼Œä½¿å¾—æ‰€æœ‰èŠ‚ç‚¹å¯ä»¥å•ç‹¬è¾¾åˆ°ä¸ç»“æœçŠ¶æ€çš„ç›¸åŒç»“è®ºå…±è¯†ã€‚è¿™æ˜¯ä¸€ä¸ªä¸»å¯¼åœ°åŒºçš„å…é™¤è‡ªç”±å¤åˆ¶æ•°æ®ç±»å‹ï¼Œæ‚¨å¯ä»¥è¯»å–egã€‚åœ¨è¿™é‡Œå‘å¸ƒçš„åšå®¢æ–‡ç« ä¸­ã€‚</p><p> How to route requests to a given resource inside of cluster? The state of our system usually consists of multiple addressable entities - which are often replicated for higher availability and resiliency. However usually the entire state is too big to fit into any single node. For this reason it&#39;s often partitioned all over the cluster  dynamically. Now this begs a question: how to tell, which node contains an entity identified by some virtual key? Naive approach would be to ask some subset of nodes in hope that at least one of them will have a data we hope for. Given cluster of  N nodes and entity replicated  R times, we should be able to reach our resource after calling  (N/R)+1 nodes.</p><p> å¦‚ä½•å°†è¯·æ±‚è·¯ç”±åˆ°é›†ç¾¤å†…éƒ¨çš„ç»™å®šèµ„æºï¼Ÿæˆ‘ä»¬çš„ç³»ç»ŸçŠ¶æ€é€šå¸¸ç”±å¤šç§å¯å¯»å€å®ä½“ç»„æˆ - é€šå¸¸å¤åˆ¶ï¼Œä»¥ä¾¿æ›´é«˜çš„å¯ç”¨æ€§å’Œå¼¹æ€§ã€‚ç„¶è€Œï¼Œé€šå¸¸æ•´ä¸ªçŠ¶æ€å¤ªå¤§ï¼Œæ— æ³•é€‚åˆä»»ä½•ä¸€ä¸ªèŠ‚ç‚¹ã€‚å‡ºäºè¿™ä¸ªåŸå› ï¼Œå®ƒç»å¸¸åŠ¨æ€åœ°åœ¨æ•´ä¸ªç¾¤é›†ä¸­åˆ’åˆ†ã€‚ç°åœ¨æ³æ±‚ä¸€ä¸ªé—®é¢˜ï¼šå¦‚ä½•å‘Šè¯‰ï¼Œå“ªä¸ªèŠ‚ç‚¹åŒ…å«ä¸€äº›è™šæ‹Ÿé”®æ ‡è¯†çš„å®ä½“ï¼Ÿå¤©çœŸçš„æ–¹æ³•æ˜¯è¯¢é—®ä¸€äº›èŠ‚ç‚¹çš„å­é›†å¸Œæœ›ï¼Œå…¶ä¸­è‡³å°‘æœ‰ä¸€ä¸ªå°†æœ‰æˆ‘ä»¬å¸Œæœ›çš„æ•°æ®ã€‚ç»™å®šç¾¤é›†çš„Nä¸ªèŠ‚ç‚¹å’Œå®ä½“å¤åˆ¶äº†Ræ¬¡ï¼Œæˆ‘ä»¬åº”è¯¥èƒ½å¤Ÿåœ¨è°ƒç”¨ï¼ˆn / rï¼‰+1èŠ‚ç‚¹ååˆ°è¾¾æˆ‘ä»¬çš„èµ„æºã€‚</p><p> More common way is to keep a registry having an information about current localization of every single entity in a system. Since this approach doesn&#39;t scale well, in practice we group and co-locate entities together within partitions and therefore compress the registry to store information about entire partition rather than individual entity. In this case resource ID is composite key of  (partitionID, entityID). This is how eg.  Akka.Cluster.Sharding or  riak core works. Frequently some subset of hot (frequently used) partitions may be cached on each node for to reduce asking central registry or even the registry itself may be a replicated store.</p><p> æ›´å¸¸è§çš„æ–¹å¼æ˜¯ä¿ç•™æ³¨å†Œè¡¨ï¼Œè¯¥æ³¨å†Œè¡¨å…·æœ‰æœ‰å…³ç³»ç»Ÿä¸­æ¯ä¸ªå•ä¸ªå®ä½“çš„å½“å‰æœ¬åœ°åŒ–çš„ä¿¡æ¯ã€‚ç”±äºè¿™ç§æ–¹æ³•å¹¶ä¸å±•ç¤ºï¼Œåœ¨å®è·µä¸­ï¼Œæˆ‘ä»¬åœ¨å®è·µä¸­ï¼Œåœ¨åˆ†åŒºä¸­ç»„åˆå¹¶å°†å®ä½“å…±åŒå®šä½åœ¨ä¸€èµ·ï¼Œå› æ­¤å‹ç¼©äº†æ³¨å†Œè¡¨ä»¥å­˜å‚¨æœ‰å…³æ•´ä¸ªåˆ†åŒºçš„ä¿¡æ¯è€Œä¸æ˜¯å•ä¸ªå®ä½“ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œèµ„æºIDæ˜¯ï¼ˆPartitionDï¼ŒEntityIDï¼‰çš„å¤åˆé”®ã€‚è¿™æ˜¯å¦‚ä½•ã€‚ akka.cluster.ardingæˆ–Riakæ ¸å¿ƒä½œå“ã€‚é€šå¸¸å¯ä»¥åœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šç¼“å­˜ä¸€äº›çƒ­ï¼ˆå¸¸ç”¨ï¼‰åˆ†åŒºçš„ä¸€äº›å­é›†ï¼Œä»¥ä¾¿å‡å°‘è¯¢é—®ä¸­å¤®æ³¨å†Œè¡¨ç”šè‡³æ³¨å†Œè¡¨æœ¬èº«å¯ä»¥æ˜¯å¤åˆ¶çš„å•†åº—ã€‚</p><p> We could also use  distributed hash tables - where our entity key is hashed and then mapped into specific node that is responsible for holding resources belonging to that specific subset of key space (a range of all possible hash values). Sometimes this may mean, that we miss node at first try eg. because cluster state is changing, and more hops need to apply.  Microsoft Orleans and Cassandra are popular solutions using that approach.</p><p> æˆ‘ä»¬è¿˜å¯ä»¥ä½¿ç”¨åˆ†å¸ƒå¼å“ˆå¸Œè¡¨ - æ•£åˆ—æˆ‘ä»¬çš„å®ä½“å¯†é’¥ï¼Œç„¶åæ˜ å°„åˆ°ç‰¹å®šèŠ‚ç‚¹ï¼Œè¯¥èŠ‚ç‚¹è´Ÿè´£å°†å±äºå…³é”®ç©ºé—´çš„ç‰¹å®šç©ºé—´å­é›†çš„èµ„æºï¼ˆæ‰€æœ‰å¯èƒ½çš„å“ˆå¸Œå€¼çš„èŒƒå›´ï¼‰ã€‚æœ‰æ—¶è¿™å¯èƒ½æ„å‘³ç€ï¼Œé¦–å…ˆå°è¯•æˆ‘ä»¬é”™è¿‡èŠ‚ç‚¹ã€‚å› ä¸ºç¾¤é›†çŠ¶æ€æ­£åœ¨å‘ç”Ÿå˜åŒ–ï¼Œå› æ­¤éœ€è¦åº”ç”¨æ›´å¤šè·³è·ƒã€‚ Microsoft Orleanså’ŒCassandraä½¿ç”¨è¯¥æ–¹æ³•æ˜¯æµè¡Œçš„è§£å†³æ–¹æ¡ˆã€‚</p><p> As you can see, even though we didn&#39;t asked all questions, there&#39;s already a lot of things happening here and the same problems may be solved with different approaches depending on the tradeoffs our system is willing to take.</p><p> æ­£å¦‚ä½ æ‰€çœ‹åˆ°çš„ï¼Œå³ä½¿æˆ‘ä»¬æ²¡æœ‰é—®è¿‡æ‰€æœ‰çš„é—®é¢˜ï¼Œé‚£é‡Œæœ‰å¾ˆå¤šäº‹æƒ…å‘ç”Ÿäº†å¾ˆå¤šäº‹æƒ…ï¼Œå¹¶ä¸”å¯ä»¥æ ¹æ®æˆ‘ä»¬çš„ç³»ç»Ÿæ„¿æ„é‡‡å–çš„æƒè¡¡æ¥è§£å†³ä¸åŒçš„æ–¹æ³•ã€‚ </p><p> While there&#39;s a chance to make a separate article about each of these in the future, today we&#39;ll focus solely on a  SWIM - used in systems such as Consul and hugely popularized over last few years - as it&#39;s easy to implement for a good start. To improve its resiliency and reduce false positive failure detection, checkout this talk about  Lifeguard - a set of extensions and observations about original SWIM protocol.</p><p>åœ¨é‚£é‡Œï¼†ï¼ƒ39;ä»Šå¤©çš„æœºä¼šåœ¨æœªæ¥åšå‡ºä¸€ä¸ªå•ç‹¬çš„æ–‡ç« ï¼Œä»Šå¤©æˆ‘ä»¬ï¼†ï¼ƒ39; llå®Œå…¨ä¸“æ³¨äºæ¸¸æ³³ - åœ¨é¢†äº‹å’ŒæŒç»­å‡ å¹´ä¸­çš„ç³»ç»Ÿä¸­çš„ç³»ç»Ÿä¸­ä½¿ç”¨ - å¦‚å®ƒï¼†ï¼ƒ39 ;å¾ˆå®¹æ˜“å®ç°è‰¯å¥½çš„å¼€å§‹ã€‚ä¸ºäº†æé«˜å…¶å¼¹æ€§ï¼Œå‡å°‘å‡é˜³æ€§æ•…éšœæ£€æµ‹ï¼Œç»“è´¦å…³äºæ•‘ç”Ÿå‘˜çš„è°ˆåˆ¤ - ä¸€ç³»åˆ—å…³äºåŸå§‹æ¸¸æ³³åè®®çš„æ‰©å±•å’Œè§‚å¯Ÿã€‚</p><p>   New node wants to join the cluster - how to make that happen and how to inform other nodes in the cluster about that event?</p><p>   æ–°èŠ‚ç‚¹æƒ³è¦åŠ å…¥ç¾¤é›† - å¦‚ä½•ä½¿å…¶å‘ç”Ÿï¼Œä»¥åŠå¦‚ä½•åœ¨ç¾¤é›†ä¸­é€šçŸ¥å…³äºè¯¥äº‹ä»¶çš„å…¶ä»–èŠ‚ç‚¹ï¼Ÿ</p><p>  Node was abruptly terminated or cannot be reached any longer. How to detect that an inform others about the fact?</p><p>  èŠ‚ç‚¹çªç„¶ç»ˆæ­¢æˆ–è€…æ— æ³•è¾¾åˆ°ä»»ä½•æ›´é•¿ã€‚å¦‚ä½•æ£€æµ‹åˆ°å…¶ä»–äº‹å®çš„ä¿¡æ¯ï¼Ÿ</p><p> While first two cases are pretty easy, all of the complexity comes with the third case. We need to discover if node cannot be reached, but that&#39;s not enough - since temporary network failures may happen even inside the same datacenter, they usually get healed fairly fast. We don&#39;t want to panic and throw the node out of the cluster just because we couldn&#39;t reach it in split second. This would lead to very shaky and fragile cluster.</p><p> è™½ç„¶å‰ä¸¤ç§æƒ…å†µå¾ˆå®¹æ˜“ï¼Œä½†æ‰€æœ‰çš„å¤æ‚æ€§éƒ½å¸¦æœ‰ç¬¬ä¸‰ç§æƒ…å†µã€‚æˆ‘ä»¬éœ€è¦å‘ç°èŠ‚ç‚¹æ˜¯å¦æ— æ³•è¾¾åˆ°ï¼Œä½†æ˜¯ï¼†ï¼ƒ39; sè¿˜ä¸å¤Ÿ - ç”±äºä¸´æ—¶ç½‘ç»œæ•…éšœå¯èƒ½å‘ç”Ÿåœ¨åŒä¸€æ•°æ®ä¸­å¿ƒå†…ï¼Œä»–ä»¬é€šå¸¸ä¼šè¿…é€Ÿæ„ˆåˆã€‚æˆ‘ä»¬ä¸æƒ³ææ…Œå¹¶å°†èŠ‚ç‚¹æ‰”å‡ºç¾¤é›†ï¼Œå› ä¸ºæˆ‘ä»¬å¯ä»¥åœ¨åˆ†å¼€ç¬¬äºŒä¸ªä¸­è¾¾åˆ°å®ƒã€‚è¿™å°†å¯¼è‡´éå¸¸æ‘‡æ‘‡æ¬²å å’Œè„†å¼±çš„é›†ç¾¤ã€‚</p><p> We&#39;ve already mentioned heartbeat mechanism - every now and then we&#39;re going to send a PING message to other random node. This node is expected to answer with ACK before expected timeout. Easy.</p><p> æˆ‘ä»¬å·²ç»æåˆ°äº†å¿ƒè·³æœºåˆ¶ - æ¯ä¸€ä¸ªç°åœ¨ï¼Œç„¶åæˆ‘ä»¬å°†pingæ¶ˆæ¯å‘é€åˆ°å…¶ä»–éšæœºèŠ‚ç‚¹ã€‚é¢„è®¡æ­¤èŠ‚ç‚¹å°†åœ¨é¢„æœŸè¶…æ—¶ä¹‹å‰ç”¨ACKå›ç­”ã€‚ç®€å•çš„ã€‚</p><p>  Now, what happens if node didn&#39;t respond under timeout? As we said, we don&#39;t want to overreact. So we don&#39;t consider this node dead yet. Instead we consider it to be  suspect and inform others about our suspicion. Other nodes that received suspect gossip expect it to be confirmed within specified timeout - otherwise they will consider it a hoax, and remove node from their suspected list.</p><p>  ç°åœ¨ï¼Œå¦‚æœèŠ‚ç‚¹æ²¡æœ‰åœ¨è¶…æ—¶å›åº”çš„æƒ…å†µä¸‹ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿæ­£å¦‚æˆ‘ä»¬æ‰€è¯´ï¼Œæˆ‘ä»¬ä¸æƒ³è¿‡åº¦ååº”ã€‚æ‰€ä»¥æˆ‘ä»¬ä¸è¦è€ƒè™‘è¿™ä¸ªèŠ‚ç‚¹å·²ç»æ­»äº†ã€‚ç›¸åï¼Œæˆ‘ä»¬è®¤ä¸ºå®ƒæ˜¯æ€€ç–‘å¹¶å‘ŠçŸ¥åˆ«äººæˆ‘ä»¬çš„æ€€ç–‘ã€‚æ¥æ”¶å«Œç–‘å…«å¦çš„å…¶ä»–èŠ‚ç‚¹é¢„è®¡å®ƒå°†åœ¨æŒ‡å®šçš„è¶…æ—¶ä¸­ç¡®è®¤ - å¦åˆ™ä»–ä»¬å°†è€ƒè™‘ä¸€ä¸ªæ¶ä½œå‰§ï¼Œå¹¶ä»å…¶ç–‘ä¼¼åˆ—è¡¨ä¸­åˆ é™¤èŠ‚ç‚¹ã€‚</p><p>  Now all we need is confirmation - in order to do so, we need to ask someone else for verification. So we&#39;re going pick another unsuspected node and ask it to ping  suspect for us (this request is known as PING-REQ). Now if that mediator managed to receive ACK from the  suspect, we know that node is alive, just our network connection was severed for some reason:</p><p>  ç°åœ¨æˆ‘ä»¬æ‰€éœ€è¦çš„åªæ˜¯ç¡®è®¤ - ä¸ºäº†è¿™æ ·åšï¼Œæˆ‘ä»¬éœ€è¦å‘åˆ«äººè¯¢é—®åˆ«äººè¿›è¡ŒéªŒè¯ã€‚æ‰€ä»¥æˆ‘ä»¬ï¼†ï¼ƒ39;é‡æ–°é€‰æ‹©å¦ä¸€ä¸ªæœªç»ç”¨çš„èŠ‚ç‚¹ï¼Œå¹¶è¦æ±‚å®ƒä¸ºæˆ‘ä»¬çš„pingå«Œç–‘äººï¼ˆæ­¤è¯·æ±‚è¢«ç§°ä¸ºping-reqï¼‰ã€‚ç°åœ¨ï¼Œå¦‚æœè¯¥è°ƒè§£å‘˜ä»å«Œç–‘äººé‚£é‡Œæ”¶åˆ°ACKï¼Œæˆ‘ä»¬çŸ¥é“èŠ‚ç‚¹æ˜¯æ´»ç€çš„ï¼Œåªæ˜¯æˆ‘ä»¬çš„ç½‘ç»œè¿æ¥è¢«åˆ‡æ–­äº†åŸå› ï¼š </p><p>  On the other side, if our mediator didn&#39;t received ACK either, we now have two-hand verification that node is unresponsive for significant time frame - therefore it can be confirmed as dead to everyone.</p><p>åœ¨å¦ä¸€è¾¹ï¼Œå¦‚æœæˆ‘ä»¬çš„è°ƒè§£å‘˜ä¹Ÿæ²¡æœ‰æ”¶åˆ°Ackï¼Œæˆ‘ä»¬ç°åœ¨æœ‰ä¸¤åªæ‰‹éªŒè¯ï¼ŒèŠ‚ç‚¹å¯¹å¤§é‡æ—¶é—´æ¡†æ¶æ— å“åº” - å› æ­¤å®ƒå¯ä»¥ç¡®è®¤ä¸ºæ¯ä¸ªäººæ­»äº¡ã€‚</p><p>  Of course this doesn&#39;t mean that node is indeed dead - one escape mechanism is that if suspect will get a suspect notification about itself, it can override it by broadcasting alive message. There maybe different reasons, why  suspect didn&#39;t respond on time: it may be too occupied (either by backlog of other requests or eg. stop-the-world garbage collection of underlying VM). It may also happen, that our network have split the cluster apart - this phenomenon is known as a split-brain.</p><p>  å½“ç„¶è¿™ä¸€ç‚¹å¹¶ä¸æ„å‘³ç€èŠ‚ç‚¹ç¡®å®æ­»äº† - ä¸€ä¸ªè½¬ä¹‰æœºåˆ¶æ˜¯ï¼Œå¦‚æœæ€€ç–‘å°†è·å¾—å…³äºè‡ªå·±çš„å¯ç–‘é€šçŸ¥ï¼Œå®ƒå¯ä»¥é€šè¿‡å¹¿æ’­æ´»åŠ¨æ¥è¦†ç›–å®ƒã€‚å¯èƒ½æœ‰ä¸åŒçš„åŸå› ï¼Œä¸ºä»€ä¹ˆæ€€ç–‘ï¼†ï¼ƒ39; tæŒ‰æ—¶å›åº”ï¼šå®ƒå¯èƒ½å¤ªå ç”¨ï¼ˆç”±å…¶ä»–è¯·æ±‚çš„ç§¯å‹æˆ–evã€‚åœæ­¢ - ä¸–ç•Œåƒåœ¾æ”¶é›†çš„åº•å±‚VMï¼‰ã€‚ä¹Ÿå¯èƒ½å‘ç”Ÿï¼Œæˆ‘ä»¬çš„ç½‘ç»œåˆ†å¼€äº†åˆ†å¼€çš„é›†ç¾¤ - è¿™ç§ç°è±¡è¢«ç§°ä¸ºåˆ†è£‚æ€§ã€‚</p><p> Ultimately there&#39;s no way to ensure 100% reliable failure detection. We&#39;re just trading reasonable ratio of false failure and how quickly can we detect node as dead - usually these two goals work against each other. In terms of SWIM, I again recommend to lookup for  Lifeguard, which addresses some of the mentioned scenarios. However we&#39;ll not cover it in our implementation below to keep it succinct.</p><p> æœ€ç»ˆæ²¡æœ‰åŠæ³•ä¿è¯100ï¼…å¯é çš„æ•…éšœæ£€æµ‹ã€‚æˆ‘ä»¬ï¼†ï¼ƒ39;é‡æ–°äº¤æ˜“åˆç†çš„é”™è¯¯å¤±è´¥æ¯”ï¼Œæˆ‘ä»¬å¦‚ä½•è¿…é€Ÿåœ°æ£€æµ‹åˆ°èŠ‚ç‚¹ä½œä¸ºæ­»äº¡ - é€šå¸¸è¿™ä¸¤ä¸ªç›®æ ‡ç›¸äº’äº’ç›¸åŠªåŠ›ã€‚åœ¨æ¸¸æ³³æ–¹é¢ï¼Œæˆ‘å†æ¬¡å»ºè®®æŸ¥æ‰¾æ•‘ç”Ÿå‘˜ï¼Œè¯¥æ•‘ç”Ÿå‘˜åœ°è§£å†³äº†ä¸€äº›æåˆ°çš„æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œæˆ‘ä»¬ï¼†ï¼ƒ39; LLåœ¨ä¸‹é¢çš„å®æ–½ä¸­æ²¡æœ‰æ¶µç›–å®ƒï¼Œä»¥ä¿æŒç®€æ´ã€‚</p><p>  Now, we&#39;re going to actually implement that protocol. In order to do so we&#39;re going to apply some simplifications - we&#39;ll do so to make protocol easier to comprehend:</p><p>  ç°åœ¨ï¼Œæˆ‘ä»¬ï¼†ï¼ƒ39;é‡æ–°å®é™…å®æ–½è¯¥åè®®ã€‚ä¸ºäº†è¿™æ ·åšï¼†ï¼ƒ39;é‡æ–°åº”ç”¨ä¸€äº›ç®€åŒ– - æˆ‘ä»¬ï¼†ï¼ƒ39; llè¿™æ ·åšï¼Œä½¿åè®®æ›´å®¹æ˜“ç†è§£ï¼š</p><p> In general to associate specific request/response messages, we should use some form of sequence numbers. This can result in scenario, when A sends PING1 request to B for which is responds with ACK1, some time later A sends another PING2 to B and receives ACK1 back - the problem was that it cannot tell which PING this message was acknowledgment for. This kind of situation is very rare, but not impossible. If may also lead to &#34;resurrecting&#34; dead nodes.</p><p> ä¸€èˆ¬æ¥è¯´è¦å…³è”ç‰¹å®šè¯·æ±‚/å“åº”æ¶ˆæ¯ï¼Œæˆ‘ä»¬åº”è¯¥ä½¿ç”¨æŸç§å½¢å¼çš„åºåˆ—å·ã€‚è¿™å¯èƒ½å¯¼è‡´åœºæ™¯ï¼Œå½“Aå‘Bå‘é€Ping1è¯·æ±‚æ—¶ï¼Œå…¶ä¸Ack1å“åº”ï¼Œä¸€æ®µæ—¶é—´åè€…å°†å¦ä¸€ä¸ªPing2å‘é€åˆ°Bå¹¶æ¥æ”¶Ack1å›æ¥ - é—®é¢˜æ˜¯å®ƒä¸èƒ½å‘Šè¯‰è¯¥æ¶ˆæ¯çš„é—®é¢˜æ˜¯ä¸ºäº†ç¡®è®¤è¿™æ¡æ¶ˆæ¯ã€‚è¿™ç§æƒ…å†µéå¸¸ç½•è§ï¼Œä½†ä¸æ˜¯ä¸å¯èƒ½çš„ã€‚å¦‚æœä¹Ÿå¯èƒ½å¯¼è‡´ï¼†ï¼ƒ34;å¤æ´»å’Œï¼ƒ34;æ­»äº¡èŠ‚ç‚¹ã€‚</p><p> While we&#39;re going to piggyback gossip on top of pings - like the paper suggests - we&#39;re simply  gossip entire membership state every time. It&#39;s not the most optimal way, but it&#39;ll work good enough.</p><p> è™½ç„¶æˆ‘ä»¬ï¼†ï¼ƒ39;é‡æ–°è¿›å…¥è‚©èƒŒä¸Šçš„è‚©å¸¦ - åƒçº¸å¼ æš—ç¤º - æˆ‘ä»¬æ¯æ¬¡éƒ½åªæ˜¯å…«å¦æ•´ä¸ªä¼šå‘˜å›½ã€‚å®ƒä¸æ˜¯æœ€ä¼˜è¶Šçš„æ–¹å¼ï¼Œä½†å®ƒï¼†ï¼ƒ39; llå·¥ä½œè¶³å¤Ÿå¥½ã€‚</p><p> PS:  Other way of avoiding sending entire state with every gossip is to compute consistent hash of current node membership view and put that hash inside of ping instead of full view. This way our PING will carry just a hash value, which responder may compare with hash of its own state. Only if hashes differ (in clusters with small churn of nodes most of the time they don&#39;t), we&#39;re putting membership state on top of ACK.</p><p> PSï¼šé¿å…ä½¿ç”¨æ¯ä¸ªå…«å¦å‘é€æ•´ä¸ªçŠ¶æ€çš„å…¶ä»–æ–¹å¼æ˜¯è®¡ç®—å½“å‰èŠ‚ç‚¹æˆå‘˜èº«ä»½è§†å›¾çš„ä¸€è‡´å“ˆå¸Œï¼Œå¹¶å°†è¯¥æ•£åˆ—æ”¾åœ¨pingå†…è€Œä¸æ˜¯å®Œæ•´è§†å›¾ã€‚è¿™æ ·æˆ‘ä»¬çš„pingå°±ä¼šæºå¸¦å“ˆå¸Œå€¼ï¼Œè¿™æ˜¯å“åº”è€…å¯ä»¥ä¸è‡ªå·±çŠ¶æ€çš„å“ˆå¸Œç›¸æ¯”ã€‚åªæœ‰å½“å“ˆå¸Œä¸åŒï¼ˆå¤§å¤šæ•°æ—¶é—´éƒ½æœ‰å¤§éƒ¨åˆ†æ—¶é—´çš„é›†ç¾¤ï¼Œä»–ä»¬åœ¨ä»–ä»¬ä¸æ—¶çš„æ—¶é—´å’Œï¼ƒ39; tï¼‰ï¼Œæˆ‘ä»¬ï¼†ï¼ƒ39;é‡æ–°å°†ä¼šå‘˜çŠ¶æ€æ”¾åœ¨Ackä¹‹ä¸Šã€‚ </p><p> The code used here is available in  this gist, which I recommend to use, as we&#39;ll not cover everything here.</p><p>æ­¤å¤„ä½¿ç”¨çš„ä»£ç å¯åœ¨æœ¬å‘æ˜å‘˜ä¸­æä¾›ï¼Œæˆ‘å»ºè®®ä½¿ç”¨ï¼Œå¦‚æˆ‘ä»¬ï¼†ï¼ƒ39; LLåœ¨è¿™é‡Œä¸æ¶µç›–æ‰€æœ‰å†…å®¹ã€‚</p><p>  Since we don&#39;t want to deal with all complexities of node-to-node communication, we&#39;ll just build a model on top of abstractions, that will let us evaluate the algorithm without derailing into solving other problems. The basic prerequisites here are:</p><p>  ç”±äºæˆ‘ä»¬ä¸æƒ³å¤„ç†èŠ‚ç‚¹åˆ°èŠ‚ç‚¹é€šä¿¡çš„æ‰€æœ‰å¤æ‚æ€§ï¼Œæˆ‘ä»¬ï¼†ï¼ƒ39; LLåªæ˜¯åœ¨æŠ½è±¡ä¹‹ä¸Šæ„å»ºä¸€ä¸ªæ¨¡å‹ï¼Œè¿™å°†è®©æˆ‘ä»¬è¯„ä¼°ç®—æ³•ï¼Œè€Œä¸ä¼šå¯¼è‡´è§£å†³å…¶ä»–é—®é¢˜ã€‚ä»¥ä¸‹åŸºæœ¬å…ˆå†³æ¡ä»¶æ˜¯ï¼š</p><p> A transport layer that will just let us send message to another (possibly remote)  endpoint. We don&#39;t want to deal with managing network connections or serialization details.</p><p> ä¸€ä¸ªä¼ è¾“å±‚ï¼Œè®©æˆ‘ä»¬å‘å¦ä¸€ä¸ªï¼ˆå¯èƒ½è¿œç¨‹ï¼‰ç«¯ç‚¹å‘é€æ¶ˆæ¯ã€‚æˆ‘ä»¬å¸Œæœ›å¤„ç†ç®¡ç†ç½‘ç»œè¿æ¥æˆ–åºåˆ—åŒ–è¯¦ç»†ä¿¡æ¯ã€‚</p><p> An agent accessible behind the  endpoint, able to serve multiple requests and change its state in thread safe manner.</p><p> å¯è®¿é—®ç«¯ç‚¹åé¢çš„ä»£ç†ï¼Œèƒ½å¤Ÿä¸ºå¤šä¸ªè¯·æ±‚æä¾›æœåŠ¡å¹¶ä»¥çº¿ç¨‹å®‰å…¨æ–¹å¼æ›´æ”¹å…¶çŠ¶æ€ã€‚</p><p> For these reasons, I&#39;m going to use Akka.NET/Akkling and model our nodes using actors living in the same process. You can easily adopt it to different actor systems connected over Akka.Remote. We keep this as a training exercise - in practice if you want to use clusters in akka, you already have entire ecosystem built on top of Akka.Cluster, which handles membership - and other problems mentioned in introduction - for you.</p><p> ç”±äºè¿™äº›åŸå› ï¼Œæˆ‘å°†ä½¿ç”¨Akka.net/Kklingå’Œæ¨¡æ‹Ÿæˆ‘ä»¬çš„èŠ‚ç‚¹ä½¿ç”¨ç”Ÿæ´»åœ¨åŒä¸€è¿‡ç¨‹ä¸­çš„æ¼”å‘˜ã€‚æ‚¨å¯ä»¥è½»æ¾åœ°é‡‡ç”¨å®ƒä»¥é€šè¿‡Akka.Remoteè¿æ¥çš„ä¸åŒæ¼”å‘˜ç³»ç»Ÿã€‚æˆ‘ä»¬å°†æ­¤ä½œä¸ºåŸ¹è®­ç»ƒä¹  - åœ¨å®è·µä¸­ï¼Œå¦‚æœæ‚¨æƒ³åœ¨Akkaä¸­ä½¿ç”¨ç¾¤é›†ï¼Œæ‚¨å·²ç»åœ¨Akka.Clusteré¡¶éƒ¨å»ºç«‹äº†æ•´ä¸ªç”Ÿæ€ç³»ç»Ÿï¼Œè¿™äº›ç”Ÿæ€ç³»ç»Ÿéƒ½åœ¨å¤„ç†ä¼šå‘˜èµ„æ ¼ - ä»¥åŠå¼•è¨€ä¸­æåˆ°çš„å…¶ä»–é—®é¢˜ - é€‚åˆæ‚¨ã€‚</p><p> Our actor will be created by providing it an initial list of contact points: these are the endpoints known to be part of the cluster:</p><p> æˆ‘ä»¬çš„æ¼”å‘˜å°†é€šè¿‡æä¾›å®ƒçš„åˆå§‹è”ç³»ç‚¹åˆ—è¡¨æ¥åˆ›å»ºï¼šè¿™äº›æ˜¯å·²çŸ¥ä¸ºç¾¤é›†çš„ä¸€éƒ¨åˆ†çš„ç«¯ç‚¹ï¼š</p><p> open Akklinguse sys = System.create &#34;swim-cluster&#34; &lt;| Configuration.parse config// In this example `a` and `b` are actor refs for other // SWIM cluster members living on other nodes. let c = spawn sys &#34;node-b&#34; &lt;| props (Swim.membership [a; b])</p><p> æ‰“å¼€akklinguse sys = system.createï¼†ï¼ƒ34;æ¸¸æ³³ç¾¤å’Œï¼ƒ34; ï¼†lt; | configuration.parse config //åœ¨æ­¤ç¤ºä¾‹ä¸­ï¼Œå…¶ä»–//`b`æ˜¯ç”Ÿæ´»åœ¨å…¶ä»–èŠ‚ç‚¹ä¸Šçš„å…¶ä»–//æ¸¸æ³³é›†ç¾¤æˆå‘˜çš„æ¼”å‘˜â€‹â€‹refsã€‚è®©C =äº§åµç³»ç»Ÿï¼†ï¼ƒ34;èŠ‚ç‚¹Bï¼†ï¼ƒ34; ï¼†lt; |é“å…·ï¼ˆæ¸¸æ³³.Membership [A; B]ï¼‰ </p><p> In order to create that actor first we need to deal with a joining process. The idea is that we&#39;ll try to pick endpoints from the provided list (it corresponds to seed node addresses in Akka.Cluster ) and send a  Join request to them. If cluster node actor will receive such request, it&#39;s obliged to accept it by sending  Joined response to ALL cluster members (including requester).</p><p>ä¸ºäº†åˆ›å»ºè¯¥æ¼”å‘˜ï¼Œæˆ‘ä»¬éœ€è¦å¤„ç†åŠ å…¥è¿‡ç¨‹ã€‚è¿™ä¸ªæƒ³æ³•æ˜¯æˆ‘ä»¬ï¼†ï¼ƒ39; llå°è¯•ä»æä¾›çš„åˆ—è¡¨ä¸­æŒ‘é€‰ç«¯ç‚¹ï¼ˆå®ƒå¯¹åº”äºAkka.Clusterä¸­çš„ç§å­èŠ‚ç‚¹åœ°å€ï¼‰å¹¶å‘å®ƒä»¬å‘é€åŠ å…¥è¯·æ±‚ã€‚å¦‚æœç¾¤é›†èŠ‚ç‚¹æ¼”å‘˜å°†æ¥æ”¶è¿™æ ·çš„è¯·æ±‚ï¼Œåˆ™ï¼†ï¼ƒ39; sä¹‰åŠ¡é€šè¿‡å‘æ‰€æœ‰ç¾¤é›†æˆå‘˜å‘é€åŠ å…¥å“åº”ï¼ˆåŒ…æ‹¬è¯·æ±‚è€…ï¼‰æ¥æ¥å—å®ƒã€‚</p><p> In case when  Joined wouldn&#39;t arrive eg. because we misconfigured our cluster and other endpoint couldn&#39;t be reached, we wait for some time and try another node from the list. This could be represented as:</p><p> å¦‚æœåŠ å…¥WORNå’Œï¼ƒ39; Tåˆ°è¾¾ï¼Œä¾‹å¦‚ã€‚å› ä¸ºæˆ‘ä»¬é”™è¯¯åœ°é…ç½®äº†æˆ‘ä»¬çš„ç¾¤é›†å’Œå…¶ä»–ç«¯ç‚¹ï¼Œæ‰€ä»¥è¾¾åˆ°äº†ï¼†ï¼ƒ39; t andï¼ƒ39;æˆ‘ä»¬ç­‰å¾…ä¸€æ®µæ—¶é—´å¹¶ä»åˆ—è¡¨ä¸­å°è¯•å¦ä¸€ä¸ªèŠ‚ç‚¹ã€‚è¿™å¯ä»¥è¡¨ç¤ºä¸ºï¼š</p><p> let membership (seeds: Endpoint list) (ctx: Actor&lt;_&gt;) = let state = { Myself = { Endpoint = ctx.Self } Active = Set.empty Suspects = Map.empty } (* rest of the actor code... *) let rec joining state cancel = actor { match! ctx.Receive() with | JoinTimeout [] -&gt; return Stop // failed to join any members | JoinTimeout (next::remaining) -&gt; next &lt;! Join state.Myself let cancel = ctx.Schedule joinTimeout ctx.Self (JoinTimeout remaining) return! joining state cancel | Joined gossip -&gt; cancel.Cancel() // cancel join timeout return! becomeReady state gossip | Join peer when peer = state.Myself -&gt; // establish new cluster cancel.Cancel() // cancel join timeout return! becomeReady state (Set.singleton state.Myself) | _ -&gt; return Unhandled } match seeds with | [] -&gt; becomeReady state (Set.singleton state.Myself) | seed::remaining -&gt; seed &lt;! Join state.Myself let cancel = ctx.Schedule joinTimeout ctx.Self (JoinTimeout remaining) joining state cancel</p><p> è®©æˆå‘˜èº«ä»½ï¼ˆç§å­ï¼šç«¯ç‚¹åˆ—è¡¨ï¼‰ï¼ˆCTXï¼šActorï¼†lt; _ï¼†gt;ï¼‰=è®¾æ€= {mylising = {endpoint = ctx.self} Active = set.emptyå«Œç–‘äºº= map.empty}ï¼ˆ* actorä»£ç çš„å…¶ä½™éƒ¨åˆ†.. ã€‚*ï¼‰è®©RECåŠ å…¥å·å–æ¶ˆ=æ¼”å‘˜{åŒ¹é…ï¼ ctx.receiveï¼ˆï¼‰ä¸|èŒå·¥[]  - ï¼†gt;è¿”å›åœæ­¢//æœªèƒ½åŠ å…¥ä»»ä½•ä¼šå‘˜|èŒå·¥ï¼ˆä¸‹ä¸€ä¸ª::å‰©ä¸‹ï¼‰ - ï¼†gt;ä¸‹ä¸€ä¸ªï¼†lt;åŠ å…¥State.Myselfè®©Callå–æ¶ˆ= CTX.schedule Connectimeout CTX.Selfï¼ˆå‰©ä½™å‰©ä½™ï¼‰è¿”å›ï¼åŠ å…¥å·å–æ¶ˆ|åŠ å…¥å…«å¦ - ï¼†gt; CANCEL.CANCELï¼ˆï¼‰//å–æ¶ˆåŠ å…¥è¶…æ—¶è¿”å›ï¼æˆä¸ºå·å…«å¦|å½“peer = statem.myself  - ï¼†gt; //å»ºç«‹æ–°ç¾¤é›†å–æ¶ˆ.Cancelï¼ˆï¼‰//å–æ¶ˆåŠ å…¥è¶…æ—¶è¿”å›ï¼æˆä¸ºå·ï¼ˆset.singleton state.myselfï¼‰| _  - ï¼†gt;è¿”å›æœªå¤„ç†çš„}åŒ¹é…ç§å­| []  - ï¼†gt;æˆä¸ºå·ï¼ˆset.singleton state.myselfï¼‰|ç§å­::å‰©ä¸‹ - ï¼†gt;ç§å­ï¼†lt;åŠ å…¥State.Myselfè®©Call = CTX.Schedule Connectimeout CTX.Selfï¼ˆå‰©ä½™å‰©ä½™ï¼‰åŠ å…¥å·å–æ¶ˆ</p><p> After receiving the  Joined request actor associates itself as a operating member of the cluster, ready to work. From now on it will trigger itself to periodically check if others remain responsive:</p><p> æ¥æ”¶åˆ°åŠ å…¥çš„è¯·æ±‚Actorå°†è‡ªå·±è§†ä¸ºç¾¤é›†çš„æ“ä½œæˆå‘˜ï¼Œå‡†å¤‡å·¥ä½œã€‚ä»ç°åœ¨å¼€å§‹ï¼Œå®ƒå°†è§¦å‘è‡ªèº«ï¼Œä»¥å®šæœŸæ£€æŸ¥å…¶ä»–äººæ˜¯å¦ä¿æŒå“åº”ï¼š</p><p> let merge gossip state = { state with Active = state.Active + gossip }let becomeReady state gossip = ctx.Schedule pingInterval ctx.Self NextRound |&gt; ignore ready (merge gossip state)</p><p> è®©åˆå¹¶å…«å¦çŠ¶æ€= {youst active = state.active + gossip}è®©bersereadyçŠ¶æ€gossip = ctx.schedule pinginterval ctx.self nextround |ï¼†gt;å¿½ç•¥å°±ç»ªï¼ˆåˆå¹¶å…«å¦çŠ¶æ€ï¼‰</p><p> As mentioned before, in this case  gossip is just a full set of active cluster members, while  ready is actor behavior used for standard cluster activities:</p><p> å¦‚å‰æ‰€è¿°ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå…«å¦åªæ˜¯ä¸€æ•´å¥—æ´»åŠ¨é›†ç¾¤æˆå‘˜ï¼Œè€Œå‡†å¤‡å¥½æ˜¯ç”¨äºæ ‡å‡†ç¾¤é›†æ´»åŠ¨çš„æ¼”å‘˜è¡Œä¸ºï¼š</p><p> let rec ready state = actor { match! ctx.Receive() with // other message handlers | _ -&gt; return Unhandled}</p><p> è®©Recrade State =æ¼”å‘˜{åŒ¹é…ï¼ ctx.receiveï¼ˆï¼‰ä½¿ç”¨//å…¶ä»–æ¶ˆæ¯å¤„ç†ç¨‹åº| _  - ï¼†gt;è¿”å›æœªå¤„ç†} </p><p> Since we covered joining procedure from requestor side, let&#39;s do that for operating cluster member as well:</p><p>ç”±äºæˆ‘ä»¬ä»è¯·æ±‚è€…æ–¹é¢è¦†ç›–äº†åŠ å…¥ç¨‹åºï¼Œå› æ­¤ä¸ºæ“ä½œé›†ç¾¤æˆå‘˜æä¾›äº†ï¼š</p><p> match! ctx.Receive with| Join peer -&gt; let gossip = Set.add peer state.Active let msg = Joined gossip gossip |&gt; Set.remove state.Myself |&gt; Set.iter (fun peer -&gt; peer.Endpoint &lt;! msg) return! ready { state with Active = gossip } | Joined gossip -&gt; return! ready (merge gossip state)// other handlers</p><p> æ¯”èµ›ï¼ ctx.receive |.åŠ å…¥peer  - ï¼†gt;è®©Gossip = Set.Add PeerçŠ¶æ€ã€‚Active Let Msg =åŠ å…¥å…«å¦å…«å¦|ï¼†gt; set.remove state.myself |ï¼†gt; set.iterï¼ˆæœ‰è¶£çš„peer  - ï¼†gt; peer.endpointï¼†lt; msgï¼‰è¿”å›ï¼å‡†å¤‡å°±ç»ª{çŠ¶æ€ä¸active = gossip} |åŠ å…¥å…«å¦ - ï¼†gt;è¿”å›ï¼å‡†å¤‡å¥½ï¼ˆåˆå¹¶å…«å¦çŠ¶æ€ï¼‰//å…¶ä»–å¤„ç†ç¨‹åº</p><p> As we said, once new member tries to join, we simply update our active members state and gossip it to all other members in  Joined message, so they could update it as well.</p><p> æ­£å¦‚æˆ‘ä»¬æ‰€è¯´ï¼Œä¸€æ—¦æ–°æˆå‘˜è¯•å›¾åŠ å…¥ï¼Œæˆ‘ä»¬åªéœ€å°†æˆ‘ä»¬çš„æ´»åŠ¨æˆå‘˜çŠ¶æ€æ›´æ–°å¹¶å°†å…¶æ±‡æŠ¥åˆ°åŠ å…¥æ¶ˆæ¯ä¸­çš„æ‰€æœ‰å…¶ä»–æˆå‘˜ï¼Œå› æ­¤å®ƒä»¬ä¹Ÿå¯ä»¥æ›´æ–°ã€‚</p><p> Now, let&#39;s cover pinging process. First we mentioned that we want to trigger our member every now and then - you could already see that when we scheduled  NextRound event in  becomeReady function. Now how it will work? We&#39;re going to pick one node at random - other than current one and not being suspected (suspects are nodes that didn&#39;t reply to pings on time) - and send a  Ping request to it. In the meantime we also schedule a timeout for that request to complete:</p><p> ç°åœ¨ï¼Œè®©ï¼†ï¼ƒ39; sè¦†ç›–pingè¿‡ç¨‹ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æåˆ°æˆ‘ä»¬ç°åœ¨æƒ³è¦è§¦å‘æˆ‘ä»¬çš„ä¼šå‘˜ï¼Œç„¶å - å½“æˆ‘ä»¬è®¡åˆ’æˆæ¯”èµ›ä¸­çš„Nextroundäº‹ä»¶æ—¶ï¼Œæ‚¨å¯ä»¥çœ‹åˆ°ã€‚ç°åœ¨å®ƒå¦‚ä½•å·¥ä½œï¼Ÿæˆ‘ä»¬ï¼†ï¼ƒ39;é‡æ–°é€‰æ‹©ä¸€ä¸ªèŠ‚ç‚¹éšæœº - é™¤äº†å½“å‰çš„ä¸€ä¸ªæ²¡æœ‰ç–‘ä¼¼ï¼ˆå«Œç–‘äººæ˜¯ï¼†ï¼ƒ39; tæŒ‰æ—¶å›å¤pingçš„èŠ‚ç‚¹ï¼‰ - å¹¶å‘å®ƒå‘é€pingè¯·æ±‚ã€‚ä¸æ­¤åŒæ—¶ï¼Œæˆ‘ä»¬è¿˜å®‰æ’äº†è¯¥è¯·æ±‚å®Œæˆçš„è¶…æ—¶ï¼š</p><p> let pick peers = match Set.count peers with | 0 -&gt; None | count -&gt; let idx = ThreadLocalRandom.Current.Next count Some (Seq.item idx peers) let ready state = actor { match! ctx.Receive() with | NextRound -&gt; ctx.Schedule pingInterval ctx.Self NextRound |&gt; ignore let suspects = state.Suspects |&gt; Map.toSeq |&gt; Seq.map fst |&gt; Set.ofSeq let others = (Set.remove state.Myself state.Active) - suspects // pick one member at random, other than self and not marked as suspected match pick others with | None -&gt; return! ready state | Some peer -&gt; // send Ping request to that peer and schedule timeout peer.Endpoint &lt;! Ping(state.Active, ctx.Self) let cancel = ctx.Schedule pingTimeout ctx.Self (PingTimeout peer) let skipList = Set.ofArray [| state.Myself; peer |] return! ready { state with Suspects = Map.add peer (cancel, WaitPingAck skipList) state.Suspects } | Ping (gossip, sender) -&gt; // reply to incoming ping right away sender &lt;! PingAck(state.Myself, state.Active) return! ready (merge gossip state) // other handlers}</p><p> è®©é€‰æ‹©åŒè¡Œ=åŒ¹é…set.countå¯¹ç­‰ä½“0  - ï¼†gt;æ²¡æœ‰|æ•° - ï¼†gt;è®©idx = threadlocalrandom.current.nextè®¡æ•°ä¸€äº›ï¼ˆSEQ.ITEM IDXå¯¹ç­‰ä½“ï¼‰è®©å°±ç»ªçŠ¶æ€= ACTOR {åŒ¹é…ï¼ ctx.receiveï¼ˆï¼‰ä¸| nextround  - ï¼†gt; ctx.schedule pinginterval ctx.self nextround |ï¼†gt;å¿½ç•¥è®©å«Œç–‘äºº= state.suspects |ï¼†gt; Map.Toseq |ï¼†Gt; seq.map fst |ï¼†gt; set.ofseqè®©åˆ«äºº=ï¼ˆset.remove state.myself state.activeï¼‰ - å«Œç–‘äºº//éšæ„é€‰æ‹©ä¸€ä¸ªæˆå‘˜ï¼Œé™¤äº†è‡ªæˆ‘è€Œä¸æ˜¯æ ‡è®°ä¸ºæ€€ç–‘åŒ¹é…é€‰æ‹©æ²¡æœ‰ - ï¼†gt;è¿”å›ï¼å°±ç»ªçŠ¶æ€|ä¸€äº›åŒä¼´ - ï¼†gt; //å°†pingè¯·æ±‚å‘é€åˆ°è¯¥å¯¹ç­‰ä½“å’Œè°ƒåº¦è¶…æ—¶peer.endpointï¼†lt; pingï¼ˆstate.activeï¼Œctx.selfï¼‰è®©å–æ¶ˆ= ctx.schedule pingtimeout ctx.selfï¼ˆpingtimeout peerï¼‰è®©skiplist = set.ofArray [|å·ã€‚åŒä¼´|]å›æ¥ï¼å‡†å¤‡{state with suspects = map.add peerï¼ˆå–æ¶ˆï¼Œwaitpingack skiplistï¼‰state.suspects} | pingï¼ˆå…«å¦ï¼Œå‘ä»¶äººï¼‰ - ï¼†gt; //å›å¤ä¼ å…¥ping ricking senderï¼†lt ;! Pingackï¼ˆå·.MYSELFï¼ŒState.activeï¼‰è¿”å›ï¼å‡†å¤‡å¥½ï¼ˆåˆå¹¶å…«å¦çŠ¶æ€ï¼‰//å…¶ä»–å¤„ç†ç¨‹åº}</p><p> Here we&#39;re using our suspects map to keep track of timeout cancellation and skip list. What&#39;s a skip list? We&#39;ll cover it soon. As we mentioned, when  PingAck doesn&#39;t arrive on time, we&#39;re going to pick another member (at random) and ask it to ping our suspect for us using  PingReq message. This way we try to mitigate risk of false negatives in our failure detection algorithm:</p><p> åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨æˆ‘ä»¬çš„å«Œç–‘äººæ˜ å°„æ¥è·Ÿè¸ªè¶…æ—¶å–æ¶ˆå’Œè·³è¿‡åˆ—è¡¨ã€‚ä»€ä¹ˆï¼†ï¼ƒ39;è·³è¿‡åˆ—è¡¨ï¼Ÿæˆ‘ä»¬å¾ˆå¿«æ©ç›–äº†ã€‚æ­£å¦‚æˆ‘ä»¬æ‰€æåˆ°çš„é‚£æ ·ï¼Œå½“Pingackï¼†ï¼ƒ39; tå‡†æ—¶åˆ°è¾¾æ—¶ï¼Œæˆ‘ä»¬ä¼šé€‰æ‹©å¦ä¸€ä¸ªæˆå‘˜ï¼ˆéšæ„ï¼‰å¹¶è¦æ±‚å®ƒä½¿ç”¨pingreqæ¶ˆæ¯æ¥ä¸ºæˆ‘ä»¬å®¡è§†æˆ‘ä»¬çš„å«Œç–‘äººã€‚è¿™æ ·ï¼Œæˆ‘ä»¬è¯•å›¾åœ¨æˆ‘ä»¬çš„æ•…éšœæ£€æµ‹ç®—æ³•ä¸­å‡è½»å‡å¦å®šçš„é£é™©ï¼š</p><p> let ready state = actor { match! ctx.Receive() with | PingReq (suspect, gossip, sender) -&gt; let cancel = ctx.Schedule indirectPingTimeout ctx.Self (PingTimeout suspect) suspect.Endpoint &lt;! Ping(state.Active, ctx.Self) return! ready { merge gossip state with Suspects = Map.add suspect (cancel, WaitPingReqAck) state.Suspects</p><p> è®©å‡†å¤‡çŠ¶æ€=æ¼”å‘˜{åŒ¹é…ï¼ ctx.receiveï¼ˆï¼‰ä¸| Pingreqï¼ˆå«Œç–‘äººï¼Œå…«å¦ï¼Œå‘ä»¶äººï¼‰ - ï¼†gt;è®©å–æ¶ˆ= ctx.schedule IndirectpingTimeout CTX.Selfï¼ˆPingTimeout Suspectï¼‰å«Œç–‘æ¬¡æ•°.EndPointï¼†lt; pingï¼ˆstate.activeï¼Œctx.selfï¼‰é€€è´§ï¼å‡†å¤‡{Merge Gossipå·ä¸å«Œç–‘äºº= map.addå«Œç–‘äººï¼ˆå–æ¶ˆï¼Œç­‰å¾…ï¼Œç­‰å¾…reqackï¼‰state.suspects </p><p>......</p><p>...... </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://bartoszsypytkowski.com/make-your-cluster-swim/">https://bartoszsypytkowski.com/make-your-cluster-swim/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/2020/">#2020</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/ç¾¤é›†/">#ç¾¤é›†</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/cluster/">#cluster</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/èŠ‚ç‚¹/">#èŠ‚ç‚¹</a></button></div></div><div class="shadow p-3 mb-5 bg-white rounded clearfix"><div class="container"><div class="row"><div class="col-sm"><div><a target="_blank" href="/story/1068381.html"><img src="http://img2.diglog.com/img/2021/6/thumb_d1615bb7dc31e3f074082558ea885b96.jpg" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1068381.html">è®ºæ¢¦æƒ³åˆ†äº«åŠå…¶ç›®çš„ï¼ˆ2020å¹´ï¼‰ </a></div><span class="my_story_list_date">2021-6-26 11:37</span></div><div class="col-sm"><div><a target="_blank" href="/story/1068267.html"><img src="http://img2.diglog.com/img/2021/6/thumb_06ea97da1ba19fd0da53a6a87d67e332.jpg" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1068267.html">David Dobrikçš„æ¦‚å†µï¼Œä¸€ä¸ªæ„šè ¢çš„æ¶ä½œå‰§è‡­åæ˜­ç€çš„æ„šè ¢æ¶ä½œå‰§ï¼Œä»–ä»¬åœ¨2020å¹´èµšå–äº†1550ä¸‡ç¾å…ƒï¼Œç°åœ¨åœ¨ç«ç¾ä¸­é­åˆ°é€ æˆåˆ›ä¼¤ </a></div><span class="my_story_list_date">2021-6-26 0:28</span></div><div class="col-sm"><div><a target="_blank" href="/story/1067914.html"><img src="http://img2.diglog.com/img/2021/6/thumb_07fceb2227c245f6b5c2c09bec8685b9.jpg" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1067914.html">è‚¥çš‚çš„å†å²ï¼ˆ2020ï¼‰ </a></div><span class="my_story_list_date">2021-6-24 15:26</span></div><div class="col-sm"><div><a target="_blank" href="/story/1067690.html"><img src="http://img2.diglog.com/img/2021/6/thumb_ac47bd9ae7ffd39361a85c87a05d50c5.png" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1067690.html">è°·æ­Œç…§ç‰‡æ˜¯å¦‚æ­¤2020-æ¬¢è¿æ¥åˆ°è‡ªä¸»ç…§ç‰‡ç®¡ç†ä¸–ç•Œ </a></div><span class="my_story_list_date">2021-6-23 20:38</span></div></div></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è®¾è®¡/">#è®¾è®¡</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/åˆ›æ„/">#åˆ›æ„</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/æ‘„å½±/">#æ‘„å½±</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/å›¾ç‰‡/">#å›¾ç‰‡</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/æ¸¸æˆ/">#æ¸¸æˆ</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è½¯ä»¶/">#è½¯ä»¶</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è§†é¢‘/">#è§†é¢‘</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/æ‰‹æœº/">#æ‰‹æœº</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/å¹¿å‘Š/">#å¹¿å‘Š</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/å…è´¹/">#å…è´¹</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/ç½‘ç«™/">#ç½‘ç«™</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/ä¸‹è½½/">#ä¸‹è½½</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/å¾®è½¯/">#å¾®è½¯</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/éŸ³ä¹/">#éŸ³ä¹</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è‹¹æœ/">#è‹¹æœ</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/åšå®¢/">#åšå®¢</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/æ¶æ/">#æ¶æ</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è‰ºæœ¯/">#è‰ºæœ¯</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è°·æ­Œ/">#è°·æ­Œ</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/å·¥å…·/">#å·¥å…·</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>