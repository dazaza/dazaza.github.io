<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>GhOStï¼šLinuxè°ƒåº¦çš„å¿«é€Ÿçµæ´»çš„ç”¨æˆ·ç©ºé—´å§”æ´¾GhOSt: Fast and Flexible User-Space Delegation of Linux Scheduling</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">GhOSt: Fast and Flexible User-Space Delegation of Linux Scheduling<br/>GhOStï¼šLinuxè°ƒåº¦çš„å¿«é€Ÿçµæ´»çš„ç”¨æˆ·ç©ºé—´å§”æ´¾</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2022-02-14 23:49:28</div><div class="page_narrow text-break page_content"><p>This is one of the last papers Iâ€™m writing about from SOSP - I am trying out something new and publishing the queue of papers I plan on reading  here. These paper reviews can  be delivered weekly to your inbox, or you can subscribe to the  Atom feed. As always, feel free to reach out on  Twitter with feedback or suggestions!</p><p>è¿™æ˜¯æˆ‘åœ¨SOSPä¸Šå†™çš„æœ€åå‡ ç¯‡è®ºæ–‡ä¹‹ä¸€â€”â€”æˆ‘æ­£åœ¨å°è¯•ä¸€äº›æ–°çš„ä¸œè¥¿ï¼Œå¹¶åœ¨è¿™é‡Œå‘å¸ƒæˆ‘è®¡åˆ’é˜…è¯»çš„è®ºæ–‡é˜Ÿåˆ—ã€‚è¿™äº›è®ºæ–‡è¯„è®ºå¯ä»¥æ¯å‘¨å‘é€åˆ°ä½ çš„æ”¶ä»¶ç®±ï¼Œä¹Ÿå¯ä»¥è®¢é˜…Atomè®¢é˜…æºã€‚ä¸€å¦‚æ—¢å¾€ï¼Œè¯·éšæ—¶åœ¨Twitterä¸Šå‘è¡¨åé¦ˆæˆ–å»ºè®®ï¼</p><p>  The ghOSt paper describes a system for implementing Linux scheduling   This paper is about CPU scheduling, not data center scheduling (like I covered in a  previous paper review).  policies in user space   See  What is difference between User space and Kernel space?. . Operating system scheduling is more complicated for data center workloads, as there are additional factors to consider when deciding what to run and when (like ensuring low latency for user queries). Previous research aims to take higher-level context about applications into consideration when making scheduling decisions   One example scheduler,  Shinjuku, is designed to reduce tail latency. The approach is able to achieve up to 6.6Ã— higher throughput and 88% lower tail latency by implementing a custom scheduling policy. , with dramatic positive results.</p><p>è¿™ç¯‡ghOStè®ºæ–‡æè¿°äº†ä¸€ä¸ªå®ç°Linuxè°ƒåº¦çš„ç³»ç»Ÿã€‚è¿™ç¯‡è®ºæ–‡æ˜¯å…³äºCPUè°ƒåº¦çš„ï¼Œè€Œä¸æ˜¯æ•°æ®ä¸­å¿ƒè°ƒåº¦çš„ï¼ˆå°±åƒæˆ‘åœ¨ä¹‹å‰çš„è®ºæ–‡å›é¡¾ä¸­æåˆ°çš„ï¼‰ã€‚ç”¨æˆ·ç©ºé—´ä¸­çš„ç­–ç•¥æŸ¥çœ‹ç”¨æˆ·ç©ºé—´å’Œå†…æ ¸ç©ºé—´ä¹‹é—´çš„åŒºåˆ«ã€‚å¯¹äºæ•°æ®ä¸­å¿ƒå·¥ä½œè´Ÿè½½æ¥è¯´ï¼Œæ“ä½œç³»ç»Ÿè°ƒåº¦æ›´ä¸ºå¤æ‚ï¼Œå› ä¸ºåœ¨å†³å®šè¦è¿è¡Œä»€ä¹ˆæ—¶å€™ä»¥åŠä½•æ—¶ï¼ˆä¾‹å¦‚ï¼Œç¡®ä¿ç”¨æˆ·æŸ¥è¯¢çš„ä½å»¶è¿Ÿï¼‰æ—¶éœ€è¦è€ƒè™‘å…¶ä»–å› ç´ ã€‚ä»¥å‰çš„ç ”ç©¶æ—¨åœ¨åœ¨åšå‡ºè°ƒåº¦å†³ç­–æ—¶è€ƒè™‘åº”ç”¨ç¨‹åºçš„æ›´é«˜çº§åˆ«ä¸Šä¸‹æ–‡ã€‚ä¸€ä¸ªç¤ºä¾‹è°ƒåº¦å™¨æ–°å®¿ï¼ˆShinjukuï¼‰æ—¨åœ¨å‡å°‘å°¾éƒ¨å»¶è¿Ÿã€‚é€šè¿‡å®æ–½å®šåˆ¶çš„è°ƒåº¦ç­–ç•¥ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå®ç°é«˜è¾¾6.6å€çš„ååé‡å’Œ88%çš„å°¾éƒ¨å»¶è¿Ÿï¼Œå–å¾—äº†æ˜¾è‘—çš„ç§¯ææˆæœã€‚</p><p> Unfortunately, custom schedulers can be difficult to implement, deploy, and maintain.  Shinjuku is an example   The paper also cites a set of Dune-themed projects, like  Caladan and  Shenango as prior work in the space that runs into the coupling problem.  of a custom scheduler facing these problems - it is designed to reduce tail latency for data center applications, but requires tight coupling between an application and the scheduler. This tight coupling means that changes to the kernel could also unintentionally impact applications using the approach, potentially causing a brittle implementation with high ongoing maintenance costs.</p><p>ä¸å¹¸çš„æ˜¯ï¼Œå®šåˆ¶è°ƒåº¦å™¨å¯èƒ½å¾ˆéš¾å®ç°ã€éƒ¨ç½²å’Œç»´æŠ¤ã€‚æ–°å®¿å°±æ˜¯ä¸€ä¸ªä¾‹å­ã€‚è®ºæ–‡è¿˜å¼•ç”¨äº†ä¸€ç³»åˆ—ä»¥æ²™ä¸˜ä¸ºä¸»é¢˜çš„é¡¹ç›®ï¼Œæ¯”å¦‚å¡æ‹‰ä¸¹å’Œé›ªå—æˆˆï¼Œä½œä¸ºè¯¥é¢†åŸŸé‡åˆ°è€¦åˆé—®é¢˜çš„å‰æœŸå·¥ä½œã€‚ä¸€ä¸ªå®šåˆ¶è°ƒåº¦å™¨é¢ä¸´è¿™äº›é—®é¢˜çš„ä¾‹å­â€”â€”å®ƒæ—¨åœ¨å‡å°‘æ•°æ®ä¸­å¿ƒåº”ç”¨ç¨‹åºçš„å°¾éƒ¨å»¶è¿Ÿï¼Œä½†éœ€è¦åº”ç”¨ç¨‹åºå’Œè°ƒåº¦å™¨ä¹‹é—´çš„ç´§å¯†è€¦åˆã€‚è¿™ç§ç´§å¯†è€¦åˆæ„å‘³ç€å¯¹å†…æ ¸çš„æ›´æ”¹ä¹Ÿå¯èƒ½ä¼šæ— æ„ä¸­å½±å“ä½¿ç”¨è¿™ç§æ–¹æ³•çš„åº”ç”¨ç¨‹åºï¼Œå¯èƒ½ä¼šå¯¼è‡´è„†å¼±çš„å®ç°å’Œé«˜æ˜‚çš„æŒç»­ç»´æŠ¤æˆæœ¬ã€‚</p><p> ghOSt aims to address the problems faced by custom schedulers and those who implement them, while facilitating the dramatic performance and scalability gains workload-specific schedulers allow. The key to its approach is separating scheduling logic and the components that interact with the kernel. Custom schedulers, called  policies, are moved into user space.</p><p>ghOStæ—¨åœ¨è§£å†³å®šåˆ¶è°ƒåº¦å™¨åŠå…¶å®ç°è€…æ‰€é¢ä¸´çš„é—®é¢˜ï¼ŒåŒæ—¶ä¿ƒè¿›ç‰¹å®šäºå·¥ä½œè´Ÿè½½çš„è°ƒåº¦å™¨æ‰€å…è®¸çš„æ˜¾è‘—æ€§èƒ½å’Œå¯ä¼¸ç¼©æ€§æå‡ã€‚å…¶æ–¹æ³•çš„å…³é”®æ˜¯åˆ†ç¦»è°ƒåº¦é€»è¾‘å’Œä¸å†…æ ¸äº¤äº’çš„ç»„ä»¶ã€‚è¢«ç§°ä¸ºç­–ç•¥çš„è‡ªå®šä¹‰è°ƒåº¦ç¨‹åºè¢«ç§»åŠ¨åˆ°ç”¨æˆ·ç©ºé—´ä¸­ã€‚</p><p> In contrast, relatively stable code that interacts directly with the Linux kernel remains in kernel-space, and exposes an API for the user-space schedulers to interact with. This split approach means that custom schedulers run just like any other application - as a result, they can be implemented in variety of languages, tested using existing infrastructure, and deployed a faster rate for a wider set of workloads.</p><p>ç›¸æ¯”ä¹‹ä¸‹ï¼Œç›´æ¥ä¸Linuxå†…æ ¸äº¤äº’çš„ç›¸å¯¹ç¨³å®šçš„ä»£ç ä»ä¿ç•™åœ¨å†…æ ¸ç©ºé—´ä¸­ï¼Œå¹¶ä¸ºç”¨æˆ·ç©ºé—´è°ƒåº¦å™¨æä¾›äº†ä¸€ä¸ªä¸ä¹‹äº¤äº’çš„APIã€‚è¿™ç§æ‹†åˆ†æ–¹æ³•æ„å‘³ç€å®šåˆ¶è°ƒåº¦å™¨çš„è¿è¡Œä¸ä»»ä½•å…¶ä»–åº”ç”¨ç¨‹åºä¸€æ ·â€”â€”å› æ­¤ï¼Œå®ƒä»¬å¯ä»¥ç”¨å¤šç§è¯­è¨€å®ç°ï¼Œä½¿ç”¨ç°æœ‰åŸºç¡€è®¾æ–½è¿›è¡Œæµ‹è¯•ï¼Œå¹¶ä»¥æ›´å¿«çš„é€Ÿåº¦éƒ¨ç½²åˆ°æ›´å¹¿æ³›çš„å·¥ä½œè´Ÿè½½ä¸­ã€‚</p><p>  The paper makes three main contributions: design and implementation of a system that allows custom scheduling logic to run in user space, implementations of several custom schedulers using the system, and evaluation of the architecture (including in a production setting).</p><p>æœ¬æ–‡çš„ä¸»è¦è´¡çŒ®æœ‰ä¸‰ä¸ªï¼šè®¾è®¡å’Œå®ç°ä¸€ä¸ªå…è®¸è‡ªå®šä¹‰è°ƒåº¦é€»è¾‘åœ¨ç”¨æˆ·ç©ºé—´ä¸­è¿è¡Œçš„ç³»ç»Ÿï¼Œä½¿ç”¨è¯¥ç³»ç»Ÿå®ç°å¤šä¸ªè‡ªå®šä¹‰è°ƒåº¦ç¨‹åºï¼Œä»¥åŠè¯„ä¼°ä½“ç³»ç»“æ„ï¼ˆåŒ…æ‹¬åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼‰ã€‚</p><p>   Implementing schedulers is hard because of the constraints posed on kernel code, like restrictions on languages   Support for  Rust in the kernel is a work in progress.  and debug tooling   See a previous discussion on difficulties with kernel debugging on  HN. .</p><p>ç”±äºå¯¹å†…æ ¸ä»£ç çš„é™åˆ¶ï¼Œå®ç°è°ƒåº¦å™¨å¾ˆå›°éš¾ï¼Œæ¯”å¦‚å¯¹è¯­è¨€çš„é™åˆ¶ï¼Œåœ¨å†…æ ¸ä¸­æ”¯æŒç”Ÿé”ˆæ˜¯ä¸€é¡¹æ­£åœ¨è¿›è¡Œçš„å·¥ä½œã€‚è°ƒè¯•å·¥å…·è§å‰é¢å…³äºHNä¸Šå†…æ ¸è°ƒè¯•å›°éš¾çš„è®¨è®ºã€‚</p><p> Deploying schedulers is even harder because upgrading a kernel requires   Technically, not all changes to the kernel require a  reboot.  a time-consuming multi-step process of shifting workloads and rebooting the machine. The potential for kernel upgrades to introduce performance regressions make the process more difficult.</p><p>éƒ¨ç½²è°ƒåº¦ç¨‹åºæ›´åŠ å›°éš¾ï¼Œå› ä¸ºå‡çº§å†…æ ¸éœ€è¦æŠ€æœ¯æ”¯æŒï¼Œè€Œä¸æ˜¯æ‰€æœ‰å¯¹å†…æ ¸çš„æ›´æ”¹éƒ½éœ€è¦é‡æ–°å¯åŠ¨ã€‚è½¬ç§»å·¥ä½œè´Ÿè½½å’Œé‡æ–°å¯åŠ¨æœºå™¨çš„è€—æ—¶å¤šæ­¥éª¤è¿‡ç¨‹ã€‚å†…æ ¸å‡çº§å¯èƒ½ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œè¿™ä½¿å¾—è¿™ä¸ªè¿‡ç¨‹æ›´åŠ å›°éš¾ã€‚</p><p> Custom schedulers must schedule kernel-level threads, not user-level threads   See  Difference between user-level and kernel-supported threads?.  - scheduling user-level threads on top of kernel-level threads does not guarantee that the associated kernel-level threads are actually run   The paper notes two approaches that allow developers to overcome the limitations of user-level threads: â€œ(1) Dedicate CPUs to the native threads running the user-threads, thus guaranteeing implicit control. However, this option wastes resources at low workload utilization, because the dedicated CPUs cannot be shared with another application (see Â§4.2), and requires extensive coordination around scaling capacity. Alternatively, developers can (2) stay at the mercy of the native thread scheduler, allowing CPUs to be shared, but ultimately losing the control over response time that they turned to a user-level runtime for.â€ .</p><p>è‡ªå®šä¹‰è°ƒåº¦ç¨‹åºå¿…é¡»è°ƒåº¦å†…æ ¸çº§çº¿ç¨‹ï¼Œè€Œä¸æ˜¯ç”¨æˆ·çº§çº¿ç¨‹ã€‚è¯·å‚é˜…ç”¨æˆ·çº§çº¿ç¨‹å’Œå†…æ ¸æ”¯æŒçº¿ç¨‹ä¹‹é—´çš„åŒºåˆ«ï¼Ÿ-åœ¨å†…æ ¸çº§çº¿ç¨‹ä¹‹ä¸Šè°ƒåº¦ç”¨æˆ·çº§çº¿ç¨‹å¹¶ä¸èƒ½ä¿è¯ç›¸å…³çš„å†…æ ¸çº§çº¿ç¨‹å®é™…è¿è¡Œã€‚æœ¬æ–‡æŒ‡å‡ºäº†ä¸¤ç§å…è®¸å¼€å‘äººå‘˜å…‹æœç”¨æˆ·çº§çº¿ç¨‹é™åˆ¶çš„æ–¹æ³•ï¼šâ€œï¼ˆ1ï¼‰å°†CPUä¸“ç”¨äºè¿è¡Œç”¨æˆ·çº¿ç¨‹çš„æœ¬æœºçº¿ç¨‹ï¼Œä»è€Œä¿è¯éšå¼æ§åˆ¶ã€‚ç„¶è€Œï¼Œæ­¤é€‰é¡¹åœ¨ä½å·¥ä½œè´Ÿè½½åˆ©ç”¨ç‡ä¸‹æµªè´¹èµ„æºï¼Œå› ä¸ºä¸“ç”¨CPUæ— æ³•ä¸å…¶ä»–åº”ç”¨ç¨‹åºå…±äº«ï¼ˆè¯·å‚è§Â§4.2ï¼‰ï¼Œå¹¶ä¸”éœ€è¦å›´ç»•æ‰©å±•å®¹é‡è¿›è¡Œå¹¿æ³›åè°ƒã€‚æˆ–è€…ï¼Œå¼€å‘äººå‘˜å¯ä»¥ï¼ˆ2ï¼‰ä»»ç”±æœ¬æœºçº¿ç¨‹è°ƒåº¦å™¨æ‘†å¸ƒï¼Œå…è®¸å…±äº«CPUï¼Œä½†æœ€ç»ˆä¼šå¤±å»å¯¹å“åº”æ—¶é—´çš„æ§åˆ¶ï¼Œè€Œè¿™æ­£æ˜¯ä»–ä»¬è½¬å‘ç”¨æˆ·çº§è¿è¡Œæ—¶çš„åŸå› ã€‚" .</p><p> Custom schedulers tailored to specific workloads pose their own challenges because they do not adapt well to different use cases (not to mention their internals are complex and potentially not shared across multiple schedulers).</p><p>ä¸ºç‰¹å®šå·¥ä½œè´Ÿè½½å®šåˆ¶çš„å®šåˆ¶è°ƒåº¦å™¨ä¹Ÿå¸¦æ¥äº†æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒä»¬ä¸èƒ½å¾ˆå¥½åœ°é€‚åº”ä¸åŒçš„ç”¨ä¾‹ï¼ˆæ›´ä¸ç”¨è¯´å®ƒä»¬çš„å†…éƒ¨ç»“æ„å¾ˆå¤æ‚ï¼Œå¯èƒ½æ— æ³•åœ¨å¤šä¸ªè°ƒåº¦å™¨ä¹‹é—´å…±äº«ï¼‰ã€‚</p><p> Existing custom scheduling techniques are not sufficient, in particular Berkeley Packet Filter (BPF)   Julia Evans has a great post on  BPF, which was originally designed to capture and filter packets inside of the kernel. More recently,  eBPF extends the idea to other parts of the kernel - see  A thorough introduction to eBPF for more details on how BPF/eBPF works. There is also an exciting ecosystem building around eBPF tooling, like  Cilium and Isovalent, the company behind the tool, recently raised money from  Andreessen Horowitz. . While BPF programs are amazingly cool, they run synchronously and block the CPU - non-ideal from a performance perspective   It is worth noting that the paper does mention using BPF for fast-path .</p><p>ç°æœ‰çš„å®šåˆ¶è°ƒåº¦æŠ€æœ¯æ˜¯ä¸å¤Ÿçš„ï¼Œå°¤å…¶æ˜¯ä¼¯å…‹åˆ©æ•°æ®åŒ…è¿‡æ»¤å™¨ï¼ˆBPFï¼‰Julia Evansåœ¨BPFä¸Šå‘è¡¨äº†ä¸€ç¯‡å¾ˆå¥½çš„æ–‡ç« ï¼Œå®ƒæœ€åˆè®¾è®¡ç”¨äºæ•è·å’Œè¿‡æ»¤å†…æ ¸å†…éƒ¨çš„æ•°æ®åŒ…ã€‚æœ€è¿‘ï¼ŒeBPFå°†è¿™ä¸ªæƒ³æ³•æ‰©å±•åˆ°äº†å†…æ ¸çš„å…¶ä»–éƒ¨åˆ†â€”â€”æœ‰å…³BPF/eBPFå¦‚ä½•å·¥ä½œçš„æ›´å¤šç»†èŠ‚ï¼Œè¯·å‚é˜…eBPFçš„å…¨é¢ä»‹ç»ã€‚å›´ç»•eBPFå·¥å…·è¿˜æœ‰ä¸€ä¸ªä»¤äººå…´å¥‹çš„ç”Ÿæ€ç³»ç»Ÿå»ºè®¾ï¼Œæ¯”å¦‚è¯¥å·¥å…·èƒŒåçš„å…¬å¸Cilium and Isopalentæœ€è¿‘ä»Andreessen Horowitzé‚£é‡Œç­¹é›†äº†èµ„é‡‘ã€‚è™½ç„¶BPFç¨‹åºéå¸¸é…·ï¼Œä½†å®ƒä»¬åŒæ­¥è¿è¡Œå¹¶é˜»å¡CPUâ€”â€”ä»æ€§èƒ½è§’åº¦æ¥çœ‹ï¼Œè¿™å¹¶ä¸ç†æƒ³ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæœ¬æ–‡ç¡®å®æåˆ°ä½¿ç”¨BPFå®ç°å¿«é€Ÿè·¯å¾„ã€‚</p><p>  Custom scheduling logic should be easy to implement and test: separating scheduling logic from the kernel simplifies development and testing.</p><p>è‡ªå®šä¹‰è°ƒåº¦é€»è¾‘åº”è¯¥æ˜“äºå®ç°å’Œæµ‹è¯•ï¼šå°†è°ƒåº¦é€»è¾‘ä¸å†…æ ¸åˆ†ç¦»å¯ä»¥ç®€åŒ–å¼€å‘å’Œæµ‹è¯•ã€‚</p><p> It should be possible to easily create scheduling logic for many different use cases: unlike previous specialized schedulers built into the kernel,  ghOSt aims to be a generic platform that schedulers can be built on top of.</p><p>åº”è¯¥å¯ä»¥è½»æ¾åœ°ä¸ºè®¸å¤šä¸åŒçš„ç”¨ä¾‹åˆ›å»ºè°ƒåº¦é€»è¾‘ï¼šä¸ä»¥å‰å†…ç½®åœ¨å†…æ ¸ä¸­çš„ä¸“ç”¨è°ƒåº¦ç¨‹åºä¸åŒï¼ŒghOStæ—¨åœ¨æˆä¸ºä¸€ä¸ªé€šç”¨å¹³å°ï¼Œå¯ä»¥åœ¨å…¶ä¸Šæ„å»ºè°ƒåº¦ç¨‹åºã€‚</p><p> Scheduling should be able to operate across multiple CPUs: existing Linux schedulers make per-CPU scheduling decisions and it is difficult to execute scheduling decisions over a set of CPUs to optimize for other properties, like tail latency   The paper cites a number of previous systems (like  Shenango: Achieving high CPU efficiency for latency-sensitive datacenter workloads) that achieve their goals by scheduling across multiple CPUs .</p><p>è°ƒåº¦åº”è¯¥èƒ½å¤Ÿè·¨å¤šä¸ªCPUè¿è¡Œï¼šç°æœ‰çš„Linuxè°ƒåº¦ç¨‹åºä¼šåšå‡ºæ¯ä¸ªCPUçš„è°ƒåº¦å†³ç­–ï¼Œå¹¶ä¸”å¾ˆéš¾åœ¨ä¸€ç»„CPUä¸Šæ‰§è¡Œè°ƒåº¦å†³ç­–ä»¥ä¼˜åŒ–å…¶ä»–å±æ€§ï¼Œåƒtail latencyä¸€æ ·ï¼Œæœ¬æ–‡å¼•ç”¨äº†è®¸å¤šä»¥å‰çš„ç³»ç»Ÿï¼ˆæ¯”å¦‚Shenangoï¼šä¸ºå»¶è¿Ÿæ•æ„Ÿçš„æ•°æ®ä¸­å¿ƒå·¥ä½œè´Ÿè½½å®ç°é«˜CPUæ•ˆç‡ï¼‰ï¼Œè¿™äº›ç³»ç»Ÿé€šè¿‡è·¨å¤šä¸ªCPUè¿›è¡Œè°ƒåº¦æ¥å®ç°å…¶ç›®æ ‡ã€‚</p><p> Non-disruptive updates and fault isolation: it should be easy to deploy scheduling logic like one would with other tasks running on a machine, allowing updates without requiring a reboot. Furthermore, failures or regressions in scheduling policies should not crash the whole machine.</p><p>æ— ä¸­æ–­æ›´æ–°å’Œæ•…éšœéš”ç¦»ï¼šåº”è¯¥å¾ˆå®¹æ˜“éƒ¨ç½²è°ƒåº¦é€»è¾‘ï¼Œå°±åƒåœ¨æœºå™¨ä¸Šè¿è¡Œå…¶ä»–ä»»åŠ¡ä¸€æ ·ï¼Œå…è®¸åœ¨ä¸éœ€è¦é‡æ–°å¯åŠ¨çš„æƒ…å†µä¸‹è¿›è¡Œæ›´æ–°ã€‚æ­¤å¤–ï¼Œè°ƒåº¦ç­–ç•¥ä¸­çš„æ•…éšœæˆ–å€’é€€ä¸åº”ä½¿æ•´ä¸ªæœºå™¨å´©æºƒã€‚</p><p>  To achieve the goals of the system, ghOSt introduces  policies (custom scheduling logic).  Policies are executed in user-space and associated scheduling decisions are communicated to the kernel.</p><p>ä¸ºäº†å®ç°ç³»ç»Ÿçš„ç›®æ ‡ï¼ŒghOStå¼•å…¥äº†ç­–ç•¥ï¼ˆè‡ªå®šä¹‰è°ƒåº¦é€»è¾‘ï¼‰ã€‚ç­–ç•¥åœ¨ç”¨æˆ·ç©ºé—´ä¸­æ‰§è¡Œï¼Œç›¸å…³çš„è°ƒåº¦å†³ç­–è¢«ä¼ é€åˆ°å†…æ ¸ã€‚</p><p>  Policies (and their scheduling decisions) propagate over three main components running across kernel and user space:</p><p>ç­–ç•¥ï¼ˆåŠå…¶è°ƒåº¦å†³ç­–ï¼‰åœ¨è¿è¡Œäºå†…æ ¸å’Œç”¨æˆ·ç©ºé—´çš„ä¸‰ä¸ªä¸»è¦ç»„ä»¶ä¸Šä¼ æ’­ï¼š</p><p> The  ghOSt scheduling class   Here is a great article about scheduling classes and Linuxâ€™s  Completely Fair Scheduler. There is also the  man page about the related  sched system call.  runs inside of the Linux kernel and provides a syscall interface that other components use to communicate scheduling decisions.</p><p>è¿™é‡Œçš„ghOStè°ƒåº¦ç±»æ˜¯ä¸€ç¯‡å…³äºè°ƒåº¦ç±»å’ŒLinuxçš„å®Œå…¨å…¬å¹³è°ƒåº¦ç¨‹åºçš„ä¼Ÿå¤§æ–‡ç« ã€‚è¿˜æœ‰ä¸€ä¸ªå…³äºç›¸å…³schedç³»ç»Ÿè°ƒç”¨çš„æ‰‹å†Œé¡µã€‚åœ¨Linuxå†…æ ¸å†…éƒ¨è¿è¡Œï¼Œå¹¶æä¾›ä¸€ä¸ªç³»ç»Ÿè°ƒç”¨æ¥å£ï¼Œå…¶ä»–ç»„ä»¶ä½¿ç”¨è¯¥æ¥å£æ¥ä¼ è¾¾è°ƒåº¦å†³ç­–ã€‚</p><p> Agents run  policies (custom scheduling logic) in user-space, and make scheduling decisions that they communicate to the  ghOSt scheduling class running in kernel-space.</p><p>ä»£ç†åœ¨ç”¨æˆ·ç©ºé—´ä¸­è¿è¡Œç­–ç•¥ï¼ˆè‡ªå®šä¹‰è°ƒåº¦é€»è¾‘ï¼‰ï¼Œå¹¶åšå‡ºè°ƒåº¦å†³ç­–ï¼Œå¹¶ä¸å†…æ ¸ç©ºé—´ä¸­è¿è¡Œçš„ghOStè°ƒåº¦ç±»é€šä¿¡ã€‚</p><p> Enclaves are groups of  agents. Each  enclave has a primary agent that makes the scheduling decisions. Assigning multiple  agents to an enclave provides redundancy in the case of the primary agent failing.</p><p>é£åœ°æ˜¯ä¸€ç¾¤ç‰¹å·¥ã€‚æ¯ä¸ªé£åœ°éƒ½æœ‰ä¸€ä¸ªä¸»è¦ä»£ç†ï¼Œè´Ÿè´£åˆ¶å®šè°ƒåº¦å†³ç­–ã€‚å°†å¤šä¸ªä»£ç†åˆ†é…ç»™ä¸€ä¸ªenclaveå¯ä»¥åœ¨ä¸»ä»£ç†å¤±è´¥çš„æƒ…å†µä¸‹æä¾›å†—ä½™ã€‚</p><p>   ghOSt components running in kernel or user-space need a way to provide information and feedback to each other. The paper discusses the two primary communication flows:  kernel-to-agent and  agent-to-kernel.</p><p>åœ¨å†…æ ¸æˆ–ç”¨æˆ·ç©ºé—´ä¸­è¿è¡Œçš„ghOStç»„ä»¶éœ€è¦ä¸€ç§ç›¸äº’æä¾›ä¿¡æ¯å’Œåé¦ˆçš„æ–¹å¼ã€‚æœ¬æ–‡è®¨è®ºäº†ä¸¤ç§ä¸»è¦çš„é€šä¿¡æµï¼šå†…æ ¸åˆ°ä»£ç†å’Œä»£ç†åˆ°å†…æ ¸ã€‚</p><p>  In the  kernel-to-agent flow, the  kernel communicates to  agents using messages and message queues   Definition of the messages  here. . The kernel sends messages on queues when events happen in the kernel that could impact scheduling decisions. Each CPU has an associated queue, and each queue is associated with an enclave   Not every agent has a message queue because in some configurations there is a single primary agent for the enclave that is receiving information from the kernel - reference the enclave diagram above for a visual representation of this idea. . While there are several existing queue approaches (including  io_uring or  BPF ring buffers), not all kernel versions support them - the authors argue that this makes ghOStâ€™s queue abstraction necessary.</p><p>åœ¨å†…æ ¸åˆ°ä»£ç†çš„æµç¨‹ä¸­ï¼Œå†…æ ¸ä½¿ç”¨æ¶ˆæ¯å’Œæ¶ˆæ¯é˜Ÿåˆ—ä¸ä»£ç†è¿›è¡Œé€šä¿¡ã€‚æ­¤å¤„å®šä¹‰äº†æ¶ˆæ¯ã€‚å½“å†…æ ¸ä¸­å‘ç”Ÿå¯èƒ½å½±å“è°ƒåº¦å†³ç­–çš„äº‹ä»¶æ—¶ï¼Œå†…æ ¸ä¼šåœ¨é˜Ÿåˆ—ä¸Šå‘é€æ¶ˆæ¯ã€‚æ¯ä¸ªCPUéƒ½æœ‰ä¸€ä¸ªå…³è”çš„é˜Ÿåˆ—ï¼Œæ¯ä¸ªé˜Ÿåˆ—éƒ½ä¸ä¸€ä¸ªenclaveå…³è”ã€‚å¹¶éæ¯ä¸ªä»£ç†éƒ½æœ‰ä¸€ä¸ªæ¶ˆæ¯é˜Ÿåˆ—ï¼Œå› ä¸ºåœ¨æŸäº›é…ç½®ä¸­ï¼Œenclaveåªæœ‰ä¸€ä¸ªä¸»ä»£ç†ä»å†…æ ¸æ¥æ”¶ä¿¡æ¯â€”â€”è¯·å‚è€ƒä¸Šé¢çš„enclaveå›¾ï¼Œä»¥è·å¾—æ­¤æƒ³æ³•çš„å¯è§†åŒ–è¡¨ç¤ºã€‚è™½ç„¶æœ‰å‡ ç§ç°æœ‰çš„é˜Ÿåˆ—æ–¹æ³•ï¼ˆåŒ…æ‹¬io__-uringæˆ–BPFç¯å½¢ç¼“å†²åŒºï¼‰ï¼Œä½†å¹¶ä¸æ˜¯æ‰€æœ‰çš„å†…æ ¸ç‰ˆæœ¬éƒ½æ”¯æŒå®ƒä»¬â€”â€”ä½œè€…è®¤ä¸ºè¿™ä½¿å¾—ghOStçš„é˜Ÿåˆ—æŠ½è±¡æˆä¸ºå¿…è¦ã€‚</p><p> In the  agent-to-kernel direction, the  agent communicates by making system calls to communicate scheduling decisions and to perform management operations on the shared queue. To send scheduling decisions, the  agent creates and commits transactions (like  TXN_CREATE() and  TXNS_COMMIT()). Transactions are important because they allow a policy to make scheduling decisions across a range of CPUs, ensuring all or none succeed, while batching scheduling information - batching is critical because it limits the number of interrupts that impact the to-be-scheduled CPUs (as the kernel component of ghOSt needs to respond to agent transactions).</p><p>åœ¨ä»£ç†åˆ°å†…æ ¸çš„æ–¹å‘ä¸Šï¼Œä»£ç†é€šè¿‡è¿›è¡Œç³»ç»Ÿè°ƒç”¨è¿›è¡Œé€šä¿¡ï¼Œä»¥ä¼ è¾¾è°ƒåº¦å†³ç­–ï¼Œå¹¶å¯¹å…±äº«é˜Ÿåˆ—æ‰§è¡Œç®¡ç†æ“ä½œã€‚ä¸ºäº†å‘é€è°ƒåº¦å†³ç­–ï¼Œä»£ç†åˆ›å»ºå¹¶æäº¤äº‹åŠ¡ï¼ˆæ¯”å¦‚TXN_CREATEï¼ˆï¼‰å’ŒTXNS_COMMITï¼ˆï¼‰ï¼‰ã€‚äº‹åŠ¡éå¸¸é‡è¦ï¼Œå› ä¸ºå®ƒä»¬å…è®¸ç­–ç•¥è·¨ä¸€ç³»åˆ—CPUåšå‡ºè°ƒåº¦å†³ç­–ï¼Œç¡®ä¿æ‰€æœ‰æˆ–æ‰€æœ‰äº‹åŠ¡éƒ½èƒ½æˆåŠŸï¼Œæ‰¹å¤„ç†è°ƒåº¦ä¿¡æ¯â€”â€”æ‰¹å¤„ç†éå¸¸å…³é”®ï¼Œå› ä¸ºå®ƒé™åˆ¶äº†å½±å“å¾…è°ƒåº¦CPUçš„ä¸­æ–­æ•°é‡ï¼ˆå› ä¸ºghOStçš„æ ¸å¿ƒç»„ä»¶éœ€è¦å“åº”ä»£ç†äº‹åŠ¡ï¼‰ã€‚</p><p> Lastly, there is a challenge to both  kernel-to-agent and  agent-to-kernel communication: keeping up to date with the state of the system. The kernel needs to ensure that it doesnâ€™t execute out of date scheduling decisions, and the agent need to make sure that it doesnâ€™t make scheduling decisions based on an old state of the world. The key piece of information used to track state is a  sequence number that exists for every agent.</p><p>æœ€åï¼Œå†…æ ¸åˆ°ä»£ç†å’Œä»£ç†åˆ°å†…æ ¸çš„é€šä¿¡éƒ½é¢ä¸´ä¸€ä¸ªæŒ‘æˆ˜ï¼šè·Ÿä¸Šç³»ç»Ÿçš„çŠ¶æ€ã€‚å†…æ ¸éœ€è¦ç¡®ä¿å®ƒä¸ä¼šæ‰§è¡Œè¿‡æ—¶çš„è°ƒåº¦å†³ç­–ï¼Œä»£ç†éœ€è¦ç¡®ä¿å®ƒä¸ä¼šæ ¹æ®æ—§çš„ä¸–ç•ŒçŠ¶æ€åšå‡ºè°ƒåº¦å†³ç­–ã€‚ç”¨äºè·Ÿè¸ªçŠ¶æ€çš„å…³é”®ä¿¡æ¯æ˜¯å­˜åœ¨äºæ¯ä¸ªä»£ç†çš„åºåˆ—å·ã€‚</p><p> In  kernel-to-agent commmunication, the kernel provides the  sequence number to agents in each message, and in a shared memory region. The sequence number in shared memory is updated by the kernel whenever it publishes a new message. The agent consumes the  sequence number from shared memory when reading messages from the queue, comparing the value to the  sequence number in shared memory. When the sequence number from consumed messages matches the value in shared memory, the agent knows it has read an up to date state.</p><p>åœ¨å†…æ ¸åˆ°ä»£ç†çš„é€šä¿¡ä¸­ï¼Œå†…æ ¸åœ¨æ¯æ¡æ¶ˆæ¯å’Œå…±äº«å†…å­˜åŒºåŸŸä¸­ä¸ºä»£ç†æä¾›åºåˆ—å·ã€‚æ¯å½“å†…æ ¸å‘å¸ƒæ–°æ¶ˆæ¯æ—¶ï¼Œå…±äº«å†…å­˜ä¸­çš„åºåˆ—å·å°±ä¼šè¢«æ›´æ–°ã€‚å½“ä»é˜Ÿåˆ—ä¸­è¯»å–æ¶ˆæ¯æ—¶ï¼Œä»£ç†ä¼šä½¿ç”¨å…±äº«å†…å­˜ä¸­çš„åºåˆ—å·ï¼Œå¹¶å°†è¯¥å€¼ä¸å…±äº«å†…å­˜ä¸­çš„åºåˆ—å·è¿›è¡Œæ¯”è¾ƒã€‚å½“å·²ä½¿ç”¨æ¶ˆæ¯çš„åºåˆ—å·ä¸å…±äº«å†…å­˜ä¸­çš„å€¼åŒ¹é…æ—¶ï¼Œä»£ç†çŸ¥é“å®ƒå·²è¯»å–æœ€æ–°çŠ¶æ€ã€‚</p><p> In  agent-to-kernel communication, the agent includes the  sequence number when sending scheduling decisions (via transactions) to the kernel. The kernel compares the  sequence number from the agentâ€™s transaction with the most recent sequence number the kernel is aware of. If the transactionâ€™s sequence number is too old, the kernel doesnâ€™t execute the scheduling decision.</p><p>åœ¨ä»£ç†åˆ°å†…æ ¸çš„é€šä¿¡ä¸­ï¼Œä»£ç†åœ¨å‘å†…æ ¸å‘é€è°ƒåº¦å†³ç­–ï¼ˆé€šè¿‡äº‹åŠ¡ï¼‰æ—¶åŒ…å«åºåˆ—å·ã€‚å†…æ ¸å°†ä»£ç†äº‹åŠ¡ä¸­çš„åºåˆ—å·ä¸å†…æ ¸çŸ¥é“çš„æœ€æ–°åºåˆ—å·è¿›è¡Œæ¯”è¾ƒã€‚å¦‚æœäº‹åŠ¡çš„åºåˆ—å·å¤ªæ—§ï¼Œå†…æ ¸å°±ä¸ä¼šæ‰§è¡Œè°ƒåº¦å†³ç­–ã€‚</p><p>  To evaluate ghOSt, the paper considers the overheads associated with the system, compares ghOSt to previous custom scheduler implementations, and evaluates the system in production.</p><p>ä¸ºäº†è¯„ä¼°ghOStï¼Œæœ¬æ–‡è€ƒè™‘äº†ä¸ç³»ç»Ÿç›¸å…³çš„å¼€é”€ï¼Œå°†ghOStä¸ä»¥å‰çš„å®šåˆ¶è°ƒåº¦å™¨å®ç°è¿›è¡Œäº†æ¯”è¾ƒï¼Œå¹¶è¯„ä¼°äº†ç”Ÿäº§ä¸­çš„ç³»ç»Ÿã€‚</p><p>  To evaluate the overheads of the system, the paper includes microbenchmarks that show the time spent in the different parts of the scheduling system, showing that it is competitive.</p><p>ä¸ºäº†è¯„ä¼°ç³»ç»Ÿçš„å¼€é”€ï¼Œæœ¬æ–‡åŒ…æ‹¬äº†å¾®åŸºå‡†ï¼Œæ˜¾ç¤ºäº†åœ¨è°ƒåº¦ç³»ç»Ÿçš„ä¸åŒéƒ¨åˆ†èŠ±è´¹çš„æ—¶é—´ï¼Œè¡¨æ˜å®ƒå…·æœ‰ç«äº‰åŠ›ã€‚</p><p>  The paper also determines the performance of a global scheduler (that schedules all cores on a system) implemented with ghOSt - previous research shows the potential advantage of this approach as the scheduler has more complete knowledge of the system. The evaluation shows that ghOSt is able to scale to millions of transactions, even when responsible for many CPUs.</p><p>æœ¬æ–‡è¿˜ç¡®å®šäº†ä½¿ç”¨ghOStå®ç°çš„å…¨å±€è°ƒåº¦å™¨ï¼ˆå¯¹ç³»ç»Ÿä¸Šçš„æ‰€æœ‰æ ¸å¿ƒè¿›è¡Œè°ƒåº¦ï¼‰çš„æ€§èƒ½â€”â€”ä¹‹å‰çš„ç ”ç©¶è¡¨æ˜ï¼Œéšç€è°ƒåº¦å™¨å¯¹ç³»ç»Ÿæœ‰æ›´å…¨é¢çš„äº†è§£ï¼Œè¿™ç§æ–¹æ³•çš„æ½œåœ¨ä¼˜åŠ¿ã€‚è¯„ä¼°è¡¨æ˜ï¼ŒghOStèƒ½å¤Ÿæ‰©å±•åˆ°æ•°ç™¾ä¸‡ä¸ªäº‹åŠ¡ï¼Œå³ä½¿å®ƒè´Ÿè´£è®¸å¤šCPUã€‚</p><p>   Next, the paper compares ghOSt to Shinjuku   See the  Shinjuku paper. , an example of a custom scheduling system tailored to reduce tail latency. The goal of this evaluation is to see whether  ghOSt performs similarly to a custom scheduler (which theoretically could achieve higher performance by using tailored optimization techniques). Shinjuku has a number of differences from  ghOSt - it uses dedicated resources (spinning threads that consume all of a CPU or set of CPUs), is constrained to a physical set of cores, and takes advantage of virtualization features to increase performance (like  posted interrupts). The authors also port the Shinjuku scheduling policy itself so that it is compatible with ghOSt.</p><p>æ¥ä¸‹æ¥ï¼Œè¿™ç¯‡è®ºæ–‡å°†é¬¼é­‚ä¸æ–°å®¿è¿›è¡Œäº†æ¯”è¾ƒã€‚å‚è§æ–°å®¿è®ºæ–‡ï¼Œè¿™æ˜¯ä¸€ä¸ªå®šåˆ¶çš„è°ƒåº¦ç³»ç»Ÿçš„ä¾‹å­ï¼Œå¯ä»¥å‡å°‘å°¾éƒ¨å»¶è¿Ÿã€‚æ­¤è¯„ä¼°çš„ç›®æ ‡æ˜¯æŸ¥çœ‹ghOStçš„æ€§èƒ½æ˜¯å¦ä¸å®šåˆ¶è°ƒåº¦å™¨ç±»ä¼¼ï¼ˆç†è®ºä¸Šï¼Œé€šè¿‡ä½¿ç”¨å®šåˆ¶çš„ä¼˜åŒ–æŠ€æœ¯ï¼Œå®šåˆ¶è°ƒåº¦å™¨å¯ä»¥å®ç°æ›´é«˜çš„æ€§èƒ½ï¼‰ã€‚æ–°å®¿ä¸ghOStæœ‰å¾ˆå¤šä¸åŒä¹‹å¤„â€”â€”å®ƒä½¿ç”¨ä¸“ç”¨èµ„æºï¼ˆå ç”¨å…¨éƒ¨CPUæˆ–ä¸€ç»„CPUçš„æ—‹è½¬çº¿ç¨‹ï¼‰ï¼Œå—é™äºä¸€ç»„ç‰©ç†å†…æ ¸ï¼Œå¹¶åˆ©ç”¨è™šæ‹ŸåŒ–åŠŸèƒ½æ¥æé«˜æ€§èƒ½ï¼ˆå¦‚å‘å¸ƒä¸­æ–­ï¼‰ã€‚ä½œè€…è¿˜ç§»æ¤äº†æ–°å®¿è°ƒåº¦ç­–ç•¥æœ¬èº«ï¼Œä»¥ä¾¿ä¸ghOStå…¼å®¹ã€‚</p><p> The two systems run a generated workload, â€œin which each request includes a GET query to an in-memory RocksDB key-value store and performs a small amount of processingâ€.</p><p>è¿™ä¸¤ä¸ªç³»ç»Ÿè¿è¡Œç”Ÿæˆçš„å·¥ä½œè´Ÿè½½ï¼Œâ€œå…¶ä¸­æ¯ä¸ªè¯·æ±‚éƒ½åŒ…æ‹¬å¯¹å†…å­˜ä¸­RocksDBé”®å€¼å­˜å‚¨çš„GETæŸ¥è¯¢ï¼Œå¹¶æ‰§è¡Œå°‘é‡å¤„ç†â€ã€‚</p><p>  ghOSt is competitive with Shinjuku for ğœ‡s-scale tail workloads, even though its Shinjuku policy is implemented in 82% fewer lines of code than the custom Shinjuku data plane system. ghOSt has slightly higher tail latencies than Shinjuku at high loads and is within 5% of Shinjukuâ€™s saturation throughput.</p><p>é¬¼é­‚ä¸æ–°å®¿ç«äº‰ğœ‡sè§„æ¨¡çš„å°¾éƒ¨å·¥ä½œè´Ÿè½½ï¼Œå°½ç®¡å…¶æ–°å®¿ç­–ç•¥çš„ä»£ç è¡Œæ¯”å®šåˆ¶çš„æ–°å®¿æ•°æ®å¹³é¢ç³»ç»Ÿå°‘82%ã€‚ghOStåœ¨é«˜è´Ÿè½½ä¸‹çš„å°¾éƒ¨å»¶è¿Ÿç•¥é«˜äºæ–°å®¿ï¼Œåœ¨æ–°å®¿é¥±å’Œååé‡çš„5%ä»¥å†…ã€‚</p><p>   Lastly, the paper runs a production workload against ghOSt and compares the results to the same workload executed by machines using the completely fair scheduler (CFS)   More info on the Completely Fair Scheduler  here - on the older side, but seems like it was updated relatively recently. .</p><p>æœ€åï¼Œæœ¬æ–‡é’ˆå¯¹ghOStè¿è¡Œäº†ä¸€ä¸ªç”Ÿäº§å·¥ä½œè´Ÿè½½ï¼Œå¹¶å°†ç»“æœä¸ä½¿ç”¨å®Œå…¨å…¬å¹³è°ƒåº¦ç¨‹åºï¼ˆCFSï¼‰çš„æœºå™¨æ‰§è¡Œçš„ç›¸åŒå·¥ä½œè´Ÿè½½è¿›è¡Œäº†æ¯”è¾ƒã€‚å…³äºå®Œå…¨å…¬å¹³è°ƒåº¦ç¨‹åºçš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…æ­¤å¤„çš„â€œè¾ƒæ—§â€éƒ¨åˆ†ï¼Œä½†å®ƒä¼¼ä¹æ˜¯æœ€è¿‘æ›´æ–°çš„ã€‚</p><p> The workload contains three query types (CPU and memory bound, IO and memory bound, and CPU-bound) - ghOSt is able to reduce tail-latency for the first two types of requests, but doesnâ€™t have a huge impact for the third   The paper does note that it is possible to impact compute bound tasks by extending the ghOSt policy with similar logic to what Linuxâ€™s CFS contains around  nice values. .</p><p>å·¥ä½œè´Ÿè½½åŒ…å«ä¸‰ç§æŸ¥è¯¢ç±»å‹ï¼ˆCPUå’Œå†…å­˜ç»‘å®šã€IOå’Œå†…å­˜ç»‘å®šä»¥åŠCPUç»‘å®šï¼‰-ghOStèƒ½å¤Ÿå‡å°‘å‰ä¸¤ç§è¯·æ±‚çš„å°¾éƒ¨å»¶è¿Ÿï¼Œä½†æ˜¯å¯¹ç¬¬ä¸‰ä¸ªæ²¡æœ‰å¤ªå¤§çš„å½±å“ã€‚è®ºæ–‡ç¡®å®æŒ‡å‡ºï¼Œé€šè¿‡ä½¿ç”¨ç±»ä¼¼äºLinuxçš„CFSåŒ…å«çš„niceå€¼çš„é€»è¾‘æ¥æ‰©å±•ghOStç­–ç•¥ï¼Œå¯ä»¥å½±å“è®¡ç®—ç»‘å®šçš„ä»»åŠ¡ã€‚</p><p> What stood out to me the most about this section is actually ghOStâ€™s impact on developer productivity:</p><p>åœ¨æˆ‘çœ‹æ¥ï¼Œè¿™ä¸€éƒ¨åˆ†æœ€çªå‡ºçš„æ˜¯ghOStå¯¹å¼€å‘äººå‘˜ç”Ÿäº§åŠ›çš„å½±å“ï¼š</p><p> When developing a kernel scheduler, the write-test-write cycle includes (a) compiling a kernel (up to 15 minutes), (b) deploying the kernel (10-20 minutes), and (c) running the test (1 hour due to database initialization following a reboot). As a result, the enthusiastic kernel developer experiments with 5 variants per day. With ghOSt, compiling, deploying and launching the new agent is comfortably done within one minute.</p><p>åœ¨å¼€å‘å†…æ ¸è°ƒåº¦å™¨æ—¶ï¼Œå†™æµ‹è¯•å†™å‘¨æœŸåŒ…æ‹¬ï¼ˆaï¼‰ç¼–è¯‘å†…æ ¸ï¼ˆæœ€å¤š15åˆ†é’Ÿï¼‰ï¼Œï¼ˆbï¼‰éƒ¨ç½²å†…æ ¸ï¼ˆ10-20åˆ†é’Ÿï¼‰ï¼Œä»¥åŠï¼ˆcï¼‰è¿è¡Œæµ‹è¯•ï¼ˆç”±äºé‡å¯åæ•°æ®åº“åˆå§‹åŒ–ï¼Œéœ€è¦1å°æ—¶ï¼‰ã€‚å› æ­¤ï¼Œè¿™ä½çƒ­å¿ƒçš„å†…æ ¸å¼€å‘äººå‘˜æ¯å¤©éƒ½è¦ç”¨5ç§å˜ä½“è¿›è¡Œå®éªŒã€‚æœ‰äº†ghOStï¼Œæ–°ä»£ç†çš„ç¼–è¯‘ã€éƒ¨ç½²å’Œå¯åŠ¨å¯ä»¥è½»æ¾åœ°åœ¨ä¸€åˆ†é’Ÿå†…å®Œæˆã€‚</p><p>  The ghOSt paper builds on a body of previous research that demonstrates how critical scheduling is to the scalability and performance of datacenter workloads. Scheduling is far from a solved problem, especially because of the â€œrise of the killer microsecondâ€ and new device types - Iâ€™m looking forward to following along future work on the  ghOSt open source project!</p><p>ghOStè®ºæ–‡å»ºç«‹åœ¨ä¹‹å‰çš„ä¸€ç³»åˆ—ç ”ç©¶çš„åŸºç¡€ä¸Šï¼Œè¿™äº›ç ”ç©¶è¯æ˜äº†è°ƒåº¦å¯¹æ•°æ®ä¸­å¿ƒå·¥ä½œè´Ÿè½½çš„å¯ä¼¸ç¼©æ€§å’Œæ€§èƒ½æœ‰å¤šä¹ˆé‡è¦ã€‚è°ƒåº¦è¿˜è¿œè¿œä¸æ˜¯ä¸€ä¸ªè§£å†³çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯å› ä¸ºâ€œæ€æ‰‹å¾®ç§’çš„å´›èµ·â€å’Œæ–°çš„è®¾å¤‡ç±»å‹â€”â€”æˆ‘æœŸå¾…ç€åœ¨ghOStå¼€æºé¡¹ç›®çš„æœªæ¥å·¥ä½œä¸­ç»§ç»­è·Ÿè¿›ï¼</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/linux/">#linux</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/fast/">#fast</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/å†…æ ¸/">#å†…æ ¸</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è®¾è®¡/">#è®¾è®¡</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/åˆ›æ„/">#åˆ›æ„</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/æ‘„å½±/">#æ‘„å½±</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/å›¾ç‰‡/">#å›¾ç‰‡</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/æ¸¸æˆ/">#æ¸¸æˆ</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è½¯ä»¶/">#è½¯ä»¶</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/å¹¿å‘Š/">#å¹¿å‘Š</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è§†é¢‘/">#è§†é¢‘</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/æ‰‹æœº/">#æ‰‹æœº</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/ç½‘ç«™/">#ç½‘ç«™</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/å…è´¹/">#å…è´¹</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/ä¸‹è½½/">#ä¸‹è½½</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/å¾®è½¯/">#å¾®è½¯</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è‹¹æœ/">#è‹¹æœ</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/éŸ³ä¹/">#éŸ³ä¹</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/åšå®¢/">#åšå®¢</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/æ¶æ/">#æ¶æ</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è°·æ­Œ/">#è°·æ­Œ</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è‰ºæœ¯/">#è‰ºæœ¯</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/å·¥å…·/">#å·¥å…·</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>