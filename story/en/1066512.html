<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>åœ¨åˆ›å»ºä¸–ç•Œä¸Šæœ€å¤§çš„å›¾è¡¨æ•°æ®åº“çš„å¹•å Behind the Scenes of Creating the Worldâ€™s Biggest Graph Database</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Behind the Scenes of Creating the Worldâ€™s Biggest Graph Database<br/>åœ¨åˆ›å»ºä¸–ç•Œä¸Šæœ€å¤§çš„å›¾è¡¨æ•°æ®åº“çš„å¹•å </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-06-18 01:06:47</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/6/9fab2a12cd244163506de0601600bcaf.jpeg"><img src="http://img2.diglog.com/img/2021/6/9fab2a12cd244163506de0601600bcaf.jpeg" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>Each forum shard contains 900 million relationships and 182 million nodes. The person shard contains 3 billion people and 16 billion relationships between them.</p><p>æ¯ä¸ªè®ºå›ç¢ç‰‡åŒ…å«9äº¿ä¸ªå…³ç³»å’Œ182ç™¾ä¸‡ä¸ªèŠ‚ç‚¹ã€‚è¯¥äººå‘˜ç¢ç‰‡åŒ…å«30äº¿äººå’Œä»–ä»¬ä¹‹é—´çš„160äº¿ä¸ªå…³ç³»ã€‚</p><p>  â€œ100 machines isnâ€™t cool. You know whatâ€™s cool? One trillion relationships.â€</p><p>  â€œ100å°æœºå™¨ä¸é…·ã€‚ä½ çŸ¥é“ä»€ä¹ˆå¾ˆé…·ï¼Ÿä¸€ä¸‡äº¿çš„å…³ç³»ã€‚â€œ</p><p> Iâ€™m not saying that someone actually uttered these exact words, but Iâ€™m pretty sure we all thought them. That was a month ago, when weâ€™d decided to try and build the biggest graph database that has ever existed.</p><p> æˆ‘å¹¶ä¸æ˜¯è¯´æœ‰äººå®é™…è¯´å‡ºäº†è¿™äº›ç¡®åˆ‡çš„è¯è¯­ï¼Œä½†æˆ‘å¾ˆç¡®å®šæˆ‘ä»¬éƒ½è®¤ä¸ºä»–ä»¬ã€‚é‚£æ˜¯ä¸€ä¸ªæœˆå‰ï¼Œå½“æˆ‘ä»¬å†³å®šå°è¯•å¹¶å»ºç«‹æ›¾ç»å­˜åœ¨çš„æœ€å¤§çš„å›¾å½¢æ•°æ®åº“ã€‚</p><p>   When we introduced  Neo4j Fabric, we also created a  proof of concept benchmark that was presented at FOSDEM 2020.</p><p>   å½“æˆ‘ä»¬æ¨å‡ºNeo4Jé¢æ–™æ—¶ï¼Œæˆ‘ä»¬è¿˜åˆ›å»ºäº†åœ¨FOSDem 2020å‘ˆç°çš„æ¦‚å¿µåŸºå‡†è¯æ®ã€‚</p><p> It showed that, for a  1TB database, throughput and latency  improve linearly with the number of shards that itâ€™s distributed across. More shards, more performance.</p><p> å®ƒè¡¨æ˜ï¼Œå¯¹äº1TBæ•°æ®åº“ï¼Œååé‡å’Œå»¶è¿Ÿéšç€å®ƒåˆ†å¸ƒåœ¨çš„ç¢ç‰‡æ•°é‡è€Œç›´çº¿åœ°æ”¹å–„ã€‚æ›´å¤šçš„ç¢ç‰‡ï¼Œæ›´æ€§èƒ½ã€‚</p><p>  The results looked good and confirmed that we had a very good understanding of the approach to scaling a graph database. Development of Fabric continued toward making it an integral part of Neo4j.</p><p>  ç»“æœçœ‹èµ·æ¥å¾ˆå¥½ï¼Œå¹¶ç¡®è®¤æˆ‘ä»¬å¯¹ç¼©æ”¾å›¾å½¢æ•°æ®åº“çš„æ–¹æ³•éå¸¸äº†è§£ã€‚å¼€å‘é¢æ–™ç»§ç»­ä½¿å…¶æˆä¸ºNeo4Jçš„ä¸€ä¸ªç»„æˆéƒ¨åˆ†ã€‚</p><p> New technologies were created and improved upon (server-side routing is a good example) and made useful for non-Fabric setups as well.</p><p> åˆ›å»ºå¹¶æ”¹è¿›äº†æ–°æŠ€æœ¯ï¼ˆæœåŠ¡å™¨ç«¯è·¯ç”±æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„ä¾‹å­ï¼‰ï¼Œä¹Ÿé€‚ç”¨äºéç»“æ„è®¾ç½®ã€‚ </p><p> But, that 1TB dataset from FOSDEM was always nagging us.  1TB is not that big, at least for Neo4j. We routinely have production setups with 10TB or more and, although they run on considerably large machines, Neo4j scales up pretty well.</p><p>ä½†æ˜¯ï¼Œæ¥è‡ªFOSDEMçš„1TBæ•°æ®é›†æ€»æ˜¯å” å¨æˆ‘ä»¬ã€‚ 1TBè‡³å°‘æ˜¯é‚£ä¹ˆå¤§ï¼Œè‡³å°‘æ˜¯Neo4Jã€‚æˆ‘ä»¬ç»å¸¸å…·æœ‰10TBæˆ–æ›´å¤§çš„ç”Ÿäº§è®¾ç½®ï¼Œè™½ç„¶å®ƒä»¬åœ¨å¤§å‹æœºå™¨ä¸Šè¿è¡Œï¼Œä½†Neo4jå±•ç°å¾—å¾ˆå¥½ã€‚</p><p> We didnâ€™t really need a solution for that; we needed a solution for  really big databases. Thatâ€™s why we had created Fabric, but we hadnâ€™t found its limit yet.</p><p> æˆ‘ä»¬å¹¶æ²¡æœ‰çœŸæ­£éœ€è¦è§£å†³æ–¹æ¡ˆ;æˆ‘ä»¬éœ€è¦ä¸€ä¸ªçœŸæ­£å¤§æ•°æ®åº“çš„è§£å†³æ–¹æ¡ˆã€‚è¿™å°±æ˜¯æˆ‘ä»¬åˆ›é€ äº†é¢æ–™çš„åŸå› ï¼Œä½†æˆ‘ä»¬è¿˜æ²¡æœ‰æ‰¾åˆ°å®ƒçš„æé™ã€‚</p><p> And what would that scale be? A year ago we built clusters of 40 machines, and they worked out pretty well. Going to 100 machines didnâ€™t seem that much of a challenge. Billions of nodes, perhaps? Well, billions of nodes is the same as a few TB of data.</p><p> é‚£ä¸ªè§„æ¨¡æ˜¯ä»€ä¹ˆï¼Ÿä¸€å¹´å‰ï¼Œæˆ‘ä»¬å»ºé€ äº†40å°æœºå™¨çš„é›†ç¾¤ï¼Œä»–ä»¬æ•ˆæœå¾ˆå¥½ã€‚å‰å¾€100å°æœºå™¨ä¼¼ä¹æ²¡æœ‰æŒ‘æˆ˜ã€‚æ•°åäº¿ä¸ªèŠ‚ç‚¹ï¼Œä¹Ÿè®¸ï¼Ÿå—¯ï¼Œæ•°åäº¿ä¸ªèŠ‚ç‚¹ä¸å‡ TBæ•°æ®ç›¸åŒã€‚</p><p> Plus, the  richness of a graph schema comes from relationships between nodes, not the nodes themselves. Remember: Weâ€™re trying to see how far we can push Neo4j, not just make ourselves feel or look good.</p><p> æ­¤å¤–ï¼Œå›¾å½¢æ¨¡å¼çš„ä¸°å¯Œæ€§æ¥è‡ªèŠ‚ç‚¹ä¹‹é—´çš„å…³ç³»ï¼Œè€Œä¸æ˜¯èŠ‚ç‚¹æœ¬èº«ã€‚è®°ä½ï¼šæˆ‘ä»¬æ­£è¯•å›¾çœ‹åˆ°æˆ‘ä»¬å¯ä»¥æ¨åŠ¨neo4jï¼Œä¸ä»…è®©è‡ªå·±æ„Ÿè§‰æˆ–çœ‹èµ·æ¥ä¸é”™ã€‚</p><p> Every now and then the same discussion would come around, and it was becoming clear that we were looking for an opportunity, a lightning rod, that would ground us in a realistic goal.</p><p> æ¯ä¸€ä¸ªç°åœ¨éƒ½ä¼šå‡ºç°åŒæ ·çš„è®¨è®ºï¼Œå¹¶ä¸”å˜å¾—æ˜æ˜¾ï¼Œæˆ‘ä»¬æ­£åœ¨å¯»æ‰¾ä¸€ä¸ªæœºé‡ï¼Œä¸€ä¸ªé¿é›·é’ˆï¼Œå°†æˆ‘ä»¬åœ¨ä¸€ä¸ªç°å®çš„ç›®æ ‡ã€‚</p><p> Turns out, that opportunity was  NODES 2021 on June 17, our biggest online developer conference.</p><p> äº‹å®è¯æ˜ï¼Œæˆ‘ä»¬æœ€å¤§çš„åœ¨çº¿å¼€å‘äººå‘˜ä¼šè®®ï¼Œè¯¥æœºä¼šæ˜¯2021å¹´6æœˆ17æ—¥çš„èŠ‚ç‚¹ã€‚</p><p>  We decided that we needed to show the world what we mean when we talk about scale. Not  just that Neo4j can retain or even improve performance when scaled horizontally, but to show how far we can go while still retaining performance.</p><p>  æˆ‘ä»¬å†³å®šï¼Œæˆ‘ä»¬éœ€è¦å‘ä¸–ç•Œå±•ç¤ºæˆ‘ä»¬è°ˆè®ºè§„æ¨¡æ—¶çš„æ„æ€ã€‚ä¸ä»…ä»…æ˜¯Neo4Jæ°´å¹³ç¼©æ”¾æ—¶å¯ä»¥ä¿ç•™ç”šè‡³æé«˜æ€§èƒ½ï¼Œä½†æ˜¾ç¤ºæˆ‘ä»¬ä»ç„¶å¯ä»¥åœ¨ä»ç„¶ä¿æŒæ€§èƒ½æ—¶è¿›è¡Œå¤šè¿œã€‚ </p><p> Weâ€™re going to need bigger numbers. Something that demands attention. Not hundreds of machines or billions of nodes.</p><p>æˆ‘ä»¬å°†éœ€è¦æ›´å¤§çš„æ•°å­—ã€‚éœ€è¦æ³¨æ„çš„ä¸œè¥¿ã€‚ä¸æ˜¯æ•°ç™¾å°æœºå™¨æˆ–æ•°åäº¿èŠ‚ç‚¹ã€‚</p><p>   Where do you even find a database with a trillion relationships? Using a production setup would present logistics problems â€” the data transfer itself would be complicated, not to mention obfuscating the data so itâ€™s appropriate for public view.  Weâ€™ll need to generate it for ourselves, from a known model. And we knew we needed to start with the data model, since that would give us the number and size of machines, the queries weâ€™d run, and the tests weâ€™d create.  In short, start with the data model to get a sense of the effort it would take.</p><p>   ä½ ç”šè‡³åœ¨å“ªé‡Œæ‰¾åˆ°ä¸€ä¸ªå¸¦æœ‰ä¸‡äº¿å…³ç³»çš„æ•°æ®åº“ï¼Ÿä½¿ç”¨ç”Ÿäº§è®¾ç½®å°†å‡ºç°ç‰©æµé—®é¢˜ - æ•°æ®ä¼ è¾“æœ¬èº«å°†æ˜¯å¤æ‚çš„ï¼Œæ›´ä¸ç”¨è¯´æ··æ·†æ•°æ®ï¼Œå› æ­¤å¯ä»¥é€‚åˆå…¬å…±è§†å›¾ã€‚æˆ‘ä»¬éœ€è¦ä»å·²çŸ¥æ¨¡å‹ç”Ÿæˆå®ƒã€‚æˆ‘ä»¬çŸ¥é“æˆ‘ä»¬éœ€è¦ä»æ•°æ®æ¨¡å‹å¼€å§‹ï¼Œå› ä¸ºå®ƒä¼šç»™æˆ‘ä»¬æœºå™¨çš„æ•°é‡å’Œå¤§å°ï¼Œæˆ‘ä»¬è¿è¡Œçš„æŸ¥è¯¢ï¼Œä»¥åŠæˆ‘ä»¬åˆ›å»ºçš„æµ‹è¯•ã€‚ç®€è€Œè¨€ä¹‹ï¼Œä»æ•°æ®æ¨¡å‹å¼€å§‹ï¼Œä»¥è·å¾—å®ƒéœ€è¦çš„åŠªåŠ›ã€‚</p><p> LDBC is a good candidate. It is a social network that contains people, forums, and posts. Itâ€™s easy to understand and explain and we are very familiar with it. We decided on  3 billion people, surpassing the largest social network on the planet. The degree of social connectivity was chosen so that the person shard would come out 850GB, and every forum shard would come at 250GB with about 900 million relationships each.  To get to the target of 1 trillion relationships weâ€™d need about  1110 forum shards in total, each its own machine.</p><p> LDBCæ˜¯ä¸€ä¸ªå¾ˆå¥½çš„å€™é€‰äººã€‚å®ƒæ˜¯ä¸€ä¸ªåŒ…å«äººï¼Œè®ºå›å’Œå¸–å­çš„ç¤¾äº¤ç½‘ç»œã€‚å®ƒå¾ˆå®¹æ˜“ç†è§£å’Œè§£é‡Šï¼Œæˆ‘ä»¬éå¸¸ç†Ÿæ‚‰å®ƒã€‚æˆ‘ä»¬å†³å®šäº†30äº¿äººï¼Œè¶…è¶Šäº†åœ°çƒä¸Šæœ€å¤§çš„ç¤¾äº¤ç½‘ç»œã€‚é€‰æ‹©äº†ç¤¾äº¤è¿æ¥ç¨‹åº¦ï¼Œä»¥ä¾¿è¯¥äººå‘˜ç¢ç‰‡å°†å‡ºç°850GBï¼Œæ¯ä¸ªè®ºå›ç¢ç‰‡éƒ½ä¼šä»¥250GBæ¥åˆ°250GBï¼Œæ¯ä¸ªäººéƒ½æœ‰çº¦900ä¸‡ä¸ªå…³ç³»ã€‚ä¸ºäº†è·å¾—1ä¸‡äº¿çš„å…³ç³»ï¼Œæˆ‘ä»¬éœ€è¦å¤§çº¦1110ä¸ªè®ºå›ç¢ç‰‡ï¼Œæ¯ä¸ªäººéƒ½æ˜¯è‡ªå·±çš„æœºå™¨ã€‚</p><p>  Thatâ€™s 1110 forum shards, plus a few more for redundancy, each of which will need to have a store generated for it.</p><p>  è¿™æ˜¯1110ä¸ªè®ºå›ç¢ç‰‡ï¼ŒåŠ ä¸Šå†—ä½™çš„æ›´å¤šä¿¡æ¯ï¼Œæ¯ä¸ªéƒ½éœ€è¦æœ‰ä¸€ä¸ªä¸ºå®ƒç”Ÿæˆçš„å•†åº—ã€‚</p><p> We also wanted 3  Fabric proxies connecting to 10, 100 and all the shards to see how the system scales as data sizes grow. And, with the clock ticking, we needed a plan to orchestrate all these machines.</p><p> æˆ‘ä»¬è¿˜æƒ³è¦è¿æ¥åˆ°10,100å’Œæ‰€æœ‰ç¢ç‰‡çš„3ä¸ªé¢æ–™ä»£ç†ï¼Œä»¥äº†è§£ç³»ç»Ÿå°ºåº¦å¦‚ä½•éšæ•°æ®å°ºå¯¸çš„å¢é•¿ã€‚å¹¶ä¸”ï¼Œéšç€æ—¶é’Ÿæ»´ç­”ä½œå“ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªè®¡åˆ’æ¥åè°ƒæ‰€æœ‰è¿™äº›æœºå™¨ã€‚</p><p> It was all about managing risk. We expected that, if trouble found us, it would be either during store generation or something glitching badly in the network. As it turned out, we were half right.</p><p> è¿™ä¸€åˆ‡éƒ½æ˜¯å…³äºç®¡ç†é£é™©ã€‚æˆ‘ä»¬é¢„æœŸï¼Œå¦‚æœéº»çƒ¦å‘ç°æˆ‘ä»¬ï¼Œå®ƒæ˜¯åœ¨å•†åº—ç”ŸæˆæœŸé—´æˆ–åœ¨ç½‘ç»œä¸­ä¸¥é‡å‘å‡ºæ•…éšœçš„ä¸œè¥¿ã€‚äº‹å®è¯æ˜ï¼Œæˆ‘ä»¬å°±æ˜¯ä¸€åŠã€‚</p><p>   One was the  large number of machines that would host the shards. The other was generating the shard data, which we knew needed to happen in parallel and would, therefore, also need orchestrating a lot of machines.</p><p>   ä¸€ä¸ªæ˜¯ä¸¾åŠç¢ç‰‡çš„å¤§é‡æœºå™¨ã€‚å¦ä¸€ä¸ªæ­£åœ¨ç”Ÿæˆç¢ç‰‡æ•°æ®ï¼Œæˆ‘ä»¬çŸ¥é“éœ€è¦å¹¶è¡Œå‘ç”Ÿï¼Œå› æ­¤ä¹Ÿéœ€è¦åè°ƒå¤§é‡æœºå™¨ã€‚ </p><p> We needed a two-step approach. The first step would be full-sized stores but a small number of Neo4j processes. That would let us test the parallel store creation, the generators, the installation of the stores from the buckets to the shards, and also test the MVP of the latency-measuring client.</p><p>æˆ‘ä»¬éœ€è¦ä¸€ä¸ªä¸¤æ­¥çš„æ–¹æ³•ã€‚ç¬¬ä¸€æ­¥å°†æ˜¯å…¨å°ºå¯¸çš„å•†åº—ï¼Œè€Œæ˜¯å°‘æ•°NEO4Jè¿›ç¨‹ã€‚è¿™å°†è®©æˆ‘ä»¬æµ‹è¯•å¹¶è¡Œå•†åº—åˆ›å»ºï¼Œå‘ç”µæœºï¼Œä»æ¡¶åˆ°åˆ†ç‰‡çš„å­˜å‚¨å™¨ï¼Œä¹Ÿæµ‹è¯•å»¶è¿Ÿæµ‹é‡å®¢æˆ·ç«¯çš„MVPã€‚</p><p> It wouldnâ€™t put any stress on our orchestration tools (that would come later), and allows us to focus on getting everything wired properly for our proof of concept. Things worked out pretty well, and we had all the pieces in place. Everything seemed to work together, and now we could hold our breath and go for the second phase â€” full size.</p><p> å®ƒä¸ä¼šå¯¹æˆ‘ä»¬çš„ç¼–æ’å·¥å…·ï¼ˆä»¥åæ¥ï¼‰å¯¹ä»»ä½•å‹åŠ›è¿›è¡Œå‹åŠ›ï¼Œå¹¶å…è®¸æˆ‘ä»¬ä¸“æ³¨äºä¸ºæˆ‘ä»¬çš„æ¦‚å¿µè¯æ˜æ­£ç¡®åœ°æ¥çº¿ã€‚äº‹æƒ…å¾ˆå¥½åœ°é”»ç‚¼èº«ä½“ï¼Œæˆ‘ä»¬éƒ½æœ‰æ‰€æœ‰çš„ä½œå“ã€‚ä¸€åˆ‡ä¼¼ä¹éƒ½åœ¨ä¸€èµ·å·¥ä½œï¼Œç°åœ¨æˆ‘ä»¬å¯ä»¥å±ä½å‘¼å¸å¹¶è·å¾—ç¬¬äºŒé˜¶æ®µ - å…¨å°ºå¯¸ã€‚</p><p> With two weeks left, we moved to step two:   We pulled out all the stops and braced for impact.</p><p> å‰©ä¸‹ä¸¤å‘¨åï¼Œæˆ‘ä»¬æ¬åˆ°ç¬¬äºŒæ­¥ï¼šæˆ‘ä»¬æ‹”å‡ºäº†æ‰€æœ‰çš„æŒ¡å—å¹¶æ”¯æ’‘äº†å½±å“ã€‚</p><p> The first issue that came up was that AWS instance provisioning started failing unpredictably at around 800 machines. Some detective work led to discovering that we have a vCore limit on our AWS account that didnâ€™t let us create any more machines. AWS Support lifted it promptly and we continued creating instances only to hit a more serious limitation:</p><p> æåˆ°çš„ç¬¬ä¸€ä¸ªé—®é¢˜æ˜¯AWSå®ä¾‹é…ç½®å¼€å§‹åœ¨å·¦å³800å°æœºå™¨æ—¶ä¸å¯é¢„æµ‹åœ°å¤±è´¥ã€‚ä¸€äº›ä¾¦æ¢å·¥ä½œå¯¼è‡´å‘ç°æˆ‘ä»¬å¯¹æˆ‘ä»¬çš„AWSå¸æˆ·æœ‰ä¸€ä¸ªvcoreé™åˆ¶ï¼Œè¿™äº›è´¦æˆ·æ²¡æœ‰è®©æˆ‘ä»¬åˆ›å»ºä»»ä½•æœºå™¨ã€‚ AWSæ”¯æŒè¿…é€Ÿæå‡ï¼Œæˆ‘ä»¬ç»§ç»­åˆ›å»ºå®ä¾‹åªä¼šè¾¾åˆ°æ›´ä¸¥é‡çš„é™åˆ¶ï¼š</p><p> â€œWe currently do not have sufficient x1e.4xlarge capacity in zones with support for â€˜gp2â€™ volumes. Our system will be working on provisioning additional capacity.â€</p><p> â€œæˆ‘ä»¬ç›®å‰æ²¡æœ‰è¶³å¤Ÿçš„x1e.4xlargeå®¹é‡ï¼Œå…·æœ‰æ”¯æŒâ€GP2â€œå·ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿå°†è‡´åŠ›äºæä¾›é¢å¤–çš„å®¹é‡ã€‚â€œ</p><p> Hmm. It would seem   Amazon had run out of capacity. This left us with two options. Either go for multiple Availability Zones or for smaller instance types. We decided that thereâ€™s more complexity and unknowns going for multi AZ, so weâ€™d do smaller instances for now and, if performance was a problem, weâ€™d deal with it later. Getting to the full number of instances was the most important goal.</p><p> å””ã€‚çœ‹èµ·æ¥äºšé©¬é€Šç¼ºä¹å®¹é‡ã€‚è¿™è®©æˆ‘ä»¬ç•™ä¸‹äº†ä¸¤ä¸ªé€‰æ‹©ã€‚è¦ä¹ˆç”¨äºå¤šä¸ªå¯ç”¨æ€§åŒºåŸŸæˆ–è¾ƒå°çš„å®ä¾‹ç±»å‹ã€‚æˆ‘ä»¬å†³å®šå¤šAZçš„å¤æ‚æ€§å’ŒæœªçŸ¥æ•°ï¼Œæ‰€ä»¥æˆ‘ä»¬ç°åœ¨åšäº†æ›´å°çš„å®ä¾‹ï¼Œå¦‚æœè¡¨ç°æ˜¯ä¸€ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ç¨åä¼šå¤„ç†å®ƒã€‚åˆ°è¾¾å…¨éƒ¨çš„å®ä¾‹æ˜¯æœ€é‡è¦çš„ç›®æ ‡ã€‚</p><p> Using smaller instances did the trick, and the next day we had the full contingent of 1129 shards up and running. The latency measuring demo app was almost ready so we decided to take some measurements to see where we stand.</p><p> ä½¿ç”¨è¾ƒå°çš„å®ä¾‹æ‰§è¡Œäº†è¯€çªï¼Œç¬¬äºŒå¤©æˆ‘ä»¬æ‹¥æœ‰1129ä¸ªç¢ç‰‡çš„å…¨éƒ¨æˆ–è·‘æ­¥ã€‚å»¶è¿Ÿæµ‹é‡æ¼”ç¤ºåº”ç”¨ç¨‹åºå‡ ä¹å‡†å¤‡å°±ç»ªï¼Œå› æ­¤æˆ‘ä»¬å†³å®šæ‹æ‘„ä¸€äº›æµ‹é‡ä»¥æŸ¥çœ‹æˆ‘ä»¬çš„ç«‹åœºã€‚ </p><p>  The complete fabric proxy, the one pointing to all 1129 shards, was timing out. Neo4j log files didnâ€™t have any relevant error messages, just the timeouts. No GC pauses, no firewall misconfigurations; none of the usual suspects were to blame. Each individual shard was responding normally, and the Fabric proxy didnâ€™t show any problems either. It took an evening of investigative work to find that the issue was   DNS query limiting. As AWS documentation points out:</p><p>å®Œæ•´çš„é¢æ–™ä»£ç†ï¼ŒæŒ‡å‘æ‰€æœ‰1129ä¸ªç¢ç‰‡çš„ä¸€ä¸ªï¼Œæ˜¯æ—¶æœºã€‚ Neo4jæ—¥å¿—æ–‡ä»¶æ²¡æœ‰ä»»ä½•ç›¸å…³çš„é”™è¯¯æ¶ˆæ¯ï¼Œåªæ˜¯è¶…æ—¶ã€‚æ²¡æœ‰GCæš‚åœï¼Œæ²¡æœ‰é˜²ç«å¢™é”™ä½;å¸¸è§„å«Œç–‘äººéƒ½æ²¡æœ‰è´£å¤‡ã€‚æ¯ä¸ªå•ç‹¬çš„ç¢ç‰‡æ­£å¸¸å“åº”ï¼Œå¹¶ä¸”é¢æ–™ä»£ç†ä¹Ÿæ²¡æœ‰æ˜¾ç¤ºä»»ä½•é—®é¢˜ã€‚å®ƒèŠ±äº†ä¸€ä¸ªæ™šä¸Šçš„è°ƒæŸ¥å·¥ä½œï¼Œå‘ç°è¿™ä¸ªé—®é¢˜æ˜¯DNSæŸ¥è¯¢é™åˆ¶ã€‚éšç€AWSæ–‡ä»¶æŒ‡å‡ºï¼š</p><p> â€œAmazon provided DNS servers enforce a limit of 1024 packets per second per elastic network interface (ENI). Amazon provided DNS servers reject any traffic exceeding this limit.â€</p><p> â€œäºšé©¬é€Šæä¾›DNSæœåŠ¡å™¨æ¯ç§’æ¯ä¸ªå¼¹æ€§ç½‘ç»œæ¥å£ï¼ˆENIï¼‰æ‰§è¡Œæ¯ç§’1024ä¸ªæ•°æ®åŒ…çš„é™åˆ¶ã€‚äºšé©¬é€Šæä¾›DNSæœåŠ¡å™¨æ‹’ç»è¶…è¿‡æ­¤é™åˆ¶çš„ä»»ä½•æµé‡ã€‚â€œ</p><p> Yup. That should do it. Our Fabric configuration was using DNS names for the shards, so that limit was reached immediately on every query we submitted. The solution was quite simple â€” just make the DNS entries static on the Fabric instance and no longer depend on DNS ( /etc/hosts FTW).</p><p> æ˜¯çš„ã€‚è¿™åº”è¯¥è¿™æ ·åšã€‚æˆ‘ä»¬çš„Fabricé…ç½®æ­£åœ¨ä½¿ç”¨ç¢ç‰‡çš„DNSåç§°ï¼Œå› æ­¤æˆ‘ä»¬åœ¨æˆ‘ä»¬æäº¤çš„æ¯ä¸ªæŸ¥è¯¢ä¸Šç«‹å³åˆ°è¾¾äº†è¯¥é™åˆ¶ã€‚è§£å†³æ–¹æ¡ˆéå¸¸ç®€å• - åªéœ€ä½¿DNSæ¡ç›®é™æ€åœ¨ç»“æ„å®ä¾‹ä¸Šï¼Œä¸å†ä¾èµ–äºDNSï¼ˆ/ etc / hosts ftwï¼‰ã€‚</p><p> And that was the last limit we had to overcome. Our initial latency numbers were looking very nice and we decided there was no reason to move to larger instance types, which would also help keep the costs reasonable.</p><p> è¿™æ˜¯æˆ‘ä»¬å¿…é¡»å…‹æœçš„æœ€åä¸€ä¸ªé™åˆ¶ã€‚æˆ‘ä»¬çš„åˆå§‹å»¶è¿Ÿç¼–å·çœ‹èµ·æ¥éå¸¸å¥½ï¼Œæˆ‘ä»¬å†³å®šæ²¡æœ‰ç†ç”±è½¬ç§»åˆ°æ›´å¤§çš„å®ä¾‹ç±»å‹ï¼Œè¿™ä¹Ÿæœ‰åŠ©äºä¿æŒæˆæœ¬åˆç†ã€‚</p><p> Overall, we had build tools that allowed us, at the press of a button, to set up a  1129 shard cluster hosting 280 TB of data, with 3 Fabric proxies, in under 3 hours. Yay! And it took us only 16 days to get there.</p><p> æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨æŒ‰ä¸€ä¸‹æŒ‰é’®æ—¶ï¼Œæˆ‘ä»¬å»ºç«‹äº†å…è®¸æˆ‘ä»¬çš„å·¥å…·ï¼Œä»¥è®¾ç½®1129ä¸ªç¢ç‰‡ç¾¤æ‰˜ç®¡280 TBæ•°æ®ï¼Œ3å°æ—¶å†…çš„3ä¸ªé¢æ–™ä»£ç†ã€‚è€¶ï¼å®ƒåªéœ€è¦16å¤©åˆ°è¾¾é‚£é‡Œã€‚</p><p> We spent the rest of the time fine-tuning the configurations and the queries, and trying out the latency measuring app. We also played around with the graph itself, created new ad-hoc queries to get a feel of working with such a large setup.</p><p> æˆ‘ä»¬å‰©ä¸‹çš„æ—¶é—´æ¸…é™¤é…ç½®å’ŒæŸ¥è¯¢ï¼Œå¹¶å°è¯•å»¶è¿Ÿæµ‹é‡åº”ç”¨ç¨‹åºã€‚æˆ‘ä»¬è¿˜åœ¨å›¾è¡¨æœ¬èº«ä¸Šæ’­æ”¾ï¼Œåˆ›å»ºäº†æ–°çš„Ad-hocæŸ¥è¯¢ï¼Œä»¥è·å¾—ä½¿ç”¨å¦‚æ­¤å¤§çš„è®¾ç½®ã€‚</p><p>     We didnâ€™t want to create a pure demo-ware, so we decided to make everything public under the ASL, you can try to run the setup yourself (at different scales) by using the   trillion-graph repository. Just remember to watch your AWS bill ğŸ’¸!</p><p>     æˆ‘ä»¬ä¸æƒ³åˆ›å»ºä¸€ä¸ªçº¯ç²¹çš„æ¼”ç¤ºç¨‹åºï¼Œå› æ­¤æˆ‘ä»¬å†³å®šé€šè¿‡ä½¿ç”¨ä¸‡äº¿å›¾è¡¨å­˜å‚¨åº“å°è¯•è‡ªå·±ï¼ˆåœ¨ä¸åŒçš„å°ºåº¦ï¼‰ä¸Šè¿è¡ŒSetupï¼ˆä»¥ä¸åŒçš„å°ºåº¦ï¼‰è¿è¡Œè®¾ç½®ã€‚è®°å¾—è¦æ³¨æ„ä½ çš„AWSè´¦å•â™¥ï¼ </p><p>   This is just the first step in a long journey. We have a lot more work to do to understand how large graph databases behave, how network variations aggregate as noise and how we should improve the system to scale beyond the numbers we achieved for this demo.</p><p>è¿™åªæ˜¯é•¿é€”æ—…ç¨‹çš„ç¬¬ä¸€æ­¥ã€‚ æˆ‘ä»¬æœ‰å¾ˆå¤šå·¥ä½œè¦åšï¼Œä»¥äº†è§£å¤§å›¾æ•°æ®åº“çš„è¡¨ç°æ–¹å¼ï¼Œç½‘ç»œå˜åŒ–å¦‚ä½•èšåˆä¸ºå™ªéŸ³ä»¥åŠæˆ‘ä»¬åº”è¯¥å¦‚ä½•æ”¹è¿›ç³»ç»Ÿä»¥è¶…å‡ºæˆ‘ä»¬ä¸ºæ­¤æ¼”ç¤ºæ‰€å®ç°çš„æ•°å­—ã€‚ </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://medium.com/neo4j/behind-the-scenes-of-creating-the-worlds-biggest-graph-database-cd22f477c843">https://medium.com/neo4j/behind-the-scenes-of-creating-the-worlds-biggest-graph-database-cd22f477c843</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/æ•°æ®åº“/">#æ•°æ®åº“</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/database/">#database</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/åˆ›å»º/">#åˆ›å»º</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/creating/">#creating</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è®¾è®¡/">#è®¾è®¡</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/åˆ›æ„/">#åˆ›æ„</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/æ‘„å½±/">#æ‘„å½±</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/å›¾ç‰‡/">#å›¾ç‰‡</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/æ¸¸æˆ/">#æ¸¸æˆ</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è½¯ä»¶/">#è½¯ä»¶</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/æ‰‹æœº/">#æ‰‹æœº</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è§†é¢‘/">#è§†é¢‘</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/å¹¿å‘Š/">#å¹¿å‘Š</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/å…è´¹/">#å…è´¹</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/ç½‘ç«™/">#ç½‘ç«™</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/ä¸‹è½½/">#ä¸‹è½½</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/å¾®è½¯/">#å¾®è½¯</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/éŸ³ä¹/">#éŸ³ä¹</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è‹¹æœ/">#è‹¹æœ</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/åšå®¢/">#åšå®¢</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/æ¶æ/">#æ¶æ</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è‰ºæœ¯/">#è‰ºæœ¯</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è°·æ­Œ/">#è°·æ­Œ</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/å·¥å…·/">#å·¥å…·</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>