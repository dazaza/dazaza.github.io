<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>æˆ‘åœ¨å¾®æœåŠ¡ä¸–ç•Œä¸­çœ‹åˆ°çš„ç¾éš¾ Disasters I've seen in a microservices world</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Disasters I've seen in a microservices world<br/>æˆ‘åœ¨å¾®æœåŠ¡ä¸–ç•Œä¸­çœ‹åˆ°çš„ç¾éš¾ </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-06-13 23:04:20</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/6/ff3420d221f81eb33d4f4e2e3712a408.jpeg"><img src="http://img2.diglog.com/img/2021/6/ff3420d221f81eb33d4f4e2e3712a408.jpeg" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>When Martin Fowler&#39;s post about  microservices came out in 2014, the teams where I worked were already building service-oriented architectures. That post and the subsequent hype made their way into almost every software team in the world. The &#34;Netflix OSS stack&#34; was the coolest thing back then, allowing engineers worldwide to leverage Netflix&#39;s lessons in distributed systems. More than six years later, if we look into software engineering jobs right now, most of them talk about a microservices&#39; architecture.</p><p>å½“Martin Fowlerï¼†ï¼ƒ39;å…³äºMicroServicesçš„å¸–å­äº2014å¹´å‡ºæ¥æ—¶ï¼Œæˆ‘å·¥ä½œçš„å›¢é˜Ÿå·²ç»å»ºç«‹äº†é¢å‘æœåŠ¡çš„æ¶æ„ã€‚é‚£ç¯‡æ–‡ç« å’Œéšåçš„ç‚’ä½œè¿›å…¥äº†ä¸–ç•Œä¸Šå‡ ä¹æ¯ä¸ªè½¯ä»¶å›¢é˜Ÿã€‚ ï¼†ï¼ƒ34; Netflix OSS Stackï¼†ï¼ƒ34;é‚£æ˜¯æœ€é…·çš„äº‹æƒ…ï¼Œå…è®¸å…¨ä¸–ç•Œå·¥ç¨‹å¸ˆåˆ©ç”¨Netflixï¼†ï¼ƒ39;åœ¨åˆ†å¸ƒå¼ç³»ç»Ÿä¸­çš„è¯¾ç¨‹ã€‚è¶…è¿‡å…­å¹´åï¼Œå¦‚æœæˆ‘ä»¬ç°åœ¨è°ƒæŸ¥è½¯ä»¶å·¥ç¨‹å°±ä¸šï¼Œä»–ä»¬å¤§å¤šæ•°äººéƒ½è°ˆåˆ°äº†ä¸€ä¸ªå¾®æœåŠ¡å’Œï¼ƒ39;å»ºç­‘å­¦ã€‚</p><p>    In the earliest part of the 2010s, many organizations were suffering challenges regarding their software development cycle. Folks working with other 50, 100 or 200 engineers struggled with development environments, heavy QA processes and programmed deployments. While Martin Fowler&#39;s &#34; Continuous Delivery&#34; book shed light on many of those teams, they started to realize their  majestic monoliths were creating organizational problems for them. Hence, microservices were appealing for software engineers. It&#39;s more challenging to introduce continuous delivery or deployment in a big project rather than start with it.</p><p>    åœ¨2010å¹´ä»£çš„æœ€æ—©éƒ¨åˆ†ï¼Œè®¸å¤šç»„ç»‡å¯¹å…¶è½¯ä»¶å¼€å‘å‘¨æœŸé­å—æŒ‘æˆ˜ã€‚äººä»¬ä½¿ç”¨å…¶ä»–50,100æˆ–200åå·¥ç¨‹å¸ˆï¼ŒåŠªåŠ›å¼€å‘ç¯å¢ƒï¼Œé‡è´¨QAæµç¨‹å’Œç¼–ç¨‹éƒ¨ç½²ã€‚è™½ç„¶é©¬ä¸ç¦å‹’ï¼†ï¼ƒ39; sï¼†ï¼ƒ34;è¿ç»­äº¤è´§ï¼†ï¼ƒ34;ä¹¦ç±æ£šåœ¨è®¸å¤šå›¢é˜Ÿä¸­ï¼Œä»–ä»¬å¼€å§‹æ„è¯†åˆ°ä»–ä»¬çš„é›„ä¼Ÿçš„å·¨çŸ³æ­£åœ¨ä¸ºä»–ä»¬åˆ›é€ ç»„ç»‡é—®é¢˜ã€‚å› æ­¤ï¼Œå¾®æœåŠ¡å¯¹è½¯ä»¶å·¥ç¨‹å¸ˆè¿›è¡Œäº†å¸å¼•åŠ›ã€‚å®ƒæ›´å…·æŒ‘æˆ˜æ€§ï¼Œåœ¨å¤§é¡¹ç›®ä¸­å»¶ç»­æŒç»­äº¤ä»˜æˆ–éƒ¨ç½²è€Œä¸æ˜¯ä»å®ƒå¼€å§‹ã€‚</p><p>  So teams started spinning off three, ten, a hundred microservices. Most of them used &#34;JSON over HTTP&#34; â€” others may say RESTful ğŸ˜‰ â€” APIs for remote calls between these components. People knew well the HTTP protocol, and it seemed a relatively easy way to convert the monoliths into smaller pieces. At this point, teams started to deploy code into production in less than 15 minutes. There was no more the &#34;Oh, team A broke the CI pipeline, and I can&#39;t deploy my code&#34;, and it felt great!</p><p>  æ‰€ä»¥çƒé˜Ÿå¼€å§‹æ—‹è½¬ä¸‰ï¼Œåï¼Œä¸€ç™¾ä¸ªå¾®æœåŠ¡ã€‚ä»–ä»¬ä¸­çš„å¤§å¤šæ•°äººä½¿ç”¨ï¼†ï¼ƒ34; json overt httpï¼†ï¼ƒ34; - å…¶ä»–äººå¯èƒ½ä¼šè¯´ä¾èµ–äºè¿™äº›ç»„ä»¶ä¹‹é—´çš„è¿œç¨‹å‘¼å«çš„APIã€‚äººä»¬çŸ¥é“HTTPåè®®å¾ˆå¥½ï¼Œå®ƒä¼¼ä¹æ˜¯å°†å¢¨åº•è½¬æ¢æˆè¾ƒå°çš„ç¢ç‰‡ç›¸å¯¹ç®€å•çš„æ–¹æ³•ã€‚æ­¤æ—¶ï¼Œå›¢é˜Ÿå¼€å§‹åœ¨ä¸åˆ°15åˆ†é’Ÿçš„æ—¶é—´å†…å°†ä»£ç éƒ¨ç½²åˆ°ç”Ÿäº§ä¸­ã€‚æ²¡æœ‰æ›´å¤šçš„ï¼†ï¼ƒ34;å“¦ï¼Œå›¢é˜Ÿçªç ´äº†CIç®¡é“ï¼Œæˆ‘å¯ä»¥å’Œï¼ƒ39; téƒ¨ç½²æˆ‘çš„ä»£ç ï¼†ï¼ƒ34;å®ƒæ„Ÿåˆ°å¾ˆæ£’ï¼</p><p>  Most engineers forgot, though, that while solving an organizational problem at the software architecture&#39;s level, they also introduced a lot of complexity. The  distributed systems fallacies became more and more evident and quickly were a headache for those teams. Even for companies that were already doing client/server architectures where they already existed, this exploded in their faces once they had 10+ moving pieces in their systems.</p><p>  å°½ç®¡å¦‚æ­¤ï¼Œå¤§å¤šæ•°å·¥ç¨‹å¸ˆå¿˜è®°äº†åœ¨è½¯ä»¶æ¶æ„ä¸­è§£å†³ç»„ç»‡é—®é¢˜ï¼†ï¼ƒ39; Sæ°´å¹³ï¼Œä»–ä»¬ä¹Ÿä»‹ç»äº†å¾ˆå¤šå¤æ‚æ€§ã€‚åˆ†å¸ƒå¼ç³»ç»Ÿè´«å›°å˜å¾—è¶Šæ¥è¶Šæ˜æ˜¾ï¼Œå¹¶ä¸”å¯¹äºè¿™äº›å›¢é˜Ÿæ¥è¯´å¾ˆå¿«å°±ä¼šå‡ºç°å¤´ç–¼ã€‚å³ä½¿å¯¹äºå·²ç»åœ¨ä»–ä»¬å·²ç»å­˜åœ¨çš„å®¢æˆ·/æœåŠ¡å™¨æ¶æ„çš„å…¬å¸æä¾›äº†å®¢æˆ·/æœåŠ¡å™¨æ¶æ„ï¼Œå®ƒä¸€æ—¦å®ƒä»¬åœ¨å…¶ç³»ç»Ÿä¸­æœ‰10ä¸ªä»¥ä¸Šçš„ç§»åŠ¨ç¢ç‰‡å°±åœ¨å…¶è„¸ä¸Šçˆ†ç‚¸ã€‚</p><p>    Going for significant architectural changes doesn&#39;t come for free. Teams started to realize that sharing a database was a single-point-of-failure. Then, they realized that separating their domains created a whole new world: eventual consistency was a thing. What about when a service where you&#39;re pulling data off is down? The number of questions and problems started to pile up. The promises of a high-speed development pace were trumped by looking for bugs, incidents, data consistency issues, etc. Another problem was that engineers needed centralized logs and observability solutions to span across tens of services to spot and correct these defects.</p><p>    ä¸ºäº†è·å¾—é‡å¤§çš„å»ºç­‘å˜é©ï¼Œä¸ä¸ºå…è´¹æ¥ã€‚å›¢é˜Ÿå¼€å§‹æ„è¯†åˆ°å…±äº«æ•°æ®åº“æ˜¯ä¸€ä¸ªå•ç‚¹æ•…éšœã€‚ç„¶åï¼Œä»–ä»¬æ„è¯†åˆ°åˆ†ç¦»ä»–ä»¬çš„åŸŸååˆ›é€ äº†ä¸€ä¸ªå…¨æ–°çš„ä¸–ç•Œï¼šæœ€ç»ˆçš„ä¸€è‡´æ€§æ˜¯ä¸€ä»¶äº‹ã€‚å½“ä½ ï¼†ï¼ƒ39çš„æœåŠ¡æ—¶ä½•æ—¶ä½•åœ°å‘¢ï¼Ÿé—®é¢˜å’Œé—®é¢˜çš„æ•°é‡å¼€å§‹å †ç§¯ã€‚é€šè¿‡å¯»æ‰¾é”™è¯¯ï¼Œäº‹ä»¶ï¼Œæ•°æ®ä¸€è‡´æ€§é—®é¢˜ç­‰ï¼Œçªç ´äº†é«˜é€Ÿå‘å±•æ­¥ä¼çš„æ‰¿è¯ºã€‚å¦ä¸€ä¸ªé—®é¢˜æ˜¯å·¥ç¨‹å¸ˆéœ€è¦é›†ä¸­æ—¥å¿—å’Œå¯è§‚å¯Ÿæ€§è§£å†³æ–¹æ¡ˆï¼Œä»¥è·¨è¶Šå‡ åç§æœåŠ¡æ¥å‘ç°å¹¶çº æ­£è¿™äº›ç¼ºé™·ã€‚</p><p>    Having the ability to create new services every day came with an explosion of developer&#39;s creativity. A new feature? Bam, let&#39;s start a service! Suddenly, teams with 20 engineers were maintaining 50 services. That&#39;s more than one service per person! The problem with code, in general, is that it rots. Maintaining every service came at a cost. Imagine propagating a library upgrade across your services&#39; fleet. Imagine that these services were started at different time points, with different architectures and some entanglement between the business logic and the frameworks used. That&#39;s  bananas! Of course, there are ways to solve these problems. Most of them weren&#39;t available back in those days, and others cost a lot in FTEs work.</p><p>    æœ‰èƒ½åŠ›æ¯å¤©åˆ›å»ºæ–°æœåŠ¡ï¼Œå¸¦æ¥å¼€å‘å•†çš„çˆ†ç‚¸ï¼†ï¼ƒ39;åˆ›é€ åŠ›ã€‚ä¸€ä¸ªæ–°åŠŸèƒ½ï¼Ÿ BAMï¼ŒLetï¼†ï¼ƒ39;å¼€å§‹æœåŠ¡ï¼çªç„¶ï¼Œ20åå·¥ç¨‹å¸ˆçš„å›¢é˜Ÿæ­£åœ¨ç»´æŒ50å®¶æœåŠ¡ã€‚è¿™ä¸ªï¼†ï¼ƒ39;æ¯ä¸ªäººå¤šäºä¸€ä¸ªæœåŠ¡ï¼ä»£ç çš„é—®é¢˜é€šå¸¸æ˜¯å®ƒrotsã€‚ç»´æŠ¤æ¯é¡¹æœåŠ¡éƒ½æ˜¯æˆæœ¬çš„ã€‚æƒ³è±¡ä¸€ä¸‹ï¼Œåœ¨æ‚¨çš„æœåŠ¡ä¸­ä¼ æ’­åº“å‡çº§ï¼†ï¼ƒ39;èˆ°é˜Ÿã€‚æƒ³è±¡ä¸€ä¸‹ï¼Œè¿™äº›æœåŠ¡åœ¨ä¸åŒçš„æ—¶é—´ç‚¹å¼€å§‹ï¼Œå…·æœ‰ä¸åŒçš„æ¶æ„ä»¥åŠä¸šåŠ¡é€»è¾‘ä¸ä½¿ç”¨çš„æ¡†æ¶ä¹‹é—´çš„ä¸€äº›çº ç¼ ã€‚é‚£ä¸ªï¼†ï¼ƒ39; sananasï¼å½“ç„¶ï¼Œæœ‰æ–¹æ³•å¯ä»¥è§£å†³è¿™äº›é—®é¢˜ã€‚ä»–ä»¬ä¸­çš„å¤§å¤šæ•°äººéƒ½åœ¨é‚£äº›æ—¥å­é‡Œæ‹¿å›æ¥ï¼Œå…¶ä»–äººåœ¨FTESå·¥ä½œä¸­æˆæœ¬å¾ˆå¤šã€‚</p><p>  Another smell was when someone told me that deploying a new feature in service A also needed a deployment â€” at the same time â€” in service B. Or when people started to write services to generate CSVs. Why would someone introduce network hops to produce a worldwide known file format? Who would maintain that? Some teams were suffering from  servicitis. Even worse than that, it generated a lot of friction while developing. One could not just look into a project in their IDE, but it required to have multiple projects open simultaneously to make sense of all that mess.</p><p>  å¦ä¸€ä¸ªå—…è§‰æ˜¯å½“æœ‰äººå‘Šè¯‰æˆ‘ï¼Œåœ¨æœåŠ¡ä¸­éƒ¨ç½²æ–°åŠŸèƒ½aä¹Ÿéœ€è¦éƒ¨ç½² - åŒæ—¶ - åœ¨æœåŠ¡Bä¸­ï¼Œæˆ–è€…äººä»¬å¼€å§‹ç¼–å†™æœåŠ¡ä»¥ç”ŸæˆCSVã€‚ä¸ºä»€ä¹ˆæœ‰äººä¼šä»‹ç»ç½‘ç»œè·³è·ƒä»¥äº§ç”Ÿå…¨çƒå·²çŸ¥çš„æ–‡ä»¶æ ¼å¼ï¼Ÿè°å°†ä¿æŒè¿™ç§æƒ…å†µï¼Ÿä¸€äº›å›¢é˜Ÿæ‚£æœ‰æœåŠ¡æ€§ã€‚ç”šè‡³æ¯”é‚£æ›´ç³Ÿç³•ï¼Œå®ƒåœ¨å‘å±•æ—¶äº§ç”Ÿäº†å¾ˆå¤šæ‘©æ“¦ã€‚ä¸€ä¸ªäººä¸èƒ½åªæ˜¯åœ¨ä»–ä»¬çš„IDEä¸­è°ƒæŸ¥ä¸€ä¸ªé¡¹ç›®ï¼Œä½†å®ƒéœ€è¦åŒæ—¶æ‰“å¼€å¤šä¸ªé¡¹ç›®æ¥äº†è§£æ‰€æœ‰è¿™äº›æ··ä¹±ã€‚ </p><p>     Hey, JoÃ£o. Do you have a minute? We need to fix our development environments! People are complaining about them all the time, and this isn&#39;t working!</p><p>å˜¿ï¼ŒjoÃ£oã€‚èƒ½æ‰“æ‰°ä½ å‡ åˆ†é’Ÿå—ï¼Ÿæˆ‘ä»¬éœ€è¦ä¿®å¤æˆ‘ä»¬çš„å¼€å‘ç¯å¢ƒï¼äººä»¬ä¸€ç›´åœ¨æŠ±æ€¨ä»–ä»¬ï¼Œè¿™æ˜¯ï¼†ï¼ƒ39;å·¥ä½œï¼</p><p>  The problem crossed different dimensions. Mobile developers not developing a feature before it was in a development environment or backend developers who wanted to try their service didn&#39;t break any business flow. It was also problematic if someone wanted to test the whole flow in a mobile app before production.</p><p>  é—®é¢˜äº¤å‰ä¸åŒçš„å°ºå¯¸ã€‚ç§»åŠ¨å¼€å‘äººå‘˜æœªåœ¨å¼€å‘ç¯å¢ƒä¸­å¼€å‘ä¸€ä¸ªåŠŸèƒ½ï¼Œæˆ–è€…åœ¨å¼€å‘ç¯å¢ƒä¸­æˆ–åç«¯å¼€å‘äººå‘˜ï¼Œä»–ä»¬æƒ³è¦å°è¯•ä»–ä»¬çš„æœåŠ¡DIDNï¼†ï¼ƒ39; tæ‰“ç ´ä»»ä½•ä¸šåŠ¡æµç¨‹ã€‚å¦‚æœæœ‰äººæƒ³åœ¨ç”Ÿäº§ä¹‹å‰æµ‹è¯•åœ¨ç§»åŠ¨åº”ç”¨ç¨‹åºä¸­çš„æ•´ä¸ªæµç¨‹ä¸­ï¼Œå®ƒä¹Ÿæ˜¯æœ‰é—®é¢˜çš„ã€‚</p><p>    How much does it cost to spin 200 services in a cloud provider? Can you do it? Can you also spin up the infrastructure needed to run them?</p><p>    åœ¨äº‘æä¾›å•†ä¸­æ—‹è½¬200ä¸ªæœåŠ¡æ˜¯å¤šå°‘é’±ï¼Ÿä½ èƒ½åšåˆ°å—ï¼Ÿæ‚¨è¿˜èƒ½æ—‹è½¬è¿è¡Œå®ƒä»¬æ‰€éœ€çš„åŸºç¡€æ¶æ„å—ï¼Ÿ</p><p> How much time does it cost to do so? What if, when a mobile engineer starts to develop a feature, there&#39;s a set of services in a given version, and when they finish, there are ten new versions deployed into production?</p><p> è¿™æ ·åšæ˜¯å¤šå°‘æ—¶é—´ï¼Ÿå¦‚æœç§»åŠ¨å·¥ç¨‹å¸ˆå¼€å§‹å¼€å‘ä¸€ä¸ªåŠŸèƒ½ï¼Œé‚£ä¹ˆåœ¨ç»™å®šç‰ˆæœ¬ä¸­çš„ä¸€å¥—æœåŠ¡ï¼Œä»¥åŠåœ¨å®Œæˆæ—¶ï¼Œå°†æœ‰åä¸ªæ–°ç‰ˆæœ¬éƒ¨ç½²åˆ°ç”Ÿäº§ä¸­ï¼Ÿ</p><p> What about test data? Do you have test data for all your services? Is it coherent across the fleet, so users and other entities match?</p><p> æµ‹è¯•æ•°æ®æ€ä¹ˆæ ·ï¼Ÿæ‚¨æ˜¯å¦æœ‰æ‰€æœ‰æœåŠ¡çš„æµ‹è¯•æ•°æ®ï¼Ÿè¿™æ˜¯èˆ¹é˜Ÿçš„ä¸€è‡´æ€§ï¼Œæ‰€ä»¥ç”¨æˆ·å’Œå…¶ä»–å®ä½“åŒ¹é…å—ï¼Ÿ</p><p> If you&#39;re developing a multi-tenant, multi-region application, what about configuration and feature flags? How do you stay in sync with production? What if the defaults change meanwhile?</p><p> å¦‚æœæ‚¨ï¼†ï¼ƒ39; reå¼€å‘å¤šç§Ÿæˆ·ï¼Œå¤šåŒºåŸŸåº”ç”¨ç¨‹åºï¼Œé‚£ä¹ˆé…ç½®å’ŒåŠŸèƒ½æ ‡å¿—å¦‚ä½•ï¼Ÿæ‚¨å¦‚ä½•ä¸ç”Ÿäº§ä¿æŒåŒæ­¥ï¼Ÿå¦‚æœé»˜è®¤å€¼æ”¹å˜äº†ä»€ä¹ˆï¼Ÿ</p><p>  That is the tip of the iceberg. One can think of throwing engineering power into this problem. It might work. But I&#39;d challenge that most organizations have the scale to do it. Doing it right is astoundingly tricky and expensive.</p><p>  è¿™æ˜¯å†°å±±ä¸€è§’ã€‚äººä»¬å¯ä»¥æƒ³åˆ°å°†å·¥ç¨‹æƒåŠ›æŠ•å…¥è¿™ä¸ªé—®é¢˜ã€‚å®ƒå¯èƒ½æœ‰æ•ˆã€‚ä½†æ˜¯ï¼Œæˆ‘ï¼†ï¼ƒ39; DæŒ‘æˆ˜å¤§å¤šæ•°ç»„ç»‡éƒ½æœ‰è§„æ¨¡è¦åšã€‚åšå¾—å¯¹ä»¤äººéœ‡æƒŠå’Œæ˜‚è´µã€‚ </p><p>    As you can imagine, end-to-end tests have similar problems to development environments. Before, it was relatively easy to create a new development environment using virtual machines or containers. It was also fairly simple to create a test suite using Selenium to go through business flows and assert they were working before deploying a new version. After microservices, even if we can solve all the above&#39;s problems with setting up environments, we cannot declare that a system is working anymore. At most, we can state that a system with specific versions of the services running and a given configuration is working at a particular point in time. That&#39;s a huge difference!</p><p>æ­£å¦‚æ‚¨å¯ä»¥æƒ³è±¡çš„é‚£æ ·ï¼Œç«¯åˆ°ç«¯æµ‹è¯•å¯¹å¼€å‘ç¯å¢ƒå…·æœ‰ç±»ä¼¼çš„é—®é¢˜ã€‚ä¹‹å‰ï¼Œä½¿ç”¨è™šæ‹Ÿæœºæˆ–å®¹å™¨åˆ›å»ºæ–°çš„å¼€å‘ç¯å¢ƒç›¸å¯¹å®¹æ˜“ã€‚ä½¿ç”¨Seleniumåˆ›å»ºä¸€ä¸ªæµ‹è¯•å¥—ä»¶ï¼Œé€šè¿‡ä¸šåŠ¡æµç¨‹å¹¶æ–­è¨€ä»–ä»¬åœ¨éƒ¨ç½²æ–°ç‰ˆæœ¬ä¹‹å‰æ­£å¸¸å·¥ä½œæ˜¯ç›¸å½“ç®€å•çš„ã€‚åœ¨å¾®æœåŠ¡ä¹‹åï¼Œå³ä½¿æˆ‘ä»¬å¯ä»¥è§£å†³æ‰€æœ‰ä¸Šè¿°ç¯å¢ƒå’Œï¼ƒ39;åœ¨è®¾ç½®ç¯å¢ƒä¸­çš„é—®é¢˜æ—¶ï¼Œæˆ‘ä»¬ä¹Ÿæ— æ³•å£°æ˜ç³»ç»Ÿæ˜¯å¦å·²å†æ¬¡å·¥ä½œã€‚è‡³å¤šï¼Œæˆ‘ä»¬å¯ä»¥è¯´æ˜å…·æœ‰ç‰¹å®šç‰ˆæœ¬çš„æœåŠ¡è¿è¡Œçš„ç³»ç»Ÿå’Œç»™å®šé…ç½®åœ¨ç‰¹å®šæ—¶é—´ç‚¹å·¥ä½œã€‚é‚£ä¸ªå·¨å¤§çš„å·®å¼‚ï¼</p><p>  It was extraordinarily tough to convince people that we could not have more than a couple of these tests. And that it wasn&#39;t enough to run them in the Continuous Integration flow. They should run continuously. And they should run against production and produce alerts accordingly. I&#39;ve shared countless times Cindy Sridharan&#39;s article &#34; Testing in production, the safe way&#34; to try to make people understand my points.</p><p>  è¯´æœäººä»¬éå¸¸éš¾ä»¥è¯´æœæˆ‘ä»¬ä¸èƒ½è¶…è¿‡ä¸€äº›è¿™äº›æµ‹è¯•ã€‚å¹¶ä¸”å®ƒæ˜¯ä¸å¤Ÿåœ¨æŒç»­ç§¯åˆ†æµä¸­è¿è¡Œå®ƒä»¬çš„ï¼†ï¼ƒ39;ä»–ä»¬åº”è¯¥è¿ç»­è¿è¡Œã€‚ä»–ä»¬åº”è¯¥è¿åç”Ÿäº§å¹¶ç›¸åº”åœ°äº§ç”Ÿè­¦æŠ¥ã€‚æˆ‘åˆ†äº«äº†æ— æ•°æ¬¡Cindy Sridharanï¼†ï¼ƒ39; sæ–‡ç« ï¼†ï¼ƒ34;ç”Ÿäº§ä¸­çš„æµ‹è¯•ï¼Œå®‰å…¨çš„æ–¹å¼ï¼†ï¼ƒ34;è¯•å›¾è®©äººä»¬äº†è§£æˆ‘çš„è§‚ç‚¹ã€‚</p><p>    An easy way out of the monoliths while keeping data consistency across them is to keep using a shared database. It does not increase the operational load, and it makes it easy to slice a monolith step-by-step. However, it also comes with considerable disadvantages. Aside from being an obvious single-point-of-failure, defeating some of the service-oriented architecture&#39;s principles, there&#39;s more. Do you create a user per service? Do you have fine-grained permissions so service A can only read or write from specific tables? What if someone removes an index unintentionally? How do we know how many services are using different tables? What about scaling?</p><p>    åœ¨ä¿æŒæ•°æ®ä¸€è‡´æ€§çš„åŒæ—¶ï¼Œåœ¨æ•´ä½“ä¸Šè½»æ¾å‡ºè·¯æ˜¯ç»§ç»­ä½¿ç”¨å…±äº«æ•°æ®åº“ã€‚å®ƒä¸ä¼šå¢åŠ æ“ä½œè´Ÿè·ï¼Œå¹¶ä¸”å¯ä»¥è½»æ¾åœ°é€æ­¥åˆ‡ç‰‡ã€‚ä½†æ˜¯ï¼Œå®ƒä¹Ÿå…·æœ‰ç›¸å½“å¤§çš„ç¼ºç‚¹ã€‚é™¤äº†ä¸€ä¸ªæ˜æ˜¾çš„å•ç‚¹å¤±è´¥ï¼Œå‡»è´¥äº†ä¸€äº›é¢å‘æœåŠ¡çš„å»ºç­‘ï¼†ï¼ƒ39;ï¼†ï¼ƒ39;æ›´å¤šã€‚æ‚¨æ¯é¡¹æœåŠ¡åˆ›å»ºç”¨æˆ·å—ï¼Ÿæ‚¨æ˜¯å¦å…·æœ‰ç»†ç²’åº¦çš„æƒé™ï¼Œå› æ­¤æœåŠ¡Aåªèƒ½ä»ç‰¹å®šè¡¨ä¸­è¯»å–æˆ–å†™å…¥ï¼Ÿå¦‚æœæœ‰äººæ— æ„ä¸­åˆ é™¤æŒ‡æ•°æ€ä¹ˆåŠï¼Ÿæˆ‘ä»¬å¦‚ä½•çŸ¥é“æœ‰å¤šå°‘æœåŠ¡ä½¿ç”¨ä¸åŒçš„è¡¨ï¼Ÿç¼©æ”¾æ€ä¹ˆæ ·ï¼Ÿ</p><p>  Disentangling all of this becomes a whole new problem on its own. Technically, it may not be trivial, considering that databases tend to outlive software. Solving the problem using data replication â€” be it Kafka, AWS DMS or whatever â€” creates a need for your engineering teams to understand database specifics and how to deal with duplicated events, and so on.</p><p>  è§£å¼€æ‰€æœ‰è¿™ä¸€åˆ‡éƒ½æˆä¸ºä¸€ä¸ªå…¨æ–°çš„é—®é¢˜ã€‚ä»æŠ€æœ¯ä¸Šè®²ï¼Œè€ƒè™‘åˆ°æ•°æ®åº“å€¾å‘äºæœ€ç»ˆçš„è½¯ä»¶ï¼Œå®ƒå¯èƒ½ä¸æ˜¯å¾®ä¸è¶³é“çš„ã€‚ä½¿ç”¨æ•°æ®å¤åˆ¶è§£å†³é—®é¢˜ - æ˜¯Kafkaï¼ŒAWS DMSæˆ–å…¶ä»– - åˆ›å»ºæ‚¨çš„å·¥ç¨‹å›¢é˜Ÿéœ€è¦äº†è§£æ•°æ®åº“ç»†èŠ‚ä»¥åŠå¦‚ä½•å¤„ç†é‡å¤çš„äº‹ä»¶ï¼Œç­‰ç­‰ã€‚</p><p>    API Gateways are a typical pattern in service-oriented architectures. They&#39;re helpful to decouple the backend from the frontend consumers. They&#39;re also beneficial when it comes to implementing endpoint aggregation, rate-limiting or authentication across your system. More recently, the industry has been leaning towards  backend-for-frontend architectures, where these gateways are deployed for every single frontend consumer â€” iOS, Android, web, or desktop apps â€”, making their evolution decoupled from each other.</p><p>    APIç½‘å…³æ˜¯é¢å‘æœåŠ¡çš„æ¶æ„ä¸­çš„å…¸å‹æ¨¡å¼ã€‚ä»–ä»¬ï¼†ï¼ƒ39;é‡æ–°æœ‰åŠ©äºä»å‰ç«¯æ¶ˆè´¹è€…åˆ†ç¦»åç«¯ã€‚å®ƒä»¬åœ¨å®ç°ç³»ç»Ÿä¸­å®ç°ç«¯ç‚¹èšåˆï¼Œé™é€Ÿæˆ–èº«ä»½éªŒè¯æ—¶ä¹Ÿæœ‰ç›Šã€‚æœ€è¿‘ï¼Œè¯¥è¡Œä¸šä¸€ç›´å€¾å‘äºå‰ç«¯æ¶æ„ï¼Œå…¶ä¸­è¿™äº›ç½‘å…³ä¸ºæ¯ä¸€ä¸ªå‰ç«¯æ¶ˆè´¹è€… -  iOSï¼ŒAndroidï¼ŒWebæˆ–æ¡Œé¢åº”ç”¨ç¨‹åºéƒ¨ç½² - ä½¿ä»–ä»¬çš„è¿›åŒ–å½¼æ­¤åˆ†ç¦»ã€‚</p><p>  As with everything in this world, people start to have new, creative use-cases for it. Sometimes it&#39;s a small hack to make the mobile application backwards compatible. Suddenly, you have your &#34;API gateway&#34; being a single-point-of-failure â€” because people find it easier to handle authentication in a single place â€”  and with some unintended business logic inside it. Instead of having a monolith getting all of the traffic, now you have a home-made Spring Boot service getting all of it! What could go wrong? Engineers quickly realize this is a mistake, but as there are many customizations, sometimes they cannot substitute this piece for stateless, scale-friendly ones.</p><p>  ä¸è¿™ä¸ªä¸–ç•Œä¸Šçš„ä¸€åˆ‡ä¸€æ ·ï¼Œäººä»¬å¼€å§‹ä¸ºå®ƒåˆ›é€ æ–°çš„åˆ›é€ æ€§ç”¨ä¾‹ã€‚æœ‰æ—¶å®ƒï¼†ï¼ƒ39;æ˜¯ä¸€ä¸ªå°çš„é»‘å®¢ï¼Œä½¿ç§»åŠ¨åº”ç”¨ç¨‹åºå‘åå…¼å®¹ã€‚çªç„¶ï¼Œä½ æœ‰ä½ çš„ï¼†ï¼ƒ34; API Gatewayï¼†ï¼ƒ34;ä½œä¸ºä¸€ä¸ªå¤±è´¥çš„å•ç‚¹ - å› ä¸ºäººä»¬å‘ç°æ›´å®¹æ˜“åœ¨ä¸€ä¸ªåœ°æ–¹å¤„ç†èº«ä»½éªŒè¯ - ä»¥åŠå®ƒå†…éƒ¨çš„ä¸€äº›æ„å¤–ä¸šåŠ¡é€»è¾‘ã€‚è€Œä¸æ˜¯æ‹¥æœ‰ä¸€åªå·¨å¤§çš„äº¤é€šï¼Œç°åœ¨ä½ æœ‰ä¸€ä¸ªè‡ªåˆ¶çš„æ˜¥å¤©å¯åŠ¨æœåŠ¡æ‰èƒ½è·å¾—æ‰€æœ‰çš„æ˜¥å¤©å¯åŠ¨æœåŠ¡ï¼ä»€ä¹ˆå¯èƒ½å‡ºé”™ï¼Ÿå·¥ç¨‹å¸ˆå¾ˆå¿«æ„è¯†åˆ°è¿™æ˜¯ä¸€ä¸ªé”™è¯¯ï¼Œä½†éšç€è®¸å¤šè‡ªå®šä¹‰ï¼Œæœ‰æ—¶ä»–ä»¬ä¸èƒ½æ›¿ä»£è¿™ä»¶ä½œå“ä»¥æ— è§„å®šï¼Œçº§åˆ«å‹å¥½çš„ã€‚</p><p>  The culprit of the API gateways disasters comes when it consumes endpoints that are not paginated or return massive responses. Or when you make an aggregation without fallback mechanisms in place, making one single API call burn down your gateway.</p><p>  API Gatewaysç¾å®³çš„ç½ªé­ç¥¸é¦–æ˜¯åœ¨æ¶ˆè€—æ²¡æœ‰åˆ†æ‰‹æˆ–è¿”å›å¤§è§„æ¨¡å“åº”çš„ç«¯ç‚¹æ—¶å‡ºç°çš„ã€‚æˆ–è€…å½“æ‚¨åœ¨æ²¡æœ‰åé€€æœºåˆ¶çš„æƒ…å†µä¸‹è¿›è¡Œèšåˆæ—¶ï¼Œä½¿ä¸€ä¸ªå•ä¸ªAPIè°ƒç”¨åˆ»å½•åˆ°ç½‘å…³ã€‚ </p><p>    Distributed systems are  constantly in a partial failure mode. What happens when service A can&#39;t contact service B? We can retry our request, right? But this promptly leads us to go down the rabbit hole. I&#39;ve seen teams using circuit breakers and then increase the timeouts of an HTTP call to a service downstream. While this might be a normal reaction to buy us some time to fix the problem, it creates second-order effects. Now, all these requests that your circuit breaker would cancel because they&#39;re too long are there for more time. If there&#39;s an increase in traffic, more and more requests will get queued, leading to a worse situation than the one you wanted to fix. I&#39;ve seen that engineers struggle to understand queue theory and why there are timeouts in place. The same thing happens when teams start to discuss thread pools for their HTTP clients and whatnot. While configuring those is an art in itself, setting values based on gut feeling may set you up for a significant outage.</p><p>åˆ†å¸ƒå¼ç³»ç»Ÿå§‹ç»ˆå¤„äºéƒ¨åˆ†æ•…éšœæ¨¡å¼ã€‚åœ¨ç½å¤´å’Œï¼ƒ39; Tè”ç³»æœåŠ¡Bæ—¶ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿæˆ‘ä»¬å¯ä»¥é‡è¯•æˆ‘ä»¬çš„è¦æ±‚ï¼Œå¯¹å—ï¼Ÿä½†è¿™ä¼šåŠæ—¶å¯¼è‡´æˆ‘ä»¬èµ°ä¸‹å…”å­æ´ã€‚ä½¿ç”¨æ–­è·¯å™¨ï¼Œç„¶åä½¿ç”¨æ–­è·¯å™¨çš„å›¢é˜Ÿï¼Œç„¶åå°†HTTPè°ƒç”¨çš„è¶…æ—¶å¢åŠ åˆ°ä¸‹æ¸¸çš„æœåŠ¡ã€‚è™½ç„¶è¿™å¯èƒ½æ˜¯ä¸€ä¸ªæ­£å¸¸çš„ååº”æ¥è´­ä¹°æˆ‘ä»¬ä¸€äº›æ—¶é—´æ¥è§£å†³é—®é¢˜ï¼Œä½†å®ƒä¼šäº§ç”ŸäºŒé˜¶æ•ˆåº”ã€‚ç°åœ¨ï¼Œæ‰€æœ‰è¿™äº›è¯·æ±‚ä½¿æ‚¨çš„æ–­è·¯å™¨ä¼šå–æ¶ˆï¼Œå› ä¸ºå®ƒä»¬ï¼†ï¼ƒ39; re the loceåœ¨é‚£é‡Œæœ‰æ›´å¤šæ—¶é—´ã€‚å¦‚æœæœ‰æµé‡å¢åŠ ï¼Œè¶Šæ¥è¶Šå¤šçš„è¯·æ±‚å°†è¢«æ’é˜Ÿï¼Œå¯¼è‡´æ¯”æ‚¨æƒ³è¦ä¿®å¤çš„æƒ…å†µæ›´å·®ã€‚æˆ‘çœ‹åˆ°å·¥ç¨‹å¸ˆåŠªåŠ›äº†è§£é˜Ÿåˆ—ç†è®ºä»¥åŠä¸ºä»€ä¹ˆæœ‰è¶…æ—¶çš„åœ°æ–¹ã€‚å½“å›¢é˜Ÿå¼€å§‹è®¨è®ºä»–ä»¬çš„HTTPå®¢æˆ·ç«¯å’ŒWhatnotæ—¶ï¼Œå°±ä¼šå‘ç”ŸåŒæ ·çš„äº‹æƒ…ã€‚åœ¨é…ç½®é‚£äº›æœ¬èº«çš„æ—¶ï¼ŒåŸºäºè‚ é“çš„è®¾å®šå€¼å¯èƒ½ä¼šä½¿æ‚¨è®¾ç½®ä¸ºæ˜¾ç€çš„ä¸­æ–­ã€‚</p><p>  A tricky thing when recovering from a failure is that not all of them are created equal. We may expect our consumer to be idempotent in some cases. But this means that we should proactively decide what to do in each of the failure scenarios. Is the consumer idempotent? Can I retry this call? I&#39;ve seen many engineers ignoring these because it&#39;s &#34;an edge case&#34;, to realize later they have a massive data integrity problem.</p><p>  ä»å¤±è´¥ä¸­æ¢å¤æ—¶æ£˜æ‰‹çš„äº‹æƒ…æ˜¯å¹¶éæ‰€æœ‰è¿™äº›éƒ½æ˜¯ç›¸ç­‰çš„ã€‚åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯èƒ½å¸Œæœ›æ¶ˆè´¹è€…èƒ½å¤Ÿå®½å®¹ã€‚ä½†è¿™æ„å‘³ç€æˆ‘ä»¬åº”è¯¥ä¸»åŠ¨å†³å®šåœ¨æ¯ä¸ªæ•…éšœæƒ…æ™¯ä¸­åšäº›ä»€ä¹ˆã€‚æ¶ˆè´¹è€…æ˜¯ä¸ªä½“åŒ–å—ï¼Ÿæˆ‘å¯ä»¥é‡è¯•è¿™ä¸ªç”µè¯å—ï¼Ÿæˆ‘çœ‹åˆ°è®¸å¤šå·¥ç¨‹å¸ˆå¿½ç•¥äº†è¿™äº›ï¼Œå› ä¸ºå®ƒï¼†ï¼ƒ39; sï¼†ï¼ƒ34;ä¸€ä¸ªè¾¹ç¼˜æ¡ˆä¾‹ï¼†ï¼ƒ34;ï¼Œä»¥ç¨åå®ç°å®ƒä»¬å…·æœ‰å¤§è§„æ¨¡çš„æ•°æ®å®Œæ•´æ€§é—®é¢˜ã€‚</p><p>  Retries are even trickier than all of this, even if you set up fallback mechanisms. Imagine that you have five million users in your mobile app and that your message bus that updates users preferences&#39; stopped working for a while. You set up a fallback mechanism for that case, which calls the users&#39; preferences service through an HTTP API. I guess you know where I&#39;m going. Now, this service got a massive traffic spike suddenly, and it may not be able to cope with all the traffic. It&#39;s even worse than that: your service  might be able to get all these new requests, but if the retries mechanism doesn&#39;t implement exponential backoff  and jitter, you might experience a distributed denial-of-service from your mobile applications.</p><p>  å³ä½¿æ‚¨è®¾ç½®äº†å›é€€æœºåˆ¶ï¼Œé‡è¯•ç”šè‡³æ¯”æ‰€æœ‰è¿™äº›éƒ½å‰§çƒˆã€‚æƒ³è±¡ä¸€ä¸‹ï¼Œæ‚¨çš„ç§»åŠ¨åº”ç”¨ç¨‹åºä¸­æœ‰äº”ç™¾ä¸‡ç”¨æˆ·ï¼Œæ‚¨çš„ç•™è¨€æ€»çº¿æ›´æ–°ç”¨æˆ·åå¥½ï¼†ï¼ƒ39;åœæ­¢å·¥ä½œä¸€ä¼šå„¿ã€‚æ‚¨ä¸ºè¯¥æ¡ˆä¾‹è®¾ç½®äº†ä¸€ä¸ªæŠµæŠ¼æœºåˆ¶ï¼Œè¯¥æœºåˆ¶è°ƒç”¨ç”¨æˆ·ï¼†ï¼ƒ39;é¦–é€‰é¡¹é€šè¿‡HTTP APIæœåŠ¡ã€‚æˆ‘çŒœä½ çŸ¥é“åœ¨å“ªé‡Œæˆ‘å»äº†ã€‚ç°åœ¨ï¼Œè¿™é¡¹æœåŠ¡çªç„¶å‡ºç°äº†å¤§é‡çš„äº¤é€šé£™å‡ï¼Œå¯èƒ½æ— æ³•åº”å¯¹æ‰€æœ‰æµé‡ã€‚å®ƒç”šè‡³æ›´ç³Ÿç³•çš„æ˜¯ï¼šæ‚¨çš„æœåŠ¡å¯èƒ½èƒ½å¤Ÿè·å¾—æ‰€æœ‰è¿™äº›æ–°è¯·æ±‚ï¼Œè€Œæ˜¯å¦‚æœé‡è¯•æœºåˆ¶ä¸å—æŒ‡æ•°é€€é¿å’ŒæŠ–åŠ¨ï¼Œæ‚¨å¯èƒ½ä¼šé‡åˆ°æ¥è‡ªæ‚¨çš„åˆ†å¸ƒå¼æ‹’ç»æœåŠ¡ç§»åŠ¨åº”ç”¨ç¨‹åºã€‚</p><p>    What if I told you that I only wrote about a fraction of the disasters I&#39;ve seen? ğŸ¤£ Distributed systems are hard to grasp, and only recently most software engineers have been consistently exposed to them.</p><p>    å¦‚æœæˆ‘å‘Šè¯‰è¿‡ä½ æˆ‘åªå†™äº†å…³äºç¾éš¾çš„ä¸€å°éƒ¨åˆ†æˆ‘çœ‹åˆ°äº†ä»€ä¹ˆï¼Ÿ ğŸ¤£åˆ†å¸ƒå¼ç³»ç»Ÿå¾ˆéš¾æŒæ¡ï¼Œæœ€è¿‘å¤§å¤šæ•°è½¯ä»¶å·¥ç¨‹å¸ˆéƒ½å§‹ç»ˆæš´éœ²äºå®ƒä»¬ã€‚</p><p>   The good thing is that many of the  disasters I&#39;ve talked about have good answers, and the industry has created better tools to make them solvable by organizations other than  FAANG.</p><p>   å¥½äº‹æ˜¯ï¼Œè®¸å¤šç¾éš¾æˆ‘è°ˆåˆ°äº†æœ‰è‰¯å¥½çš„ç­”æ¡ˆï¼Œè¡Œä¸šåˆ›é€ äº†æ›´å¥½çš„å·¥å…·ï¼Œä½¿ä»–ä»¬ç”±æ¦´æ™¯ä»¥å¤–çš„ç»„ç»‡åˆ¶å®šã€‚</p><p>  I still love distributed systems, and I still think that microservices are a good solution for organizational problems. However, the problems come when we think about failures as &#34;edge cases&#34; or things that we think will never happen to us. These edge cases become the new normal at a certain scale, and we should cope with them.</p><p>  æˆ‘ä»ç„¶å–œæ¬¢åˆ†å¸ƒå¼ç³»ç»Ÿï¼Œæˆ‘ä»ç„¶è®¤ä¸ºå¾®æœåŠ¡æ˜¯ç»„ç»‡é—®é¢˜çš„å¥½è§£å†³æ–¹æ¡ˆã€‚ä½†æ˜¯ï¼Œå½“æˆ‘ä»¬è€ƒè™‘åˆ°ï¼†ï¼ƒ34çš„æ•…éšœæ—¶ï¼Œé—®é¢˜æ¥è‡ªäºï¼†ï¼ƒ34;è¾¹ç¼˜æ¡ˆä¾‹ï¼†ï¼ƒ34;æˆ–è€…æˆ‘ä»¬è®¤ä¸ºæ°¸è¿œä¸ä¼šå‘ç”Ÿåœ¨æˆ‘ä»¬èº«ä¸Šçš„äº‹æƒ…ã€‚è¿™äº›è¾¹ç¼˜æ¡ˆä¾‹ä»¥ä¸€å®šçš„è§„æ¨¡å˜æˆæ–°çš„æ­£å¸¸ï¼Œæˆ‘ä»¬åº”è¯¥åº”å¯¹å®ƒä»¬ã€‚ </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://world.hey.com/joaoqalves/disasters-i-ve-seen-in-a-microservices-world-a9137a51">https://world.hey.com/joaoqalves/disasters-i-ve-seen-in-a-microservices-world-a9137a51</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/æœåŠ¡/">#æœåŠ¡</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/ve/">#ve</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è®¾è®¡/">#è®¾è®¡</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/åˆ›æ„/">#åˆ›æ„</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/æ‘„å½±/">#æ‘„å½±</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/æ¸¸æˆ/">#æ¸¸æˆ</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/å›¾ç‰‡/">#å›¾ç‰‡</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è½¯ä»¶/">#è½¯ä»¶</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è§†é¢‘/">#è§†é¢‘</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/æ‰‹æœº/">#æ‰‹æœº</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/å¹¿å‘Š/">#å¹¿å‘Š</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/å…è´¹/">#å…è´¹</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/ç½‘ç«™/">#ç½‘ç«™</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/ä¸‹è½½/">#ä¸‹è½½</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/å¾®è½¯/">#å¾®è½¯</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è‹¹æœ/">#è‹¹æœ</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/éŸ³ä¹/">#éŸ³ä¹</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/åšå®¢/">#åšå®¢</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/æ¶æ/">#æ¶æ</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è‰ºæœ¯/">#è‰ºæœ¯</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/å·¥å…·/">#å·¥å…·</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/åˆ†äº«/">#åˆ†äº«</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>