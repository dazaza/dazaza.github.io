<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>åœ¨å®è·µä¸­å¾ˆå°‘æ‹æ‘„å­¦ä¹ ï¼šGPT-NEOï¼†'HUGGINGFACE'åŠ é€Ÿæ¨ç†API Few-Shot Learning in Practice: GPT-Neo & 'HuggingFace' Accelerated Inference API</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Few-Shot Learning in Practice: GPT-Neo & 'HuggingFace' Accelerated Inference API<br/>åœ¨å®è·µä¸­å¾ˆå°‘æ‹æ‘„å­¦ä¹ ï¼šGPT-NEOï¼†'HUGGINGFACE'åŠ é€Ÿæ¨ç†API </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-06-05 02:58:42</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/6/5c5513aee074a0f4b419b0118de6a9c0.png"><img src="http://img2.diglog.com/img/2021/6/5c5513aee074a0f4b419b0118de6a9c0.png" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>In many Machine Learning applications, the amount of available labeled data is a barrier to producing a high-performing model. The latest developments in NLP show that you can overcome this limitation by providing a few examples at inference time with a large language model - a technique known as Few-Shot Learning. In this blog post, we&#39;ll explain what Few-Shot Learning is, and explore how a large language model called GPT-Neo, and the ğŸ¤— Accelerated Inference API, can be used to generate your own predictions.</p><p>åœ¨è®¸å¤šæœºå™¨å­¦ä¹ åº”ç”¨ä¸­ï¼Œå¯ç”¨æ ‡è®°æ•°æ®çš„é‡æ˜¯äº§ç”Ÿé«˜æ€§èƒ½æ¨¡å‹çš„éšœç¢ã€‚ NLPçš„æœ€æ–°è¿›å±•è¡¨æ˜ï¼Œæ‚¨å¯ä»¥é€šè¿‡æä¾›å…·æœ‰å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†æ—¶é—´çš„å‡ ä¸ªä¾‹å­æ¥å…‹æœè¿™ç§é™åˆ¶ - ä¸€ç§ç§°ä¸ºå°‘é‡å­¦ä¹ çš„æŠ€æœ¯ã€‚åœ¨è¿™ä¸ªåšå®¢æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬ï¼†ï¼ƒ39; llè§£é‡Šäº†å¾ˆå°‘æ‹æ‘„çš„å­¦ä¹ ï¼Œå¹¶æ¢ç´¢ç§°ä¸ºgpt-neoçš„å¤§å‹è¯­è¨€æ¨¡å‹å’ŒğŸ¤—åŠ é€Ÿæ¨ç†APIï¼Œå¯ç”¨äºç”Ÿæˆè‡ªå·±çš„é¢„æµ‹ã€‚</p><p>   Few-Shot Learning refers to the practice of feeding a machine learning model with a very small amount of training data to guide its predictions, like a few examples at inference time, as opposed to standard fine-tuning techniques which require a relatively large amount of training data for the pre-trained model to adapt to the desired task with accuracy.</p><p>   å°‘é‡å­¦ä¹ æ˜¯æŒ‡ç”¨éå¸¸å°‘é‡çš„è®­ç»ƒæ•°æ®å–‚å…»æœºå™¨å­¦ä¹ æ¨¡å‹çš„åšæ³•ï¼Œä»¥æŒ‡å¯¼å…¶é¢„æµ‹ï¼Œä¾‹å¦‚æ¨ç†æ—¶é—´çš„å‡ ä¸ªä¾‹å­ï¼Œè€Œä¸æ˜¯éœ€è¦ç›¸å¯¹å¤§é‡çš„æ ‡å‡†å¾®è°ƒæŠ€æœ¯é¢„å…ˆè®­ç»ƒæ¨¡å‹çš„åŸ¹è®­æ•°æ®ï¼Œä»¥å‡†ç¡®æ€§é€‚åº”æ‰€éœ€çš„ä»»åŠ¡ã€‚</p><p> This technique has been mostly used in computer vision, but with some of the latest Language Models, like  EleutherAI GPT-Neo and  OpenAI GPT-3, we can now use it in Natural Language Processing (NLP).</p><p> è¿™ç§æŠ€æœ¯ä¸»è¦ç”¨äºè®¡ç®—æœºæ„¿æ™¯ï¼Œä½†ä¸ä¸€äº›æœ€æ–°çš„è¯­è¨€æ¨¡å‹ï¼Œå¦‚Eleutherai GPT-Neoå’ŒOpenai GPT-3ï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­ä½¿ç”¨å®ƒã€‚</p><p> In NLP, Few-Shot Learning can be used with Large Language Models, which have learned to perform a wide number of tasks implicitly during their pre-training on large text datasets. This enables the model to generalize, that is to understand related but previously unseen tasks, with just a few examples.</p><p> åœ¨NLPä¸­ï¼Œå¾ˆå°‘æ‹æ‘„çš„å­¦ä¹ å¯ä»¥ä¸å¤§å‹è¯­è¨€æ¨¡å‹ä¸€èµ·ä½¿ç”¨ï¼Œè¿™å·²ç»å­¦ä¼šäº†åœ¨ä»–ä»¬åœ¨å¤§å‹æ–‡æœ¬æ•°æ®é›†çš„é¢„è®­ç»ƒæœŸé—´éšå«åœ°æ‰§è¡Œå¹¿æ³›çš„ä»»åŠ¡ã€‚è¿™ä½¿å¾—æ¨¡å‹æ¦‚æ‹¬ï¼Œå³è¦äº†è§£ç›¸å…³ä½†ä»¥å‰çœ‹ä¸è§çš„ä»»åŠ¡ï¼Œåªæœ‰å‡ ä¸ªä¾‹å­ã€‚</p><p>  Task Description: A short description of what the model should do, e.g. &#34;Translate English to French&#34;</p><p>  ä»»åŠ¡è¯´æ˜ï¼šç®€çŸ­æè¿°æ¨¡å‹åº”è¯¥åšçš„äº‹æƒ…ï¼Œä¾‹å¦‚ï¼†ï¼ƒ34;å°†è‹±è¯­ç¿»è¯‘ä¸ºæ³•è¯­ï¼†ï¼ƒ34;</p><p> Examples: A few examples showing the model what it is expected to predict, e.g. &#34;sea otter =&gt; loutre de mer&#34;</p><p> ç¤ºä¾‹ï¼šä¸€äº›ä¾‹å­ï¼Œæ˜¾ç¤ºäº†é¢„æœŸé¢„æµ‹çš„æ¨¡å‹ï¼Œä¾‹å¦‚é¢„æœŸã€‚ ï¼†ï¼ƒ34;æµ·ç­=ï¼†gt; loutre de merï¼†ï¼ƒ34;</p><p> Prompt: The beginning of a new example, which the model should complete by generating the missing text, e.g. &#34;cheese =&gt; &#34;</p><p> æç¤ºï¼šé€šè¿‡ç”Ÿæˆä¸¢å¤±çš„æ–‡æœ¬ï¼Œä¾‹å¦‚ï¼Œè¯¥æ¨¡å‹çš„å¼€å¤´ï¼Œä¾‹å¦‚ï¼Œè¯¥æ¨¡å‹åº”è¯¥å®Œæˆã€‚ ï¼†ï¼ƒ34;å¥¶é…ª=ï¼†gt; ï¼†ï¼ƒ34; </p><p>  Creating these few-shot examples can be tricky, since you need to articulate the â€œtaskâ€ you want the model to perform through them. A common issue is that models, especially smaller ones, are very sensitive to the way the examples are written.</p><p>åˆ›å»ºè¿™äº›å‡ å¼ é•œå¤´ç¤ºä¾‹å¯èƒ½å¾ˆæ£˜æ‰‹ï¼Œå› ä¸ºæ‚¨éœ€è¦é˜æ˜â€œä»»åŠ¡â€ï¼Œå› æ­¤æ‚¨å¸Œæœ›æ¨¡å‹é€šè¿‡å®ƒä»¬æ‰§è¡Œã€‚å¸¸è§é—®é¢˜æ˜¯æ¨¡å‹ï¼Œå°¤å…¶æ›´å°çš„æ¨¡å‹å¯¹æè¿°çš„æ–¹å¼éå¸¸æ•æ„Ÿã€‚</p><p> An approach to optimize Few-Shot Learning in production is to learn a common representation for a task and then train task-specific classifiers on top of this representation.</p><p> ä¸€ç§ä¼˜åŒ–ç”Ÿäº§ä¸­çš„å°‘é‡å­¦ä¹ çš„æ–¹æ³•æ˜¯å­¦ä¹ ä»»åŠ¡çš„å…±åŒè¡¨ç¤ºï¼Œç„¶ååœ¨æ­¤è¡¨ç¤ºçš„é¡¶éƒ¨åŸ¹è®­ç‰¹å®šäºç‰¹å®šäºç‰¹å®šäºç‰¹å®šçš„åˆ†ç±»å™¨ã€‚</p><p> OpenAI showed in the  GPT-3 Paper that the few-shot prompting ability improves with the number of language model parameters.</p><p> Openaiåœ¨GPT-3çº¸ä¸Šæ˜¾ç¤ºï¼Œå‡ æ¬¡å°„å‡»æç¤ºèƒ½åŠ›éšç€è¯­è¨€æ¨¡å‹å‚æ•°çš„æ•°é‡è€Œæ”¹è¿›ã€‚</p><p>  Let&#39;s now take a look at how at how GPT-Neo and the ğŸ¤— Accelerated Inference API can be used to generate your own Few-Shot Learning predictions!</p><p>  Letï¼†ï¼ƒ39;ç°åœ¨çœ‹çœ‹å¦‚ä½•ä½¿ç”¨GPT-NEOå’ŒğŸ¤—åŠ é€Ÿæ¨ç†APIæ¥ç”Ÿæˆæ‚¨è‡ªå·±çš„å°‘é‡å­¦ä¹ é¢„æµ‹ï¼</p><p>   GPTâ -â Neo is a family of transformer-based language models from  EleutherAI based on the GPT architecture.  EleutherAI&#39;s primary goal is to train a model that is equivalent in size to GPTâ -â 3 and make it available to the public under an open license.</p><p>   GPTâ -â neoæ˜¯ä¸€ç³»åˆ—æ¥è‡ªEleutheraiçš„åŸºäºå˜å‹å™¨çš„è¯­è¨€æ¨¡å‹ï¼ŒåŸºäºGPTæ¶æ„ã€‚ eleutheraiï¼†ï¼ƒ39;åˆçº§ç›®æ ‡æ˜¯åŸ¹è®­ä¸€ä¸ªç›¸å½“äºå°ºå¯¸çš„æ¨¡å‹ï¼Œä»¥ä¾¿åœ¨å…¬å¼€è®¸å¯ä¸‹å‘å…¬ä¼—æä¾›ã€‚</p><p> All of the currently available GPT-Neo checkpoints are trained with the Pile dataset, a large text corpus that is extensively documented in ( Gao et al., 2021). As such, it is expected to function better on the text that matches the distribution of its training text; we recommend keeping this in mind when designing your examples.</p><p> æ‰€æœ‰å½“å‰å¯ç”¨çš„GPT-NEOæ£€æŸ¥ç‚¹éƒ½æ¥å—äº†æ¡©æ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªå¹¿æ³›è®°å½•çš„å¤§æ–‡æœ¬è¯­æ–™åº“ï¼ˆGaoç­‰ï¼Œ2021ï¼‰ã€‚å› æ­¤ï¼Œé¢„è®¡å°†åœ¨ä¸å…¶åŸ¹è®­æ–‡æœ¬åˆ†é…åŒ¹é…çš„æ–‡æœ¬ä¸Šæ›´å¥½åœ°è¿è¡Œ;æˆ‘ä»¬å»ºè®®åœ¨è®¾è®¡ç¤ºä¾‹æ—¶ç‰¢è®°è¿™ä¸€ç‚¹ã€‚</p><p>   The  Accelerated Inference API is our hosted service to run inference on any of the 10,000+ models publicly available on the ğŸ¤— Model Hub, or your own private models, via simple API calls. The API includes acceleration on CPU and GPU with  up to 100x speedup compared to out of the box deployment of Transformers.</p><p>   åŠ é€Ÿæ¨ç†APIæ˜¯æˆ‘ä»¬çš„æ‰˜ç®¡æœåŠ¡ï¼Œä»¥é€šè¿‡ç®€å•çš„APIè°ƒç”¨åœ¨ã€Œæ¨¡å‹é›†çº¿å™¨ä¸Šå…¬å¼€å¯ç”¨çš„10,000å¤šç§å‹å·ä¸­çš„ä»»ä½•10,000å¤šç§å‹å·çš„æ¨ç†ã€‚ APIåŒ…æ‹¬CPUå’ŒGPUçš„åŠ é€Ÿï¼Œä¸OFFORMEREREçš„ç›’å­éƒ¨ç½²ç›¸æ¯”ï¼Œé«˜é€Ÿå¢é€Ÿã€‚ </p><p> To integrate Few-Shot Learning predictions with  GPT-Neo in your own apps, you can use the ğŸ¤— Accelerated Inference API with the code snippet below. You can find your API Token  here, if you don&#39;t have an account you can get started  here.</p><p>è¦åœ¨æ‚¨è‡ªå·±çš„åº”ç”¨ç¨‹åºä¸­ä¸GPT-Neoé›†æˆäº†å‡ ç§’é’Ÿå­¦ä¹ é¢„æµ‹ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä¸‹é¢çš„ä»£ç ç‰‡æ®µçš„ğŸ¤—åŠ é€Ÿæ¨ç†APIã€‚å¦‚æœæ‚¨æ²¡æœ‰è´¦æˆ·ï¼Œæ‚¨å¯ä»¥åœ¨æ­¤å¤„æ‰¾åˆ°æ‚¨çš„APIä»¤ç‰Œï¼Œæ‚¨å¯ä»¥åœ¨æ­¤å¤„å¼€å§‹ã€‚</p><p> import json import requestsAPI_TOKEN =  &#34;&#34;  def  ( payload= &#39;&#39;,parameters= None,options={ &#39;use_cache&#39;:  False}): API_URL =  &#34;https://api-inference.huggingface.co/models/EleutherAI/gpt-neo-2.7B&#34; headers = { &#34;Authorization&#34;:  f&#34;Bearer  {API_TOKEN}&#34;} body = { &#34;inputs&#34;:payload, &#39;parameters&#39;:parameters, &#39;options&#39;:options} response = requests.request( &#34;POST&#34;, API_URL, headers=headers, data= json.dumps(body))  try: response.raise_for_status()  except requests.exceptions.HTTPError:  return  &#34;Error:&#34;+ &#34; &#34;.join(response.json()[ &#39;error&#39;])  else:  return response.json()[ 0][ &#39;generated_text&#39;]parameters = {  &#39;max_new_tokens&#39;: 25,  # number of generated tokens  &#39;temperature&#39;:  0.5,  # controlling the randomness of generations  &#39;end_sequence&#39;:  &#34;###&#34;  # stopping sequence for generation}prompt= &#34;....&#34;  # few-shot promptdata = query(prompt,parameters,options)</p><p> å¯¼å…¥JSON Import RequestSapi_Token =ï¼†ï¼ƒ34;ï¼†ï¼ƒ34; defï¼ˆpayload =ï¼†ï¼ƒ39;ï¼†ï¼ƒ39;ï¼Œparameters = noneï¼Œé€‰é¡¹= {ï¼†ï¼ƒ39; use_cacheï¼†ï¼ƒ39 ;: false}ï¼‰ï¼šapi_url =ï¼†ï¼ƒ34; httpsï¼š//api-inference.huggingface.co /models/eleutherai/gpt-neoge.7b&#34;æ ‡é¢˜= {ï¼†ï¼ƒ34;æˆæƒï¼†ï¼ƒ34 ;: fï¼†ï¼ƒ34;æŒç¥¨äºº{api_token}ï¼†ï¼ƒ34;} body = {ï¼†ï¼ƒ34;è¾“å…¥ï¼†ï¼ƒ34;ï¼šæœ‰æ•ˆè½½è·ï¼Œï¼†ï¼ƒ39;å‚æ•°ï¼†ï¼ƒ39;ï¼šå‚æ•°ï¼Œ ï¼†ï¼ƒ39;é€‰é¡¹ï¼†ï¼ƒ39;ï¼šé€‰é¡¹} response} response = requests.requestï¼ˆï¼†ï¼ƒ34; post andï¼ƒ34; api_urlï¼Œheaders =æ ‡é¢˜ï¼Œdata = json.dumpsï¼ˆbodyï¼‰ï¼‰å°è¯•ï¼šresponse.raise_for_statusï¼ˆï¼‰é™¤è¯·æ±‚ä¹‹å¤–.Exceptions.httperrorï¼šè¿”å›ï¼†ï¼ƒ34;é”™è¯¯ï¼šï¼†ï¼ƒ34; +ï¼†ï¼ƒ34; ï¼†ï¼ƒ34; .joinï¼ˆresponse.jsonï¼ˆï¼‰[ï¼†ï¼ƒ39;é”™è¯¯ï¼†ï¼ƒ39;]ï¼‰elseï¼šreturn response.jsonï¼ˆï¼‰[0] [ï¼†ï¼ƒ39;ç”Ÿæˆ_textï¼†ï¼ƒ39; parameters = {ï¼†ï¼ƒ39 ; max_new_tokensï¼†ï¼ƒ39 ;: 25ï¼Œç”Ÿæˆçš„ä»¤ç‰Œçš„æ•°é‡å’Œï¼ƒ39;æ¸©åº¦ï¼†ï¼ƒ39 ;: 0.5ï¼Œï¼ƒæ§åˆ¶å‡ ä»£äººçš„éšæœºæ€§ï¼†ï¼ƒ39; end_sequenceï¼†ï¼ƒ39 ;:ï¼†ï¼ƒ34; ###ï¼†ï¼ƒ34; ï¼ƒåœæ­¢ç”Ÿæˆåºåˆ—}æç¤º=ï¼†ï¼ƒ34; ....ï¼†ï¼ƒ34; ï¼ƒå°‘é‡æ‹æ‘„æç¤º=æŸ¥è¯¢ï¼ˆæç¤ºï¼Œå‚æ•°ï¼Œé€‰é¡¹ï¼‰</p><p>   Here are some practical insights, which help you get started using  GPT-Neo and the ğŸ¤— Accelerated Inference API.</p><p>   ä»¥ä¸‹æ˜¯ä¸€äº›å®ç”¨çš„è§è§£ï¼Œå¸®åŠ©æ‚¨ä½¿ç”¨GPT-NEOå’ŒğŸ¤—åŠ é€Ÿæ¨ç†APIå¼€å§‹ã€‚</p><p> Since  GPT-Neo (2.7B) is about 60x smaller than  GPT-3 (175B), it does not generalize as well to zero-shot problems and needs 3-4 examples to achieve good results. When you provide more examples  GPT-Neo understands the task and takes the  end_sequence into account, which allows us to control the generated text pretty well.</p><p> ç”±äºGPT-Neoï¼ˆ2.7bï¼‰å°äºGPT-3ï¼ˆ175bï¼‰ï¼Œå› æ­¤å®ƒä¸æ¦‚æ‹¬ä¸ºé›¶å°„å‡»é—®é¢˜ï¼Œå¹¶ä¸”éœ€è¦3-4ä¸ªä¾‹å­æ¥å®ç°è‰¯å¥½çš„ç»“æœã€‚å½“æ‚¨æä¾›æ›´å¤šç¤ºä¾‹æ—¶ï¼ŒGPT-neoäº†è§£ä»»åŠ¡å¹¶å°†End_sequenceå ç”¨ï¼Œè¿™å…è®¸æˆ‘ä»¬æ§åˆ¶ç”Ÿæˆçš„æ–‡æœ¬ã€‚</p><p>  The hyperparameter  End Sequence,  Token Length &amp;  Temperature can be used to control the  text-generation of the model and you can use this to your advantage to solve the task you need. The  Temperature controlls the randomness of your generations, lower temperature results in less random generations and higher temperature results in more random generations.</p><p>  å°é—­å¼è®¡ç»“æŸåºåˆ—ï¼Œä»¤ç‰Œé•¿åº¦ï¼†amp;æ¸©åº¦å¯ç”¨äºæ§åˆ¶æ¨¡å‹çš„æ–‡æœ¬ç”Ÿæˆï¼Œå¹¶ä¸”æ‚¨å¯ä»¥å°†æ­¤ç”¨äºè§£å†³æ‚¨æ‰€éœ€çš„ä»»åŠ¡ã€‚æ¸©åº¦æ§åˆ¶å‡ ä»£éšæœºæ€§ï¼Œè¾ƒä½çš„æ¸©åº¦å¯¼è‡´éšæœºçš„å‡ ä»£ï¼Œæ›´é«˜çš„æ¸©åº¦å¯¼è‡´æ›´å¤šéšæœºçš„å‡ ä»£ã€‚</p><p>  In the example, you can see how important it is to define your hyperparameter. These can make the difference between solving your task or failing miserably.</p><p>  åœ¨è¯¥ç¤ºä¾‹ä¸­ï¼Œæ‚¨å¯ä»¥çœ‹åˆ°å®šä¹‰HyperParameteræœ‰å¤šé‡è¦ã€‚è¿™äº›å¯ä»¥åœ¨è§£å†³ä»»åŠ¡æˆ–æ¶ä½œå‰§å¤±è´¥ä¹‹é—´äº§ç”Ÿå·®å¼‚ã€‚</p><p>   Few-Shot Learning is a powerful technique but also presents unique pitfalls that need to be taken into account when designing uses cases.To illustrate this, let&#39;s consider the default  Sentiment Analysis setting provided in the widget. After seeing three examples of sentiment classification, the model makes the following predictions 4 times out of 5, with  temperature set to 0.1:</p><p>   å¾ˆå°‘æ‹æ‘„çš„å­¦ä¹ æ˜¯ä¸€ç§å¼ºå¤§çš„æŠ€æœ¯ï¼Œä½†ä¹Ÿå‘ˆç°äº†åœ¨è®¾è®¡ä½¿ç”¨æƒ…å†µæ—¶éœ€è¦è€ƒè™‘çš„ç‹¬ç‰¹é™·é˜±ã€‚è¦ç¤ºå‡ºæ­¤æ“ä½œï¼Œè¯·ï¼†ï¼ƒ39; sè€ƒè™‘çª—å£å°éƒ¨ä»¶ä¸­æä¾›çš„é»˜è®¤æƒ…ç»ªåˆ†æè®¾ç½®ã€‚åœ¨çœ‹åˆ°ä¸‰ä¸ªæƒ…ç»ªåˆ†ç±»çš„ä¾‹å­ä¹‹åï¼Œæ¨¡å‹ä½¿ä»¥ä¸‹é¢„æµ‹å€¼ä¸º5çš„4æ¬¡ï¼Œæ¸©åº¦è®¾å®šä¸º0.1ï¼š </p><p>  What could go wrong? Imagine that you are using sentiment analysis to aggregate reviews of products on an online shopping website: a possible outcome could be that items useful to people with disabilities would be automatically down-ranked - a form of automated discrimination. For more on this specific issue, we recommend the ACL 2020 paper  Social Biases in NLP Models as Barriers for Persons with Disabilities. Because Few-Shot Learning relies more directly on information and associations picked up from pre-training, it makes it more sensitive to this type of failures.</p><p>ä»€ä¹ˆå¯èƒ½å‡ºé”™ï¼Ÿæƒ³è±¡ä¸€ä¸‹ï¼Œæ‚¨æ­£åœ¨ä½¿ç”¨æƒ…æ„Ÿåˆ†æåœ¨ç½‘ä¸Šè´­ç‰©ç½‘ç«™ä¸Šçš„äº§å“æ±‡æ€»å®¡æŸ¥ï¼šå¯èƒ½çš„ç»“æœå¯ä»¥æ˜¯å¯¹æ®‹ç–¾äººæœ‰ç”¨çš„ç‰©å“å°†è‡ªåŠ¨æ’æ”¾ - ä¸€ç§è‡ªåŠ¨æ­§è§†çš„å½¢å¼ã€‚æœ‰å…³æ­¤å…·ä½“é—®é¢˜çš„æ›´å¤šä¿¡æ¯ï¼Œæˆ‘ä»¬å»ºè®®å°†ACL 2020çº¸å¼ ç¤¾ä¼šåè§ä½œä¸ºæ®‹ç–¾äººçš„éšœç¢ã€‚ç”±äºå°‘é‡å­¦ä¹ æ›´ç›´æ¥ä¾èµ–äºä»é¢„è®­ç»ƒä¸­æ‹¾å–çš„ä¿¡æ¯å’Œå…³è”ï¼Œå› æ­¤å®ƒä½¿å…¶å¯¹è¿™ç§ç±»å‹çš„æ•…éšœæ›´æ•æ„Ÿã€‚</p><p>   Make sure people know which parts of their user experience depend on the outputs of the ML system</p><p>   ç¡®ä¿äººä»¬çŸ¥é“å…¶ç”¨æˆ·ä½“éªŒçš„å“ªäº›éƒ¨åˆ†å–å†³äºMLç³»ç»Ÿçš„è¾“å‡º</p><p>  Provide a mechanism for users to give feedback on the model decision, and to override it</p><p>  æä¾›äº†ä¸€ç§æœºåˆ¶ï¼Œè®©ç”¨æˆ·å¯¹æ¨¡å‹å†³å®šæä¾›åé¦ˆï¼Œå¹¶è¦†ç›–å®ƒ</p><p> What needs most to be avoided is to use the model to automatically make decisions for, or about, a user, without opportunity for a human to provide input or correct the output. Several regulations, such as  GDPR in Europe, require that users be provided an explanation for automatic decisions made about them.</p><p> æœ€éœ€è¦é¿å…çš„æ˜¯ä½¿ç”¨è¯¥æ¨¡å‹æ¥è‡ªåŠ¨ä¸ºç”¨æˆ·è‡ªåŠ¨åšå‡ºå†³ç­–ï¼Œè€Œä¸ä¸ºäººç±»æä¾›è¾“å…¥æˆ–æ›´æ­£è¾“å‡ºçš„æœºä¼šã€‚æ¬§æ´²çš„è‹¥å¹²æ³•è§„ï¼ˆå¦‚GDPRï¼‰è¦æ±‚ç”¨æˆ·æä¾›å…³äºå¯¹å…¶ä½œå‡ºçš„è‡ªåŠ¨å†³ç­–çš„è§£é‡Šã€‚</p><p>  To use GPT-Neo or any Hugging Face model in your own application, you can  start a free trial of the ğŸ¤— Accelerated Inference API.If you need help mitigating bias in models and AI systems, or leveraging Few-Shot Learning, the ğŸ¤— Expert Acceleration Program can  offer your team direct premium support from the Hugging Face team.</p><p>  è¦åœ¨æ‚¨è‡ªå·±çš„åº”ç”¨ç¨‹åºä¸­ä½¿ç”¨GPT-neoæˆ–ä»»ä½•æ‹¥æŠ±é¢éƒ¨æ¨¡å‹ï¼Œæ‚¨å¯ä»¥å¼€å§‹è‡ªç”±è¯•éªŒçš„ğŸ¤—åŠ é€Ÿæ¨ç†API.IFæ‚¨éœ€è¦å¸®åŠ©ç¼“è§£æ¨¡å‹å’ŒAIç³»ç»Ÿçš„åè§ï¼Œæˆ–åˆ©ç”¨å°‘é‡å­¦ä¹ ï¼ŒğŸ¤—ä¸“å®¶åŠ é€Ÿè®¡åˆ’å¯ä»¥ä»æ‹¥æŠ±é¢éƒ¨å›¢é˜Ÿæä¾›æ‚¨çš„å›¢é˜Ÿç›´æ¥çš„ä¼˜è´¨æ”¯æŒã€‚ </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://huggingface.co/blog/few-shot-learning-gpt-neo-and-inference-api">https://huggingface.co/blog/few-shot-learning-gpt-neo-and-inference-api</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/å­¦ä¹ /">#å­¦ä¹ </a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/learning/">#learning</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/æ¨¡å‹/">#æ¨¡å‹</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è®¾è®¡/">#è®¾è®¡</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/åˆ›æ„/">#åˆ›æ„</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/æ‘„å½±/">#æ‘„å½±</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/æ¸¸æˆ/">#æ¸¸æˆ</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/å›¾ç‰‡/">#å›¾ç‰‡</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è½¯ä»¶/">#è½¯ä»¶</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è§†é¢‘/">#è§†é¢‘</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/æ‰‹æœº/">#æ‰‹æœº</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/å¹¿å‘Š/">#å¹¿å‘Š</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/ç½‘ç«™/">#ç½‘ç«™</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/å…è´¹/">#å…è´¹</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/ä¸‹è½½/">#ä¸‹è½½</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/å¾®è½¯/">#å¾®è½¯</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è‹¹æœ/">#è‹¹æœ</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/éŸ³ä¹/">#éŸ³ä¹</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/åšå®¢/">#åšå®¢</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/æ¶æ/">#æ¶æ</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è‰ºæœ¯/">#è‰ºæœ¯</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è°·æ­Œ/">#è°·æ­Œ</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/å·¥å…·/">#å·¥å…·</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>