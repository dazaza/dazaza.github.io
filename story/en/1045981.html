<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>å¾®å‹æ ‡æª Micro Benchmarking Dart</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Micro Benchmarking Dart<br/>å¾®å‹æ ‡æª </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-01-23 17:44:24</div><div class="page_narrow text-break page_content"><p>In the past few months I have started receiving more and more questions aboutperformance of some specific Dart operations. Here is an example of such aquestion asked by  Romain Rastel in the contextof his work on  improving performance of ChangeNotifierin Flutter.</p><p>åœ¨è¿‡å»çš„å‡ ä¸ªæœˆä¸­ï¼Œæˆ‘å¼€å§‹æ”¶åˆ°è¶Šæ¥è¶Šå¤šçš„æœ‰å…³æŸäº›ç‰¹å®šDartæ“ä½œæ€§èƒ½çš„é—®é¢˜ã€‚è¿™æ˜¯Romain Rastelåœ¨æé«˜Flutterçš„ChangeNotifieræ€§èƒ½æ–¹é¢æ‰€åšçš„å·¥ä½œä¸­æå‡ºçš„è¿™æ ·ä¸€ä¸ªé—®é¢˜çš„ç¤ºä¾‹ã€‚</p><p> Looks like creating a fixed-length list with a small number of items, can be, sometimes a lot less performant than creating a growable list.  pic.twitter.com/B5opjZkmrX</p><p> çœ‹èµ·æ¥åˆ›å»ºä¸€ä¸ªåŒ…å«å°‘é‡é¡¹ç›®çš„å®šé•¿åˆ—è¡¨å¯èƒ½æ¯”åˆ›å»ºå¯æ‰©å±•åˆ—è¡¨çš„æ€§èƒ½ä½å¾ˆå¤šã€‚ pic.twitter.com/B5opjZkmrX</p><p>â€” Romain Rastel ğŸ’™ (@lets4r)  November 30, 2020</p><p>-Romain RastelğŸ’™ï¼ˆ@ lets4rï¼‰2020å¹´11æœˆ30æ—¥</p><p> Given my experience I knew  exactly what was going wrong in this particularbenchmark after the very first glanceâ€¦ but for the sake of storytellinglet me pretend that I did not. How would I approach this then?</p><p> æœ‰äº†æˆ‘çš„ç»éªŒï¼Œæˆ‘ä¸€çœ¼å°±çŸ¥é“äº†è¿™ä¸ªåŸºå‡†æµ‹è¯•ä¸­åˆ°åº•å‡ºäº†ä»€ä¹ˆé—®é¢˜â€¦â€¦ä½†æ˜¯ä¸ºäº†è®²æ•…äº‹ï¼Œæˆ‘è£…ä½œæ²¡æœ‰ã€‚é‚£æˆ‘è¯¥å¦‚ä½•å¤„ç†å‘¢ï¼Ÿ</p><p> I would normally start by trying to repeat reported numbers. In this particularcase I would start by creating an empty Flutter application</p><p> æˆ‘é€šå¸¸ä¼šå…ˆå°è¯•é‡å¤æŠ¥å‘Šçš„æ•°å­—ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘å°†ä»åˆ›å»ºä¸€ä¸ªç©ºçš„Flutteråº”ç”¨ç¨‹åºå¼€å§‹</p><p>   // ubench/lib/benchmark.dart import  &#39;package:benchmark_harness/benchmark_harness.dart&#39; ; abstract  class  Benchmark  extends  BenchmarkBase  {  const  Benchmark ( String  name )  :  super ( name );  @override  void  exercise ()  {  for  ( int  i  =  0 ;  i  &lt;  100000 ;  i ++)  {  run ();  }  } } class  GrowableListBenchmark  extends  Benchmark  {  const  GrowableListBenchmark ( this . length )  :  super ( &#39;growable[ $length ]&#39; );  final  int  length ;  @override  void  run ()  {  List &lt; int &gt;().. length  =  length ;  } } class  FixedLengthListBenchmark  extends  Benchmark  {  const  FixedLengthListBenchmark ( this . length )  :  super ( &#39;fixed-length[ $length ]&#39; );  final  int  length ;  @override  void  run ()  {  List ( length );  } } void  main ( )  {  const  GrowableListBenchmark ( 32 ). report ();  const  FixedLengthListBenchmark ( 32 ). report (); }</p><p>   // ubench / lib / benchmark.dart importï¼†ï¼ƒ39; packageï¼šbenchmark_harness / benchmark_harness.dartï¼†ï¼ƒ39; ;æŠ½è±¡ç±»Benchmarkæ‰©å±•BenchmarkBase {const Benchmarkï¼ˆString nameï¼‰ï¼šsuperï¼ˆnameï¼‰; @override void Exerciseï¼ˆï¼‰{forï¼ˆint i = 0; iï¼†lt; 100000; i ++ï¼‰{runï¼ˆï¼‰; }}}ç±»GrowableListBenchmarkæ‰©å±•äº†Benchmark {const GrowableListBenchmarkï¼ˆthisã€‚lengthï¼‰ï¼šsuperï¼ˆï¼†ï¼ƒ39; growable [$ length]ï¼†ï¼ƒ39;ï¼‰;æœ€ç»ˆçš„inté•¿åº¦; @override void runï¼ˆï¼‰{åˆ—è¡¨ï¼†lt; intï¼†gt;ï¼ˆï¼‰.. length = length; }}ç±»FixedLengthListBenchmarkæ‰©å±•äº†Benchmark {const FixedLengthListBenchmarkï¼ˆthisã€‚lengthï¼‰ï¼šsuperï¼ˆï¼†ï¼ƒ39; fixed-length [$ length]ï¼†ï¼ƒ39;ï¼‰;æœ€ç»ˆçš„inté•¿åº¦; @override void runï¼ˆï¼‰{Listï¼ˆlengthï¼‰; }} void mainï¼ˆï¼‰{const GrowableListBenchmarkï¼ˆ32ï¼‰ã€‚æŠ¥å‘Šï¼ˆï¼‰; const FixedLengthListBenchmarkï¼ˆ32ï¼‰ã€‚æŠ¥å‘Šï¼ˆï¼‰; }</p><p>   The result seems to show fixed length lists being 43 times fasterto allocate than growable lists. Should we leave it at that and headover to refactor our code to use as many fixed-length lists as possible?</p><p>   ç»“æœä¼¼ä¹æ˜¾ç¤ºå›ºå®šé•¿åº¦åˆ—è¡¨çš„åˆ†é…é€Ÿåº¦æ¯”å¯å¢é•¿åˆ—è¡¨å¿«43å€ã€‚æˆ‘ä»¬æ˜¯å¦åº”è¯¥ä¿ç•™å®ƒï¼Œç„¶åé‡æ–°æ„å»ºä»£ç ä»¥ä½¿ç”¨å°½å¯èƒ½å¤šçš„å›ºå®šé•¿åº¦åˆ—è¡¨ï¼Ÿ </p><p> Absolutely notâ€¦ or at least not with an expectation that our code becomes43 times faster.  It does  actually make sense to prefer fixed-length lists over growable lists where fixed-length lists are a natural fit. They have slightly smaller memory footprint, are faster to allocate and involve less indirections to access an element. But you should do this choice deliberately based on clear understanding of how things work and not based on raw uninterpreted results of microbenchmarks.</p><p>ç»å¯¹ä¸ä¼šâ€¦â€¦æˆ–è€…è‡³å°‘ä¸ä¼šæœŸæœ›æˆ‘ä»¬çš„ä»£ç å¿«43å€ã€‚å®é™…ä¸Šï¼Œåœ¨å›ºå®šé•¿åº¦åˆ—è¡¨å¾ˆé€‚åˆçš„æƒ…å†µä¸‹ï¼Œä¼˜å…ˆé€‰æ‹©å›ºå®šé•¿åº¦åˆ—è¡¨è€Œä¸æ˜¯å¯å¢é•¿åˆ—è¡¨ã€‚å®ƒä»¬çš„å†…å­˜å ç”¨ç©ºé—´ç•¥å°ï¼Œåˆ†é…é€Ÿåº¦æ›´å¿«ï¼Œå¹¶ä¸”æ¶‰åŠè®¿é—®å…ƒç´ çš„é—´æ¥è°ƒç”¨æ›´å°‘ã€‚ä½†æ˜¯ï¼Œæ‚¨åº”è¯¥åŸºäºå¯¹äº‹ç‰©å·¥ä½œåŸç†çš„æ¸…æ™°äº†è§£è€Œä¸æ˜¯åŸºäºå¾®åŸºå‡†çš„æœªç»è§£é‡Šçš„åŸå§‹ç»“æœæ¥æ•…æ„è¿›è¡Œæ­¤é€‰æ‹©ã€‚</p><p> Drawing conclusions from raw microbenchmark numbers without anysort of critical analysis is a common pitfall associated withmicrobenchmarking and we should do our best to avoid falling into it.Unfortunately  package:benchmark_harness is not making it easier to avoid suchpitfalls: it provides developers with a way to write microbenchmarks but doesnot give them tools or guidance on how to validate your benchmarks andinterpret their results. To make things worse  package:benchmark_harness doesnot even attempt to make it very straightforward to write an accuratemicrobenchmark.</p><p> åœ¨æ²¡æœ‰è¿›è¡Œä»»ä½•ä¸¥æ ¼åˆ†æçš„æƒ…å†µä¸‹ä»åŸå§‹å¾®åŸºå‡†æ•°å­—å¾—å‡ºç»“è®ºæ˜¯ä¸å¾®åŸºå‡†ç›¸å…³çš„å¸¸è§é™·é˜±ï¼Œä½†æˆ‘ä»¬åº”å°½æœ€å¤§åŠªåŠ›é¿å…é™·å…¥è¿™ç§å›°å¢ƒã€‚packageï¼šbenchmark_harnesså¹¶ä¸èƒ½ä½¿å…¶æ›´å®¹æ˜“é¿å…æ­¤ç±»é™·é˜±ï¼šå®ƒä¸ºå¼€å‘äººå‘˜æä¾›äº†ä¸€ç§ç¼–å†™æ–¹æ³•å¾®åŸºå‡†æµ‹è¯•ï¼Œä½†æ²¡æœ‰ä¸ºå®ƒä»¬æä¾›æœ‰å…³å¦‚ä½•éªŒè¯åŸºå‡†å¹¶è§£é‡Šå…¶ç»“æœçš„å·¥å…·æˆ–æŒ‡å—ã€‚æ›´ç³Ÿç³•çš„æ˜¯ï¼Œpackageï¼šbenchmark_harnessç”šè‡³æ²¡æœ‰å°è¯•ä½¿ç¼–å†™ç²¾ç¡®çš„å¾®åŸºå‡†æµ‹è¯•éå¸¸ç®€å•ã€‚</p><p> Consider for example that I could have written this list benchmark in thefollowing way, without overriding  exercise to repeat  run  100000 times:</p><p> ä¾‹å¦‚ï¼Œè€ƒè™‘ä¸€ä¸‹æˆ‘å¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ–¹å¼ç¼–å†™æ­¤åˆ—è¡¨åŸºå‡†ï¼Œè€Œæ— éœ€è¿›è¡Œè¿‡å¤šç»ƒä¹ æ¥é‡å¤è¿è¡Œ100000æ¬¡ï¼š</p><p> // ubench/lib/benchmark-without-exercise.dart import  &#39;package:benchmark_harness/benchmark_harness.dart&#39; ; // Just using BenchmarkBase directly. Rest is the same. class  GrowableListBenchmark  extends  BenchmarkBase  {  // ... } // Just using BenchmarkBase directly. Rest is the same. class  FixedLengthListBenchmark  extends  BenchmarkBase  {  // ... }</p><p> // ubench / lib / benchmark-without-exercise.dart importï¼†ï¼ƒ39; packageï¼šbenchmark_harness / benchmark_harness.dartï¼†ï¼ƒ39; ; //ä»…ç›´æ¥ä½¿ç”¨BenchmarkBaseã€‚ä¼‘æ¯æ˜¯ä¸€æ ·çš„ã€‚ class GrowableListBenchmarkæ‰©å±•äº†BenchmarkBase {// ...} //ä»…ç›´æ¥ä½¿ç”¨BenchmarkBaseã€‚ä¼‘æ¯æ˜¯ä¸€æ ·çš„ã€‚ç±»FixedLengthListBenchmarkæ‰©å±•BenchmarkBase {// ...}</p><p> Running this variant would show that growable lists are only 6 times slowerthan fixed length lists</p><p> è¿è¡Œæ­¤å˜ä½“å°†æ˜¾ç¤ºå¯å¢é•¿åˆ—è¡¨ä»…æ¯”å›ºå®šé•¿åº¦åˆ—è¡¨æ…¢6å€</p><p>  Which benchmark result should I trust?  Neither of them really! I should lookunder the hood and try to understand what exactly is happening.</p><p>  æˆ‘åº”è¯¥ç›¸ä¿¡å“ªä¸ªåŸºå‡†æµ‹è¯•ç»“æœï¼Ÿä»–ä»¬ä¿©éƒ½ä¸æ˜¯ï¼æˆ‘åº”è¯¥ä»”ç»†ç ”ç©¶ä¸€ä¸‹ï¼Œå¹¶è¯•å›¾äº†è§£æ­£åœ¨å‘ç”Ÿçš„äº‹æƒ…ã€‚</p><p> Flutter and Dart already provide developers with enough tooling to figure outwhy benchmark numbers look this way. Unfortunately some of this tooling issomewhat obscure and hard to discover.</p><p> Flutterå’ŒDartå·²ç»ä¸ºå¼€å‘äººå‘˜æä¾›äº†è¶³å¤Ÿçš„å·¥å…·ï¼Œä»¥å¼„æ¸…åŸºå‡†æ•°å­—ä¸ºä½•å¦‚æ­¤ã€‚ä¸å¹¸çš„æ˜¯ï¼Œå…¶ä¸­ä¸€äº›å·¥å…·æœ‰äº›æ™¦æ¶©éš¾æ‡‚ï¼Œå¾ˆéš¾å‘ç°ã€‚ </p><p> For example, it is well known that you can use  flutter run --profile toprofile your application with Observatory, however it is not well known thatyou can also profile release builds using native profilers (like  simpleperfon Android or Instruments on iOS). Similarly it is not known (most likely notknown at all outside of a group of engineers working on the VM) that you candump annotated disassembly of a specific method from an AOT build by doing</p><p>ä¾‹å¦‚ï¼Œä¼—æ‰€å‘¨çŸ¥ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨flutter run --profileé€šè¿‡Observatoryå¯¹åº”ç”¨ç¨‹åºè¿›è¡Œæ¦‚è¦åˆ†æï¼Œä½†æ˜¯ï¼Œæ‚¨è¿˜å¯ä»¥ä½¿ç”¨æœ¬æœºæ¦‚è¦åˆ†æå™¨ï¼ˆä¾‹å¦‚simpleperfon Androidæˆ–iOSä¸Šçš„Instrumentsï¼‰å¯¹å‘è¡Œç‰ˆæœ¬è¿›è¡Œæ¦‚è¦åˆ†æã€‚ç±»ä¼¼åœ°ï¼Œæ‚¨æ— æ³•ï¼ˆé€šè¿‡åœ¨VMä¸Šå·¥ä½œçš„ä¸€ç»„å·¥ç¨‹å¸ˆä»¥å¤–çš„äººå®Œå…¨ä¸çŸ¥é“ï¼‰å¯ä»¥é€šè¿‡ä»¥ä¸‹æ“ä½œä»AOTæ„å»ºä¸­è½¬å‚¨å¸¦æ³¨é‡Šçš„ç‰¹å®šæ–¹æ³•çš„åæ±‡ç¼–ï¼š</p><p>  I could spend the rest of this post explaining how one could use these toolsto understand what exactly is going on in these list benchmarks, but insteadI would like to try and imagine how an integrated tooling for benchmarkingcould be built out of the primitives provided by Dart and Flutter. Thistooling should not only run benchmarks, but also automatically provideenough insight for a developer to spot mistakes they made during benchmarkingand help them interpret the results.</p><p>  æˆ‘å¯ä»¥ç”¨è¿™ç¯‡æ–‡ç« çš„å…¶ä½™éƒ¨åˆ†è§£é‡Šå¦‚ä½•ä½¿ç”¨è¿™äº›å·¥å…·æ¥ç†è§£è¿™äº›åˆ—è¡¨åŸºå‡†æµ‹è¯•ä¸­åˆ°åº•å‘ç”Ÿäº†ä»€ä¹ˆï¼Œä½†æ˜¯æˆ‘æƒ³å°è¯•æƒ³è±¡ä¸€ä¸‹å¦‚ä½•ä»Dartå’ŒDartæä¾›çš„åŸè¯­ä¸­æ„å»ºç”¨äºåŸºå‡†æµ‹è¯•çš„é›†æˆå·¥å…·ã€‚æ‰‘ã€‚è¯¥å·¥å…·ä¸ä»…åº”è¿è¡ŒåŸºå‡†æµ‹è¯•ï¼Œè¿˜åº”ä¸ºå¼€å‘äººå‘˜è‡ªåŠ¨æä¾›è¶³å¤Ÿçš„æ´å¯ŸåŠ›ï¼Œä»¥å‘ç°ä»–ä»¬åœ¨åŸºå‡†æµ‹è¯•è¿‡ç¨‹ä¸­çŠ¯çš„é”™è¯¯å¹¶å¸®åŠ©ä»–ä»¬è§£é‡Šç»“æœã€‚</p><p>  I have forked  benchmark_harness package into   mraleph/benchmark_harness on GitHub. All of my  prototype code is going to live in a new   experimental-cli branch in the fork.</p><p>  æˆ‘å·²å°†Benchmark_harnessåŒ…åˆ†å‰åˆ°GitHubä¸Šçš„mraleph / benchmark_harnessä¸­ã€‚æˆ‘æ‰€æœ‰çš„åŸå‹ä»£ç éƒ½å°†é©»ç•™åœ¨forkä¸­çš„ä¸€ä¸ªæ–°çš„experimental-cliåˆ†æ”¯ä¸­ã€‚</p><p> From here on I will document an evolution of this experimental benchmarking CLI. I would like to stress a highly experimental nature of this tooling: as you will notice that some of its features will end up depending on a patches to Dart and Flutter SDK internals. It might be weeks or months before these patches land and it will become possible to just merge my changes into upstream version of the harness.</p><p> ä»è¿™é‡Œå¼€å§‹ï¼Œæˆ‘å°†è®°å½•æ­¤å®éªŒæ€§åŸºå‡†æµ‹è¯•CLIçš„æ¼”å˜ã€‚æˆ‘æƒ³å¼ºè°ƒä¸€ä¸‹è¯¥å·¥å…·çš„é«˜åº¦å®éªŒæ€§è´¨ï¼šæ‚¨ä¼šæ³¨æ„åˆ°ï¼Œè¯¥å·¥å…·çš„æŸäº›åŠŸèƒ½æœ€ç»ˆå°†å–å†³äºDartå’ŒFlutter SDKå†…éƒ¨çš„è¡¥ä¸ã€‚è¿™äº›è¡¥ä¸å¯èƒ½è¦èŠ±å‡ å‘¨æˆ–å‡ ä¸ªæœˆçš„æ—¶é—´ï¼Œæ‰æœ‰å¯èƒ½å°†æˆ‘çš„æ›´æ”¹åˆå¹¶åˆ°çº¿æŸçš„ä¸Šæ¸¸ç‰ˆæœ¬ä¸­ã€‚</p><p> I started by adding a trivial  bin/benchmark_harness.dart script which wouldserve as an entry point to our new benchmarking tooling.</p><p> æˆ‘é¦–å…ˆæ·»åŠ äº†ä¸€ä¸ªç®€å•çš„bin / benchmark_harness.dartè„šæœ¬ï¼Œè¯¥è„šæœ¬å°†ä½œä¸ºæˆ‘ä»¬æ–°çš„åŸºå‡†æµ‹è¯•å·¥å…·çš„åˆ‡å…¥ç‚¹ã€‚</p><p> $  git clone  [emailÂ protected]:mraleph/benchmark_harness.git $   cd benchmark_harness $   cat  &gt; bin/benchmark_harness.dart void main() { print(&#39;Running benchmarks...&#39;);   } ^D</p><p> $ git clone [å—ç”µå­é‚®ä»¶ä¿æŠ¤]ï¼šmraleph / benchmark_harness.git $ cd Benchmark_harness $ catï¼†gt; bin / benchmark_harness.dart void mainï¼ˆï¼‰{printï¼ˆï¼†ï¼ƒ39;æ­£åœ¨è¿è¡ŒåŸºå‡†æµ‹è¯•...ï¼†ï¼ƒ39;ï¼‰; } ^ D</p><p> Finally I changed  pubspec.yaml in  ubench project (remember it is an emptyFlutter project we created to host our benchmarks) to have a path dependencyon my version of  benchmark_harness</p><p> æœ€åï¼Œæˆ‘åœ¨ubenché¡¹ç›®ä¸­æ›´æ”¹äº†pubspec.yamlï¼ˆè¯·è®°ä½ï¼Œè¿™æ˜¯æˆ‘ä»¬åˆ›å»ºçš„ç”¨äºæ‰˜ç®¡åŸºå‡†æµ‹è¯•çš„emptyFlutteré¡¹ç›®ï¼‰ï¼Œä»¥å¯¹æˆ‘çš„beta_harnessç‰ˆæœ¬å…·æœ‰è·¯å¾„ä¾èµ–æ€§ </p><p>      It turns out that this package is doing something rather simple (and to anextent naive): it starts a  Stopwatch, then repeatedly calls  exerciseuntil 2 seconds elapses according to the stopwatch. Time elapsed divided bynumber of times  exercise was called is the reported benchmark score. Takea look yourself:</p><p>äº‹å®è¯æ˜ï¼Œæ­¤ç¨‹åºåŒ…çš„æ“ä½œç›¸å½“ç®€å•ï¼ˆå¤©çœŸåœ°å¤ªå¤©çœŸäº†ï¼‰ï¼šå®ƒå¯åŠ¨äº†ä¸€ä¸ªç§’è¡¨ï¼Œç„¶åé‡å¤è°ƒç”¨exerciseï¼Œç›´åˆ°æ ¹æ®è¯¥ç§’è¡¨ç»è¿‡2ç§’ä¸ºæ­¢ã€‚æŠ¥å‘Šçš„åŸºå‡†åˆ†æ•°æ˜¯ç»è¿‡çš„æ—¶é—´é™¤ä»¥è¢«ç§°ä¸ºè¿åŠ¨çš„æ¬¡æ•°ã€‚ Takeaçœ‹çœ‹è‡ªå·±ï¼š</p><p> // benchmark_harness/lib/src/benchmark_base.dart abstract  class  BenchmarkBase  {  // Measures the score for the benchmark and returns it.  double  measure ()  {  // ...  // Run the benchmark for at least 2000ms.  var  result  =  measureFor ( exercise ,  2000 );  // ...  }  // Exercises the benchmark. By default invokes [run] 10 times.  void  exercise ()  {  for  ( var  i  =  0 ;  i  &lt;  10 ;  i ++)  {  run ();  }  }  // Measures the score for this benchmark by executing it repeatedly until  // time minimum has been reached.  static  double  measureFor ( Function  f ,  int  minimumMillis )  {  var  minimumMicros  =  minimumMillis  *  1000 ;  var  iter  =  0 ;  var  watch  =  Stopwatch ();  watch . start ();  var  elapsed  =  0 ;  while  ( elapsed  &lt;  minimumMicros )  {  f ();  elapsed  =  watch . elapsedMicroseconds ;  iter ++;  }  return  elapsed  /  iter ;  } }</p><p> // Benchmark_harness / lib / src / benchmark_base.dartæŠ½è±¡ç±»BenchmarkBase {//æµ‹é‡åŸºå‡†å¾—åˆ†å¹¶è¿”å›ã€‚ double measureï¼ˆï¼‰{// ... //è¿è¡ŒåŸºå‡†æµ‹è¯•è‡³å°‘2000msã€‚ varç»“æœ= measureForï¼ˆç»ƒä¹ ï¼Œ2000ï¼‰; // ...} //æ‰§è¡ŒåŸºå‡†æµ‹è¯•ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œè°ƒç”¨[run] 10æ¬¡ã€‚æ— æ•ˆè¿åŠ¨ï¼ˆï¼‰{forï¼ˆvar i = 0; iï¼†lt; 10; i ++ï¼‰{runï¼ˆï¼‰; }} //é€šè¿‡é‡å¤æ‰§è¡Œè¯¥åŸºå‡†æµ‹è¯•åˆ†æ•°ï¼Œç›´åˆ°//è¾¾åˆ°æœ€å°æ—¶é—´ã€‚é™æ€double measureForï¼ˆå‡½æ•°fï¼Œint minimumMillisï¼‰{var minimumMicros = minimumMillis * 1000; var iter = 0; var watch =ç§’è¡¨ï¼ˆï¼‰;çœ‹ã€‚å¼€å§‹ï¼ˆï¼‰; var elapsed = 0;è€Œï¼ˆè¿‡å»çš„ï¼†lt; minimumMicrosï¼‰{fï¼ˆï¼‰;è¿‡å»=çœ‹ã€‚ç»è¿‡çš„å¾®ç§’; iter ++;è¿”å›ç»è¿‡/è¿­ä»£; }}</p><p> This code unfortunately has an issue which makes it unsuitable formicrobenchmarking: measured loop has a bunch of overhead unrelated to the exercise itself. Most noticeably it gets current time from the OS oneach and every iteration. There is also an overhead associated withmultiple levels of virtual dispatch between measured loop and the body of run method containing an actual operation we want to measure.  There was a  PR against  benchmark_harness, which tried to address the issue of calling  Stopwatch.elapsedMilliseconds too often, but it somehow got stuck in limbo despite being approved.</p><p> ä¸å¹¸çš„æ˜¯ï¼Œè¿™æ®µä»£ç æœ‰ä¸€ä¸ªé—®é¢˜ï¼Œä½¿å…¶ä¸é€‚åˆè¿›è¡Œå¾®åŸºå‡†æµ‹è¯•ï¼šè¢«æµ‹å¾ªç¯å…·æœ‰å¤§é‡ä¸ç»ƒä¹ æœ¬èº«æ— å…³çš„å¼€é”€ã€‚æœ€æ˜æ˜¾çš„æ˜¯ï¼Œå®ƒä»æ¯ä¸ªæ“ä½œç³»ç»Ÿä»¥åŠæ¯æ¬¡è¿­ä»£è·å–å½“å‰æ—¶é—´ã€‚åœ¨æ‰€æµ‹é‡çš„å¾ªç¯å’ŒåŒ…å«è¦æµ‹é‡çš„å®é™…æ“ä½œçš„runæ–¹æ³•ä¸»ä½“ä¹‹é—´ï¼Œè¿˜å­˜åœ¨ä¸å¤šçº§è™šæ‹Ÿè°ƒåº¦ç›¸å…³çš„å¼€é”€ã€‚æœ‰ä¸€ä¸ªé’ˆå¯¹Benchmark_harnessçš„PRï¼Œè¯¥PRè¯•å›¾è§£å†³è¿‡äºé¢‘ç¹åœ°è°ƒç”¨Stopwatch.elapsedMillisecondsçš„é—®é¢˜ï¼Œä½†æ˜¯å°½ç®¡è·å¾—äº†æ‰¹å‡†ï¼Œå®ƒè¿˜æ˜¯é™·å…¥äº†å›°å¢ƒã€‚</p><p> The best way to avoid these overheads it to have a separate measured loop for each benchmark.</p><p> é¿å…è¿™äº›å¼€é”€çš„æœ€ä½³æ–¹æ³•æ˜¯ä¸ºæ¯ä¸ªåŸºå‡†è®¾ç½®ä¸€ä¸ªå•ç‹¬çš„æµ‹é‡ç¯è·¯ã€‚</p><p> Here is how this could look like. User declares microbenchmarks by writing a top-level function marked with  @benchmark annotation.</p><p> è¿™å°±æ˜¯å®ƒçš„æ ·å­ã€‚ç”¨æˆ·é€šè¿‡ç¼–å†™æ ‡æœ‰@benchmarkæ‰¹æ³¨çš„é¡¶çº§å‡½æ•°æ¥å£°æ˜å¾®åŸºå‡†ã€‚</p><p> // ubench/lib/main.dart import  &#39;package:benchmark_harness/benchmark_harness.dart&#39; ; const  N  =  32 ; @benchmark void  allocateFixedArray ( )  {  List . filled ( N ,  null ,  growable:  false ); } @benchmark void  allocateGrowableArray ( )  {  List . filled ( N ,  null ,  growable:  true ); }</p><p> // ubench / lib / main.dart importï¼†ï¼ƒ39; packageï¼šbenchmark_harness / benchmark_harness.dartï¼†ï¼ƒ39; ;å¸¸é‡N = 32; @benchmark voidallocateFixedArrayï¼ˆï¼‰{åˆ—è¡¨ã€‚å¡«å……ï¼ˆNï¼Œnullï¼Œgrowableï¼šfalseï¼‰; } @benchmark voidallocateGrowableArrayï¼ˆï¼‰{åˆ—è¡¨ã€‚å¡«å……ï¼ˆNï¼Œnullï¼Œgrowableï¼štrueï¼‰; }</p><p> Benchmarking tooling would then generate an auxiliary source file which contains a measured loop for each benchmark, plus some code to select which benchmarks should run at compile time:</p><p> ç„¶åï¼ŒåŸºå‡†æµ‹è¯•å·¥å…·å°†ç”Ÿæˆä¸€ä¸ªè¾…åŠ©æºæ–‡ä»¶ï¼Œå…¶ä¸­åŒ…å«æ¯ä¸ªåŸºå‡†çš„ä¸€ä¸ªæµ‹é‡å¾ªç¯ï¼Œä»¥åŠä¸€äº›ä»£ç æ¥é€‰æ‹©åœ¨ç¼–è¯‘æ—¶åº”è¿è¡Œçš„åŸºå‡†ï¼š </p><p> // ubench/lib/main.benchmark.dart import  &#39;package:benchmark_harness/benchmark_harness.dart&#39;  as  benchmark_harness ; import  &#39;package:ubench/main.dart&#39;  as  lib ; // ... void  _$measuredLoop$allocateFixedArray ( int  numIterations )  {  while  ( numIterations --  &gt;  0 )  {  lib . allocateFixedArray ();  } } // ... const  _targetBenchmark  =  String . fromEnvironment ( &#39;targetBenchmark&#39; ,  defaultValue:  &#39;all&#39; ); const  _shouldMeasureAll  =  _targetBenchmark  ==  &#39;all&#39; ; const  _shouldMeasure$allocateFixedArray  =  _shouldMeasureAll  ||  _targetBenchmark  ==  &#39;allocateFixedArray&#39; ; // ... void  main ( )  {  benchmark_runner . runBenchmarks ( const  {  // ...  if  ( _shouldMeasure$allocateFixedArray )  &#39;allocateFixedArray&#39; :  _$measuredLoop$allocateFixedArray ,  // ...  }); }</p><p>// ubench / lib / main.benchmark.dart importï¼†ï¼ƒ39; packageï¼šbenchmark_harness / benchmark_harness.dartï¼†ï¼ƒ39;ä½œä¸ºåŸºå‡†çº¿æŸ;å¯¼å…¥packageï¼šubench / main.dartï¼†ï¼ƒ39;ä½œä¸ºlib; // ... void _ $ measuredLoop $ allocateFixedArrayï¼ˆint numIterationsï¼‰{whileï¼ˆnumIterations-ï¼†gt; 0ï¼‰{libã€‚ allocateFixedArrayï¼ˆï¼‰; }} // ... const _targetBenchmark = Stringã€‚ fromEnvironmentï¼ˆï¼†ï¼ƒ39; targetBenchmarkï¼†ï¼ƒ39;ï¼Œé»˜è®¤å€¼ï¼šï¼†ï¼ƒ39; allï¼†ï¼ƒ39;ï¼‰; const _shouldMeasureAll = _targetBenchmark ==ï¼†ï¼ƒ39; allï¼†ï¼ƒ39; ; const _shouldMeasure $ allocateFixedArray = _shouldMeasureAll || _targetBenchmark ==ï¼†ï¼ƒ39; allocateFixedArrayï¼†ï¼ƒ39; ; // ... void mainï¼ˆï¼‰{åŸºå‡†æµ‹è¯•è¿è¡Œå™¨ã€‚ runBenchmarksï¼ˆconst {// ... ifï¼ˆ_shouldMeasure $ allocateFixedArrayï¼‰ï¼†ï¼ƒ39; allocateFixedArrayï¼†ï¼ƒ39;ï¼š_ $ measuredLoop $ allocateFixedArrayï¼Œ// ...}ï¼‰; }</p><p>  // benchmark_harness/lib/benchmark_runner.dart /// Runs the given measured [loop] function with an exponentially increasing /// parameter values until it finds one that causes [loop] to run for at /// least [thresholdMilliseconds] and returns [BenchmarkResult] describing /// that run. BenchmarkResult  measure ( void  Function ( int )  loop ,  { required  String  name ,  int  thresholdMilliseconds  =  5000 })  {  var  n  =  2 ;  final  sw  =  Stopwatch ();  do  {  n  *=  2 ;  sw . reset ();  sw . start ();  loop ( n );  sw . stop ();  }  while  ( sw . elapsedMilliseconds  &lt;  thresholdMilliseconds );  return  BenchmarkResult (  name:  name ,  elapsedMilliseconds:  sw . elapsedMilliseconds ,  numIterations:  n ,  ); }</p><p>  // Benchmark_harness / lib / benchmark_runner.dart ///ä»¥ç»™å®šçš„æµ‹é‡å€¼[loop]å‡½æ•°ä»¥æŒ‡æ•°æ–¹å¼å¢åŠ ///ï¼Œç›´åˆ°æ‰¾åˆ°ä¸€ä¸ªå¯¼è‡´[loop]è¿è¡Œ///è‡³å°‘[thresholdMilliseconds]å’Œè¿”å›æè¿°è¿è¡Œçš„[/ BenchmarkResult]ã€‚ BenchmarkResultåº¦é‡ï¼ˆvoid Functionï¼ˆintï¼‰å¾ªç¯ï¼Œ{å¿…éœ€çš„å­—ç¬¦ä¸²åç§°ï¼Œint thresholdMilliseconds = 5000}ï¼‰{var n = 2; final sw =ç§’è¡¨ï¼ˆï¼‰;åš{n * = 2; swé‡å¯ ï¼ˆï¼‰; swå¼€å§‹ï¼ˆï¼‰;å¾ªç¯ï¼ˆnï¼‰; swåœ ï¼ˆï¼‰; } whileï¼ˆswã€‚elapsedMillisecondsï¼†lt; thresholdMillisecondsï¼‰;è¿”å›BenchmarkResultï¼ˆåç§°ï¼šnameï¼ŒelapsedMillisecondsï¼šswã€‚elapsedMillisecondsï¼ŒnumIterationsï¼šnï¼Œï¼‰ï¼› }</p><p> We are starting with a very simple implementation, which should neverthelesssatisfy our initial microbenchmarking needs. However for more complex caseswe might want to do something a bit more rigorous: for example once large enough numIterations is found we can repeat  loop(numIterations) multiple timesand asses statistical properties of observed running times.</p><p> æˆ‘ä»¬ä»ä¸€ä¸ªéå¸¸ç®€å•çš„å®ç°å¼€å§‹ï¼Œå°½ç®¡è¿™æ ·åº”è¯¥å¯ä»¥æ»¡è¶³æˆ‘ä»¬æœ€åˆçš„å¾®åŸºå‡†æµ‹è¯•éœ€æ±‚ã€‚ä½†æ˜¯å¯¹äºæ›´å¤æ‚çš„æƒ…å†µï¼Œæˆ‘ä»¬å¯èƒ½éœ€è¦åšä¸€äº›æ›´ä¸¥æ ¼çš„æ“ä½œï¼šä¾‹å¦‚ï¼Œä¸€æ—¦æ‰¾åˆ°è¶³å¤Ÿå¤§çš„numIterationsï¼Œæˆ‘ä»¬å¯ä»¥é‡å¤æ‰§è¡Œloopï¼ˆnumIterationsï¼‰å¤šæ¬¡å¹¶è¯„ä¼°è§‚å¯Ÿåˆ°çš„è¿è¡Œæ—¶é—´çš„ç»Ÿè®¡å±æ€§ã€‚</p><p>  To generate  main.benchmark.dart we need to parse  main.dart and find allfunctions annotated with  @benchmark annotation. Fortunately Dart has anumber of canonical tools for code generation which make this really easy.</p><p>  è¦ç”Ÿæˆmain.benchmark.dartï¼Œæˆ‘ä»¬éœ€è¦è§£æmain.dartå¹¶æ‰¾åˆ°æ‰€æœ‰å¸¦æœ‰@benchmarkæ³¨é‡Šçš„å‡½æ•°ã€‚å¹¸è¿çš„æ˜¯ï¼ŒDartæœ‰è®¸å¤šç”¨äºä»£ç ç”Ÿæˆçš„è§„èŒƒå·¥å…·ï¼Œè¿™ä½¿è¿™å˜å¾—éå¸¸å®¹æ˜“ã€‚</p><p> All I had to do is to depend on  package:source_gen and to define a subclass of   GeneratorForAnnotation:</p><p> æˆ‘è¦åšçš„å°±æ˜¯ä¾é packageï¼šsource_genå¹¶å®šä¹‰GeneratorForAnnotationçš„å­ç±»ï¼š</p><p> // benchmark_harness/lib/src/benchmark_generator.dart class  BenchmarkGenerator  extends  GeneratorForAnnotation &lt; Benchmark &gt;  {  // ...  @override  String  generateForAnnotatedElement (  Element  element ,  ConstantReader  annotation ,  BuildStep  buildStep )  {  final  name  =  element . name ;  return  &#39;&#39;&#39;void  ${_\$measuredLoop\$$name} (int numIterations) { while (numIterations-- &gt; 0) { lib. ${name} (); }}&#39;&#39;&#39; ;  } }</p><p> // Benchmark_harness / lib / src / benchmark_generator.dartç±»BenchmarkGeneratoræ‰©å±•GeneratorForAnnotationï¼†lt;åŸºå‡†ï¼†gt; {// ... @overrideå­—ç¬¦ä¸²generateForAnnotatedElementï¼ˆå…ƒç´ å…ƒç´ ï¼ŒConstantReaderæ³¨é‡Šï¼ŒBuildStep buildStepï¼‰{æœ€ç»ˆåç§°=å…ƒç´ ã€‚åç§° ;è¿”å›ï¼†ï¼ƒ39;ï¼†ï¼ƒ39;ï¼†void $ {_ \ $ measuredLoop \ $$ name}ï¼ˆint numIterationsï¼‰{whileï¼ˆnumIterations-ï¼†gt; 0ï¼‰{libã€‚ $ {name}ï¼ˆï¼‰; }}ï¼†ï¼ƒ39;ï¼†ï¼ƒ39;ï¼†ï¼ƒ39; ; }}</p><p>     That was basically it. Now whenever I run  build_runner build in  ubench I willget  lib/main.benchmark.dart generated for benchmarks defined in  lib/main.dart:</p><p>     åŸºæœ¬ä¸Šå°±æ˜¯è¿™æ ·ã€‚ç°åœ¨ï¼Œæ¯å½“æˆ‘åœ¨ubenchä¸­è¿è¡Œbuild_runner buildæ—¶ï¼Œæˆ‘éƒ½ä¼šä¸ºlib / main.dartä¸­å®šä¹‰çš„åŸºå‡†ç”Ÿæˆlib / main.benchmark.dartï¼š </p><p>     $  flutter run  --release  --dart-define  targetBenchmark =allocateFixedArray  -t lib/main.benchmark.dart Launching lib/main.benchmark.dart on Pixel 3a in release mode...Running Gradle task &#39;assembleRelease&#39;...Running Gradle task &#39;assembleRelease&#39;... Done 4.9sâœ“ Built build/app/outputs/flutter-apk/app-release.apk (4.9MB).Installing build/app/outputs/flutter-apk/app.apk... 1,268msFlutter run key commands.h Repeat this help message.c Clear the screenq Quit (terminate the application on the device).I/flutter (12463): benchmark_harness[{&#34;event&#34;:&#34;benchmark.running&#34;}]I/flutter (12463): benchmark_harness[{&#34;event&#34;:&#34;benchmark.result&#34;,&#34;params&#34;:{...}}]I/flutter (12463): benchmark_harness[{&#34;event&#34;:&#34;benchmark.done&#34;}]Application finished.</p><p>$ flutter run --release --dart-define targetBenchmark = allocateFixedArray -t lib / main.benchmark.dartåœ¨é‡Šæ”¾æ¨¡å¼ä¸‹åœ¨Pixel 3aä¸Šå¯åŠ¨lib / main.benchmark.dart ...æ­£åœ¨è¿è¡ŒGradleä»»åŠ¡ï¼†ï¼ƒ39; assembleReleaseï¼†ï¼ƒ39 ; ...æ­£åœ¨è¿è¡ŒGradleä»»åŠ¡ï¼†ï¼ƒ39; assembleReleaseï¼†ï¼ƒ39; ...å®Œæˆ4.9ç§’âœ“æ„å»ºäº†build / app / outputs / flutter-apk / app-release.apkï¼ˆ4.9MBï¼‰ã€‚å®‰è£…build / app / outputs / flutter-apk / app.apk ... 1,268msé¢¤æŒ¯è¿è¡Œé”®å‘½ä»¤ã€‚hé‡å¤æ­¤å¸®åŠ©æ¶ˆæ¯ã€‚cæ¸…é™¤screenqé€€å‡ºï¼ˆç»ˆæ­¢è®¾å¤‡ä¸Šçš„åº”ç”¨ç¨‹åºï¼‰ã€‚I / flutterï¼ˆ12463ï¼‰ï¼šBenchmark_harness [{ï¼†ï¼ƒ34; eventï¼†ï¼ƒ34;ï¼šï¼†ï¼ƒ34; bunningmark.runningï¼†ï¼ƒ34;}] I / flutterï¼ˆ12463ï¼‰ï¼šbenchmark_harness [{ï¼†ï¼ƒ34; eventï¼†ï¼ƒ34;ï¼šï¼†ï¼ƒ34; benchmark.resultï¼†ï¼ƒ34;ï¼Œï¼†ï¼ƒ34 ; paramsï¼†ï¼ƒ34;ï¼š{...}}] I / flutterï¼ˆ12463ï¼‰ï¼šBenchmark_harness [{ï¼†ï¼ƒ34; eventï¼†ï¼ƒ34;ï¼šï¼†ï¼ƒ34; benchmark.doneï¼†ï¼ƒ34;}]åº”ç”¨ç¨‹åºå®Œæˆã€‚</p><p> But doing this manually is not exactly what I was aiming for. Instead I amgoing to change  bin/benchmark_harness.dart script to both build benchmarksand then to run all generated files to collect benchmark results (for full code see  this commit).</p><p> ä½†æ˜¯æ‰‹åŠ¨æ‰§è¡Œæ­¤æ“ä½œå¹¶ä¸æ˜¯æˆ‘çš„ç›®æ ‡ã€‚ç›¸åï¼Œæˆ‘æ‰“ç®—å°†bin / benchmark_harness.dartè„šæœ¬æ›´æ”¹ä¸ºæ—¢æ„å»ºåŸºå‡†ï¼Œåˆè¿è¡Œæ‰€æœ‰ç”Ÿæˆçš„æ–‡ä»¶ä»¥æ”¶é›†åŸºå‡†ç»“æœï¼ˆæœ‰å…³å®Œæ•´ä»£ç ï¼Œè¯·å‚è§æ­¤æäº¤ï¼‰ã€‚</p><p> // benchmark_harness/bin/benchmark_harness.dart void  main ( )  async  {  // ...  // Generate benchmark wrapper scripts.  print ( red ( &#39;Generating benchmark wrappers&#39; ));  &#39;flutter pub run build_runner build&#39; . start ( progress:  Progress . devNull ());  // Run all generated benchmarks.  final  resultsByFile  =  &lt; String ,  Map &lt; String ,  BenchmarkResult &gt;&gt;{};  for  ( var  file  in  find ( &#39;*.benchmark.dart&#39; ). toList (). map ( p . relative ))  {  resultsByFile [ file ]  =  await  runBenchmarksIn ( file );  }  // Report results.  // ... } /// Runs all benchmarks in `.benchmark.dart` [file] one by one and collects /// their results. Future &lt; Map &lt; String ,  BenchmarkResult &gt;&gt;  runBenchmarksIn ( String  file )  async  {  // ... }</p><p> // // Benchmark_harness / bin / benchmark_harness.dart void mainï¼ˆï¼‰async {// ... //ç”ŸæˆåŸºå‡†åŒ…è£…å™¨è„šæœ¬ã€‚æ‰“å°ï¼ˆçº¢è‰²ï¼ˆï¼†ï¼ƒ39;ç”ŸæˆåŸºå‡†åŒ…è£…å™¨ï¼†ï¼ƒ39;ï¼‰ï¼‰; ï¼†ï¼ƒ39; flutter pub run build_runner buildï¼†ï¼ƒ39; ã€‚ startï¼ˆprogressï¼šProgressã€‚devNullï¼ˆï¼‰ï¼‰; //è¿è¡Œæ‰€æœ‰ç”Ÿæˆçš„åŸºå‡†ã€‚æœ€ç»ˆç»“æœByFile =ï¼†lt;å­—ç¬¦ä¸²ï¼Œæ˜ å°„ï¼†lt; Stringï¼ŒBenchmarkResultï¼†gt;ï¼†gt; {}; forï¼ˆvaræ–‡ä»¶in findï¼ˆï¼†ï¼ƒ39; *ã€‚benchmark.dartï¼†ï¼ƒ39;ï¼‰ã€‚toListï¼ˆï¼‰ã€‚mapï¼ˆpã€‚relativeï¼‰ï¼‰{resultsByFile [file] =ç­‰å¾…runBenchmarksInï¼ˆfileï¼‰; } //æŠ¥å‘Šç»“æœã€‚ // ...} ///é€ä¸€è¿è¡Œ`.benchmark.dart` [file]ä¸­çš„æ‰€æœ‰åŸºå‡†ï¼Œå¹¶æ”¶é›†///çš„ç»“æœã€‚æœªæ¥ï¼†lt;åœ°å›¾ï¼†lt; Stringï¼ŒBenchmarkResultï¼†gt;ï¼†gt; runBenchmarksInï¼ˆå­—ç¬¦ä¸²æ–‡ä»¶ï¼‰å¼‚æ­¥{// ...</p><p>  $  flutter pub run benchmark_harness Generating benchmark wrappersFound 2 benchmarks in lib/main.benchmark.dart measuring allocateFixedArray benchmark is running done measuring allocateGrowableArray benchmark is running done--------------------------------------------------------------------------------Results for lib/main.benchmark.dartallocateFixedArray: 0.0000030226074159145355 ms/iteration (fastest)allocateGrowableArray: 0.00018900632858276367 ms/iteration (62.5 times as slow)</p><p>  $ flutter pub runåŸºå‡†æµ‹è¯•_harnessç”ŸæˆåŸºå‡†æµ‹è¯•åŒ…è£…å™¨åœ¨lib / main.benchmark.dartä¸­æ‰¾åˆ°2ä¸ªåŸºå‡†æµ‹è¯•ï¼ŒallocateFixedArrayåŸºå‡†æµ‹è¯•å·²å®Œæˆï¼Œæµ‹é‡allocateGrowableArrayåŸºå‡†æµ‹è¯•å·²å®Œæˆï¼Œ-------------------- -------------------------------------------------- ---------- lib / main.benchmark.dartallocateFixedArrayçš„ç»“æœï¼š0.0000030226074159145355 ms /è¿­ä»£ï¼ˆæœ€å¿«ï¼‰allocateGrowableArrayï¼š0.00018900632858276367 ms /è¿­ä»£ï¼ˆæ…¢62.5å€ï¼‰</p><p>  Now that we have a tool for running microbenchmarks, lets extend it withsupport for profiling benchmarks as they are running. This would help us tounderstand where benchmark is spending time and confirm that it ismeasuring exactly what we want it to measure.</p><p>  æ—¢ç„¶æˆ‘ä»¬æœ‰äº†è¿è¡Œå¾®åŸºå‡†æµ‹è¯•çš„å·¥å…·ï¼Œå°±å¯ä»¥åœ¨è¿è¡ŒåŸºå‡†æµ‹è¯•æ—¶å¯¹å…¶è¿›è¡Œæ‰©å±•ä»¥æä¾›æ”¯æŒã€‚è¿™å°†æœ‰åŠ©äºæˆ‘ä»¬äº†è§£åŸºå‡†æµ‹è¯•åœ¨å“ªé‡ŒèŠ±è´¹æ—¶é—´ï¼Œå¹¶ç¡®è®¤å®ƒæ­£åœ¨å‡†ç¡®åœ°æµ‹é‡æˆ‘ä»¬æƒ³è¦è¡¡é‡çš„æ°´å¹³ã€‚</p><p> Flutterâ€™s  release builds exclude Dartâ€™s builtin profiler so we will haveto use a native profiler instead, for example  simpleperf on Android.</p><p> Flutterçš„å‘è¡Œç‰ˆæœ¬ä¸åŒ…å«Dartçš„å†…ç½®æ¢æŸ¥å™¨ï¼Œå› æ­¤æˆ‘ä»¬å°†ä¸å¾—ä¸ä½¿ç”¨æœ¬æœºæ¢æŸ¥å™¨ï¼Œä¾‹å¦‚Androidä¸Šçš„simpleperfã€‚</p><p> Android has comprehensive  documentation for using  simpleperf, which I am not going to duplicate here.  simpleperf also comeswith C++ (and Java) code called   app_api which can be linked into an application to allow programmatic access to the profiler.</p><p> Androidæä¾›äº†æœ‰å…³ä½¿ç”¨simpleperfçš„å…¨é¢æ–‡æ¡£ï¼Œåœ¨æ­¤ä¸å†èµ˜è¿°ã€‚ simpleperfè¿˜é™„å¸¦æœ‰åä¸ºapp_apiçš„C ++ï¼ˆå’ŒJavaï¼‰ä»£ç ï¼Œå¯ä»¥å°†å…¶é“¾æ¥åˆ°åº”ç”¨ç¨‹åºä¸­ï¼Œä»¥å…è®¸ä»¥ç¼–ç¨‹æ–¹å¼è®¿é—®æ¢æŸ¥å™¨ã€‚ </p><p> In reality  app_api does not do anything overly fancy: it just runs simpleperf binary with the right command line options. Thatâ€™s why I decidedto just port relevant parts of  app_api to pure Dart.  We could also bind to C++ version of  app_api using Dart FFI, but that requires packaging this C++ into a  Flutter plugin, which complicates things, because  benchmark_harness is a pure Dart package and it canâ€™t depend on a Flutter plugin package.</p><p>å®é™…ä¸Šï¼Œapp_apiå¹¶ä¸ä¼šåšä»»ä½•èŠ±å“¨çš„äº‹æƒ…ï¼šå®ƒåªæ˜¯ä½¿ç”¨æ­£ç¡®çš„å‘½ä»¤è¡Œé€‰é¡¹è¿è¡ŒsimpleperfäºŒè¿›åˆ¶æ–‡ä»¶ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘å†³å®šåªå°†app_apiçš„ç›¸å…³éƒ¨åˆ†ç§»æ¤åˆ°çº¯Dartçš„åŸå› ã€‚æˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨Dart FFIç»‘å®šåˆ°app_apiçš„C ++ç‰ˆæœ¬ï¼Œä½†è¿™éœ€è¦å°†C ++æ‰“åŒ…åˆ°Flutteræ’ä»¶ä¸­ï¼Œè¿™ä½¿äº‹æƒ…å˜å¾—å¤æ‚ï¼Œå› ä¸ºBenchmark_harnessæ˜¯çº¯Dartç¨‹åºåŒ…ï¼Œå¹¶ä¸”ä¸èƒ½ä¾èµ–Flutteræ’ä»¶ç¨‹åºåŒ…ã€‚</p><p> // benchmark_harness/lib/src/simpleperf/profiling_session.dart class  ProfilingSession  {  Future &lt; void &gt;  start (  { RecordingOptions  options  =  const  RecordingOptions ()})  async  {  // ...  await  _startSimpleperfProcess ( options );  }  Future &lt; void &gt;  _startSimpleperfProcess ( RecordingOptions  options )  async  {  final  simpleperfBinary  =  await  _findSimplePerf ();  _simpleperf  =  await  Process . start (  simpleperfBinary ,  [  &#39;record&#39; ,  &#39;--log-to-android-buffer&#39; ,  &#39;--log&#39; ,  &#39;debug&#39; ,  &#39;--stdio-controls-profiling&#39; ,  &#39;--in-app&#39; ,  &#39;--tracepoint-events&#39; ,  &#39;/data/local/tmp/tracepoint_events&#39; ,  &#39;-o&#39; ,  options . outputFilename  ??  _makeOutputFilename (),  &#39;-e&#39; ,  options . event ,  &#39;-f&#39; ,  options . frequency . toString (),  &#39;-p&#39; ,  _getpid (). toString (),  ... _callgraphFlagsFrom ( options ),  ],  workingDirectory:  simpleperfDataDir ,  );  // ...  } }</p><p> // // beta_harness / lib / src / simpleperf / profiling_session.dartç±»ProfilingSession {æœªæ¥ï¼†lt;æ— æ•ˆå¼€å§‹ï¼ˆ{RecordingOptions options = const RecordingOptionsï¼ˆï¼‰}ï¼‰async {// ...ç­‰å¾…_startSimpleperfProcessï¼ˆoptionsï¼‰; }æœªæ¥ï¼†lt;æ— æ•ˆ_startSimpleperfProcessï¼ˆRecordingOptionsé€‰é¡¹ï¼‰async {æœ€ç»ˆsimpleperfBinary =ç­‰å¾…_findSimplePerfï¼ˆï¼‰; _simpleperf =ç­‰å¾…æµç¨‹ã€‚å¼€å§‹ï¼ˆsimpleperfBinaryï¼Œ[ï¼†ï¼ƒ39; recordï¼†ï¼ƒ39;ï¼Œï¼†ï¼ƒ39;-log-to-android-bufferï¼†ï¼ƒ39;ï¼Œï¼†ï¼ƒ39;-logï¼†ï¼ƒ39;ï¼Œï¼†ï¼ƒ39; debugï¼†ï¼ƒ39; ï¼Œ--stdio-controls-profilingï¼†ï¼ƒ39;ï¼Œï¼†ï¼ƒ39;-app-ï¼ƒ39; --tracepoint-eventsï¼†ï¼ƒ39;ï¼Œï¼†ï¼ƒ39; / data / local / tmp / tracepoint_eventsï¼†ï¼ƒ39;ï¼Œï¼†ï¼ƒ39; -oï¼†ï¼ƒ39;ï¼Œoptionsã€‚outputFilename ?? _makeOutputFilenameï¼ˆï¼‰ï¼Œï¼†ï¼ƒ39; -eï¼†ï¼ƒ39;ï¼Œoptionsã€‚eventï¼Œï¼†ï¼ƒ39; -fï¼†ï¼ƒ39; ï¼Œoptionsã€‚frequencyã€‚toStringï¼ˆï¼‰ï¼Œï¼†ï¼ƒ39; -pï¼†ï¼ƒ39;ï¼Œ_getpidï¼ˆï¼‰ã€‚toStringï¼ˆï¼‰ï¼Œ... _callgraphFlagsFromï¼ˆoptionsï¼‰ï¼Œ]ï¼ŒworkingDirectoryï¼šsimpleperfDataDirï¼Œï¼‰; // ...}}</p><p> Then I adjusted  benchmark_runner.dart to run benchmark it just measuredunder the profiler and save profile into a  perf-$benchmarkName.datafile. This file will be created in applicationâ€™s data directory:</p><p> ç„¶åï¼Œæˆ‘è°ƒæ•´äº†Benchmark_runner.dartä»¥è¿è¡Œå®ƒåœ¨åˆ†æå™¨ä¸‹æµ‹é‡çš„åŸºå‡†ï¼Œå¹¶å°†é…ç½®æ–‡ä»¶ä¿å­˜åˆ°perf- $ benchmarkName.dataæ–‡ä»¶ä¸­ã€‚è¯¥æ–‡ä»¶å°†åœ¨åº”ç”¨ç¨‹åºçš„æ•°æ®ç›®å½•ä¸­åˆ›å»ºï¼š</p><p> Future &lt; void &gt;  runBenchmarks ( Map &lt; String ,  void  Function ( int )&gt;  benchmarks )  async  {  _event ( &#39;benchmark.running&#39; );  final  profiler  =  Platform . isAndroid  ?  ProfilingSession ()  :  null ;  for  ( var  entry  in  benchmarks . entries )  {  final  result  =  measure ( entry . value ,  name:  entry . key );  _event ( &#39;benchmark.result&#39; ,  result );  if  ( profiler  !=  null )  {  // Run benchmark for the same amount of iterations and profile it.  await  profiler . start (  options:  RecordingOptions ( outputFilename:  &#39;perf- ${entry.key} .data&#39; ));  entry . value ( result . numIterations );  await  profiler . stop ();  }  }  _event ( &#39;benchmark.done&#39; ); }</p><p> æœªæ¥ï¼†lt;æ— æ•ˆrunBenchmarksï¼ˆMapï¼†lt; Stringï¼Œvoid Functionï¼ˆintï¼‰ï¼†gt;åŸºå‡†ï¼‰async {_eventï¼ˆï¼†ï¼ƒ39; benchmark.runningï¼†ï¼ƒ39;ï¼‰;æœ€ç»ˆæ¢æŸ¥å™¨= Platformã€‚æ˜¯Androidå—ï¼Ÿ ProfilingSessionï¼ˆï¼‰ï¼šç©ºï¼›å¯¹äºï¼ˆåŸºå‡†ä¸­çš„å¯å˜é¡¹ã€‚é¡¹ï¼‰{æœ€ç»ˆç»“æœ=åº¦é‡ï¼ˆé¡¹ã€‚å€¼ï¼Œåç§°ï¼šé¡¹ã€‚é”®ï¼‰; _eventï¼ˆï¼†benchmark.resultï¼†ï¼ƒ39;ï¼Œresultï¼‰; ifï¼ˆprofilerï¼= nullï¼‰{//ä»¥ç›¸åŒçš„è¿­ä»£æ¬¡æ•°è¿è¡ŒåŸºå‡†å¹¶å¯¹å…¶è¿›è¡Œæ¦‚è¦åˆ†æã€‚ç­‰å¾…åˆ†æå™¨ã€‚ startï¼ˆé€‰é¡¹ï¼šRecordingOptionsï¼ˆoutputFilenameï¼šï¼†ï¼ƒ39; perf- $ {entry.key} .dataï¼†ï¼ƒ39;ï¼‰ï¼‰ï¼›è¿›å…¥ã€‚å€¼ï¼ˆç»“æœã€‚numIterationsï¼‰ï¼›ç­‰å¾…åˆ†æå™¨ã€‚åœ ï¼ˆï¼‰; }} _eventï¼ˆï¼†ï¼ƒ39; benchmark.doneï¼†ï¼ƒ39;ï¼‰; }</p><p>  api_profiler.py prepare configures your device for profiling - we are goingto call it before running benchmarks;</p><p>  api_profiler.py prepareé…ç½®æ‚¨çš„è®¾å¤‡è¿›è¡Œæ€§èƒ½åˆ†æ-æˆ‘ä»¬å°†åœ¨è¿è¡ŒåŸºå‡†æµ‹è¯•ä¹‹å‰è°ƒç”¨å®ƒï¼›</p><p>  api_profiler.py collect pulls collected profiles from the device - weare going to call it after all benchmarks finish running to pull all generated perf-*.data from the device.</p><p>  api_profiler.py collectä»è®¾å¤‡ä¸­æå–æ”¶é›†åˆ°çš„é…ç½®æ–‡ä»¶-åœ¨æ‰€æœ‰åŸºå‡†æµ‹è¯•è¿è¡Œå®Œæ¯•ä¹‹åï¼Œå°†è°ƒç”¨å®ƒæ¥ä»è®¾å¤‡ä¸­æå–æ‰€æœ‰ç”Ÿæˆçš„perf-*ã€‚dataã€‚</p><p>  NDKâ€™s  simpleperf binary supports both  record and  report commands, justlike Linux  perf. Looking around in the NDK I have also discovered a bunch of helper scripts written in Python (e.g.  report_html.py which can generate a HTML report). Peakinginto those scripts I have discovered that they make use of  libsimpleperf_report.so librarywhich handles parsing and symbolization of collected profiles. The API forthis library is defined at the top of   simpleperf/report_lib_interface.cpp file in simpleperf sources.</p><p>  NDKçš„simpleperfäºŒè¿›åˆ¶æ–‡ä»¶æ”¯æŒè®°å½•å’ŒæŠ¥å‘Šå‘½ä»¤ï¼Œå°±åƒLinuxæ€§èƒ½ä¸€æ ·ã€‚åœ¨NDKä¸­ç¯é¡¾å››å‘¨ï¼Œæˆ‘è¿˜å‘ç°äº†ä¸€å †ç”¨Pythonç¼–å†™çš„å¸®åŠ©ç¨‹åºè„šæœ¬ï¼ˆä¾‹å¦‚ï¼Œreport_html.pyå¯ä»¥ç”ŸæˆHTMLæŠ¥å‘Šï¼‰ã€‚æ·±å…¥äº†è§£è¿™äº›è„šæœ¬ï¼Œæˆ‘å‘ç°å®ƒä»¬ä½¿ç”¨libsimpleperf_report.soåº“ï¼Œè¯¥åº“å¤„ç†æ”¶é›†çš„é…ç½®æ–‡ä»¶çš„è§£æå’Œç¬¦å·åŒ–ã€‚è¯¥åº“çš„APIåœ¨simpleperfæºä»£ç ä¸­çš„simpleperf / report_lib_interface.cppæ–‡ä»¶çš„é¡¶éƒ¨å®šä¹‰ã€‚ </p><p> Using   ffigen I generated  dart:ffibased bindings for this library, allowing me to use it from  benchmark_harnessscript to process collected profiling samples:</p><p>ä½¿ç”¨ffigenï¼Œæˆ‘ä¸ºæ­¤åº“ç”Ÿæˆäº†åŸºäºdartï¼šffiçš„ç»‘å®šï¼Œä»è€Œä½¿æˆ‘å¯ä»¥ä»Benchmark_harnessscriptä¸­ä½¿ç”¨å®ƒæ¥å¤„ç†æ”¶é›†çš„æ€§èƒ½åˆ†ææ ·æœ¬ï¼š</p><p> final  reportLib  =  report_bindings . NativeLibrary (  ffi . DynamicLibrary . open ( ndk . simpleperfReportLib )); Future &lt; void &gt;  _printProfile ( String  profileData )  async  {  final  session  =  reportLib . CreateReportLib ();  reportLib . SetRecordFile ( session ,  Utf8 . toUtf8 ( profileData ). cast ());  // Iterate over all collected samples.  for  (;;)  {  final  sample  =  reportLib . GetNextSample ( session );  if  ( sample  ==  ffi . nullptr )  {  break ;  }  final  period  =  sample . ref . period ;  final  symbol  =  reportLib . GetSymbolOfCurrentSample ( session );  final  dsoName  =  Utf8 . fromUtf8 ( symbol . ref . dso_name . cast ());  final  symbolName  =  Utf8 . fromUtf8 ( symbol . ref . symbol_name . cast ());  // Process sample for the symbol [symbolName] in dso [dsoName] and collect  // aggregate statistics (samples per symbol, total sampling period, etc).  // ...  }  // Report top N hottest symbols }</p><p> æœ€ç»ˆreportLib = report_bindingsã€‚ NativeLibraryï¼ˆffiã€‚DynamicLibraryã€‚openï¼ˆndkã€‚simpleperfReportLibï¼‰ï¼‰;æœªæ¥ï¼†lt;æ— æ•ˆ_printProfileï¼ˆå­—ç¬¦ä¸²profileDataï¼‰å¼‚æ­¥{æœ€ç»ˆä¼šè¯= reportLibã€‚ CreateReportLibï¼ˆï¼‰; reportLibã€‚ SetRecordFileï¼ˆsessionï¼ŒUtf8ã€‚toUtf8ï¼ˆprofileDataï¼‰ã€‚castï¼ˆï¼‰ï¼‰; //éå†æ‰€æœ‰æ”¶é›†çš„æ ·æœ¬ã€‚ forï¼ˆ;;ï¼‰{æœ€ç»ˆæ ·æœ¬= reportLibã€‚ GetNextSampleï¼ˆsessionï¼‰;å¦‚æœï¼ˆæ ·æœ¬== ffiã€‚nullptrï¼‰{ä¸­æ–­; }æœŸæœ«=æ ·æœ¬ã€‚å‚è€ƒã€‚æ—¶æœŸ;æœ€ç»ˆç¬¦å·= reportLibã€‚ GetSymbolOfCurrentSampleï¼ˆsessionï¼‰;æœ€ç»ˆdsoName = Utf8ã€‚ fromUtf8ï¼ˆç¬¦å·ã€‚refã€‚dso_nameã€‚castï¼ˆï¼‰ï¼‰ï¼›æœ€åçš„symbolName = Utf8ã€‚ fromUtf8ï¼ˆsymbolã€‚refã€‚symbol_nameã€‚castï¼ˆï¼‰ï¼‰; //åœ¨dso [dsoName]ä¸­å¤„ç†ç¬¦å·[symbolName]çš„æ ·æœ¬å¹¶//æ”¶é›†æ±‡æ€»ç»Ÿè®¡ä¿¡æ¯ï¼ˆæ¯ä¸ªç¬¦å·çš„æ ·æœ¬ï¼Œæ€»é‡‡æ ·æ—¶é—´ç­‰ï¼‰ã€‚ // ...} //æŠ¥å‘Šå‰Nä¸ªæœ€çƒ­é—¨çš„ç¬¦å·}</p><p> When I run this for the first time Iâ€™ve discovered that  simpleperf canâ€™treally attribute most of the samples to a meaningful symbol neither for libapp.so (which contains AOT compiled Dart code) nor for  libflutter.so(which contains Flutter engine code). Here is the very first report I got:</p><p> å½“æˆ‘ç¬¬ä¸€æ¬¡è¿è¡Œæ­¤ç¨‹åºæ—¶ï¼Œæˆ‘å‘ç°simpleperfä¸èƒ½å°†å¤§å¤šæ•°æ ·æœ¬çœŸæ­£åœ°å½’å› äºlibapp.soï¼ˆåŒ…å«AOTç¼–è¯‘çš„Dartä»£ç ï¼‰æˆ–libflutter.soï¼ˆåŒ…å«Flutterå¼•æ“ä»£ç ï¼‰çš„æœ‰æ„ä¹‰ç¬¦å·ã€‚ ï¼‰ã€‚è¿™æ˜¯æˆ‘å¾—åˆ°çš„ç¬¬ä¸€ä»½æŠ¥å‘Šï¼š</p><p> Hot methods when running allocateGrowableArray: 88.24% _kDartIsolateSnapshotInstructions (libapp.so) 4.04% unknown (libflutter.so) 3.15% unknown ([kernel.kallsyms]) 1.44% pthread_mutex_lock (libc.so) 1.30% pthread_mutex_unlock (libc.so)  ...</p><p> è¿è¡ŒallocateGrowableArrayæ—¶çš„çƒ­é—¨æ–¹æ³•ï¼š88.24ï¼…_kDartIsolateSnapshotInstructionsï¼ˆlibapp.soï¼‰4.04ï¼…æœªçŸ¥ï¼ˆlibflutter.soï¼‰3.15ï¼…æœªçŸ¥ï¼ˆ[kernel.kallsyms]ï¼‰1.44ï¼…pthread_mutex_lockï¼ˆlibc.soï¼‰1.30ï¼…pthread_mutex_unlockï¼ˆlibc.soï¼‰.. ã€‚</p><p> This is not surprising: both of these libraries are stripped anddonâ€™t contain any useful symbol information for  simpleperf to use.</p><p> è¿™ä¸è¶³ä¸ºå¥‡ï¼šè¿™ä¸¤ä¸ªåº“éƒ½è¢«å‰¥ç¦»ï¼Œå¹¶ä¸”ä¸åŒ…å«ä»»ä½•æœ‰ç”¨çš„ç¬¦å·ä¿¡æ¯ä¾›simpleperfä½¿ç”¨ã€‚</p><p> Fortunately,  libflutter.so symbols can be fetched from Cloud Storage wherebuild infrastructure is archiving them, e.g. symbols for an ARM64 Androidrelease build of Flutter engine at commit  e115066d...reside in gs://flutter_infra/flutter/e115066d.../android-arm64-release/symbols.zip. Just few months ago I have written some Dart code for downloading and caching Flutter Engine symbols based on commit hash for   @flutter-symbolizer-bot, so I could just reuse the very same code here.</p><p> å¹¸è¿çš„æ˜¯ï¼Œå¯ä»¥ä»Cloud Storageæå–libflutter.soç¬¦å·ï¼Œå…¶ä¸­æ„å»ºåŸºç¡€æ¶æ„æ­£åœ¨å°†å®ƒä»¬å½’æ¡£ã€‚ä½äºæäº¤e115066dçš„Flutterå¼•æ“çš„ARM64 Androidç‰ˆæœ¬æ„å»ºçš„ç¬¦å·...é©»ç•™åœ¨gsï¼š//flutter_infra/flutter/e115066d.../android-arm64-release/symbols.zipä¸­ã€‚å°±åœ¨å‡ ä¸ªæœˆå‰ï¼Œæˆ‘å·²ç»ç¼–å†™äº†ä¸€äº›Dartä»£ç ï¼Œç”¨äºåŸºäº@ flutter-symbolizer-botçš„æäº¤å“ˆå¸Œå€¼æ¥ä¸‹è½½å’Œç¼“å­˜Flutter Engineç¬¦å·ï¼Œå› æ­¤æˆ‘å¯ä»¥åœ¨è¿™é‡Œé‡å¤ä½¿ç”¨ç›¸åŒçš„ä»£ç ã€‚</p><p> Getting symbols for  libapp.so is a more interesting problem. Dart VM AOTcompiler is capable of producing DWARF debug sections in the ELF bin</p><p> è·å–libapp.soçš„ç¬¦å·æ˜¯ä¸€ä¸ªæ›´æœ‰è¶£çš„é—®é¢˜ã€‚ Dart VM AOTcompilerèƒ½å¤Ÿåœ¨ELF binä¸­ç”ŸæˆDWARFè°ƒè¯•éƒ¨åˆ† </p><p>......</p><p>...... </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://mrale.ph/blog/2021/01/21/microbenchmarking-dart-part-1.html">https://mrale.ph/blog/2021/01/21/microbenchmarking-dart-part-1.html</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/dart/">#dart</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/æ ‡æª/">#æ ‡æª</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/åŸºå‡†/">#åŸºå‡†</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è®¾è®¡/">#è®¾è®¡</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/åˆ›æ„/">#åˆ›æ„</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/æ‘„å½±/">#æ‘„å½±</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/å›¾ç‰‡/">#å›¾ç‰‡</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/æ¸¸æˆ/">#æ¸¸æˆ</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è½¯ä»¶/">#è½¯ä»¶</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è§†é¢‘/">#è§†é¢‘</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/æ‰‹æœº/">#æ‰‹æœº</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/å¹¿å‘Š/">#å¹¿å‘Š</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/ç½‘ç«™/">#ç½‘ç«™</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/å…è´¹/">#å…è´¹</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/ä¸‹è½½/">#ä¸‹è½½</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è‹¹æœ/">#è‹¹æœ</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/å¾®è½¯/">#å¾®è½¯</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/éŸ³ä¹/">#éŸ³ä¹</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/åšå®¢/">#åšå®¢</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/æ¶æ/">#æ¶æ</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è‰ºæœ¯/">#è‰ºæœ¯</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/å·¥å…·/">#å·¥å…·</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/åˆ†äº«/">#åˆ†äº«</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>