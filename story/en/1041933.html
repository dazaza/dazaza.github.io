<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>2020å¹´çš„æ–°è®¾ç½® New setup for 2020</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">New setup for 2020<br/>2020å¹´çš„æ–°è®¾ç½® </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2020-12-27 07:30:29</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2020/12/d120b123b23c0efe0814098e10b00187.png"><img src="http://img2.diglog.com/img/2020/12/d120b123b23c0efe0814098e10b00187.png" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>We talked about the changelog.com setup for 2019 in  episode #344, and also captured it last year  in this post. At the time, changelog.com was running on Docker Swarm, everything was managed by Terraform, and the end-result was simpler &amp; better because:</p><p>æˆ‘ä»¬åœ¨ç¬¬344é›†ä¸­è®¨è®ºäº†2019å¹´çš„changelog.comè®¾ç½®ï¼Œå¹¶åœ¨å»å¹´çš„è¿™ç¯‡æ–‡ç« ä¸­ä¹Ÿè¿›è¡Œäº†ä»‹ç»ã€‚å½“æ—¶ï¼Œchangelog.comåœ¨Docker Swarmä¸Šè¿è¡Œï¼Œæ‰€æœ‰å†…å®¹å‡ç”±Terraformç®¡ç†ï¼Œæœ€ç»ˆç»“æœæ›´åŠ ç®€å•æ›´å¥½æ˜¯å› ä¸ºï¼š</p><p>  This year, we simplified and improved the changelog.com setup further by replacing Docker Swarm and Terraform with Linode Kubernetes Engine (LKE). Not only is the new setup more cohesive, but deploys are 20% faster, changelog.com is more resilient with a mean time to recovery (MTTR) of just under 8 minutes, and interacting with the entire setup is done via a single pane of glass:</p><p>  ä»Šå¹´ï¼Œæˆ‘ä»¬ç”¨Linode Kubernetes Engineï¼ˆLKEï¼‰å–ä»£äº†Docker Swarmå’ŒTerraformï¼Œä»è€Œè¿›ä¸€æ­¥ç®€åŒ–å’Œæ”¹è¿›äº†changelog.comçš„è®¾ç½®ã€‚æ–°è®¾ç½®ä¸ä»…æ›´å…·å‡èšåŠ›ï¼Œè€Œä¸”éƒ¨ç½²é€Ÿåº¦æé«˜äº†20ï¼…ï¼Œchangelog.comçš„æ¢å¤èƒ½åŠ›æ›´å¼ºï¼Œå¹³å‡æ¢å¤æ—¶é—´ï¼ˆMTTRï¼‰ä¸åˆ°8åˆ†é’Ÿï¼Œå¹¶ä¸”é€šè¿‡å•ä¸ªçª—æ ¼å³å¯å®Œæˆæ•´ä¸ªè®¾ç½®çš„äº¤äº’ç»ç’ƒï¼š</p><p>    We will answer all these questions, and also cover what worked well, what could have been better, and what comes next for changelog.com. Letâ€™s start with the big one:</p><p>    æˆ‘ä»¬å°†å›ç­”æ‰€æœ‰è¿™äº›é—®é¢˜ï¼Œå¹¶ä»‹ç»æœ‰æ•ˆçš„æ–¹æ³•ï¼Œå¯èƒ½ä¼šæ›´å¥½çš„æ–¹æ³•ä»¥åŠchangelog.comçš„åç»­å†…å®¹ã€‚è®©æˆ‘ä»¬ä»å¤§çš„å¼€å§‹ï¼š</p><p>  Because managing infrastructure doesnâ€™t bring any value to Changelog, the business. Kubernetes enables us to declare stateless &amp; stateful workloads, web traffic ingresses, certificates and other higher level concepts instead of servers, load balancers, and other similar lower-level concepts. At the end of the day, we donâ€™t care how things happen, as long as the following happens: automatic zero-downtime deploys on every commit, daily backups, minimal disruption when an actual server goes away, etc. Kubernetes makes this easy.</p><p>  ç”±äºç®¡ç†åŸºç¡€æ¶æ„ä¸ä¼šä¸ºChangelogå¸¦æ¥ä»»ä½•ä»·å€¼ï¼Œå› æ­¤å¯¹ä¼ä¸šè€Œè¨€ã€‚ Kubernetesä½¿æˆ‘ä»¬èƒ½å¤Ÿå£°æ˜æ— çŠ¶æ€ï¼†amp;çŠ¶æ€å·¥ä½œè´Ÿè½½ï¼ŒWebæµé‡å…¥å£ï¼Œè¯ä¹¦å’Œå…¶ä»–è¾ƒé«˜çº§åˆ«çš„æ¦‚å¿µï¼Œè€Œä¸æ˜¯æœåŠ¡å™¨ï¼Œè´Ÿè½½å¹³è¡¡å™¨å’Œå…¶ä»–ç±»ä¼¼çš„è¾ƒä½çº§åˆ«çš„æ¦‚å¿µã€‚å½’æ ¹ç»“åº•ï¼Œåªè¦å‘ç”Ÿä»¥ä¸‹æƒ…å†µï¼Œæˆ‘ä»¬å°±ä¸ä¼šåœ¨æ„äº‹æƒ…çš„å‘ç”Ÿï¼šåœ¨æ¯æ¬¡æäº¤æ—¶è‡ªåŠ¨éƒ¨ç½²é›¶åœæœºæ—¶é—´ï¼Œæ¯å¤©å¤‡ä»½ï¼Œåœ¨å®é™…æœåŠ¡å™¨æ¶ˆå¤±æ—¶å°†ä¸­æ–­é™åˆ°æœ€ä½ç­‰ã€‚Kubernetesä½¿æ­¤æ“ä½œå˜å¾—å®¹æ˜“ã€‚</p><p> Let us expand with a specific example: what business value does renewing the TLS certificate bring? Yes, all web requests that we serve must be secured, and the SSL certificate needs to be valid, but why should we spend any effort renewing it?</p><p> è®©æˆ‘ä»¬ä»¥ä¸€ä¸ªç‰¹å®šçš„ç¤ºä¾‹è¿›è¡Œæ‰©å±•ï¼šç»­è®¢TLSè¯ä¹¦å¸¦æ¥äº†å“ªäº›ä¸šåŠ¡ä»·å€¼ï¼Ÿæ˜¯çš„ï¼Œæˆ‘ä»¬æä¾›çš„æ‰€æœ‰Webè¯·æ±‚éƒ½å¿…é¡»å¾—åˆ°ä¿æŠ¤ï¼Œå¹¶ä¸”SSLè¯ä¹¦å¿…é¡»æœ‰æ•ˆï¼Œä½†æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬è¦èŠ±äº›ç²¾åŠ›æ¥æ›´æ–°å®ƒï¼Ÿ</p><p> Maybe your IaaS provider already manages SSL for you, but does that mean that you are forced to use the CDN service from the same provider? Or does your CDN provider manage one certificate, and your IaaS provider another certificate?</p><p> ä¹Ÿè®¸æ‚¨çš„IaaSæä¾›å•†å·²ç»ä¸ºæ‚¨ç®¡ç†SSLï¼Œä½†è¿™æ˜¯å¦æ„å‘³ç€æ‚¨è¢«è¿«ä½¿ç”¨åŒä¸€æä¾›å•†çš„CDNæœåŠ¡ï¼Ÿè¿˜æ˜¯æ‚¨çš„CDNæä¾›ç¨‹åºç®¡ç†ä¸€ä¸ªè¯ä¹¦ï¼Œè€ŒIaaSæä¾›ç¨‹åºç®¡ç†å¦ä¸€ä¸ªè¯ä¹¦ï¼Ÿ</p><p> In our case, even if Fastly, our CDN partner, can manage Letâ€™s Encrypt (LE) certificates for us, we need to figure out how to keep them updated in our Linode NodeBalancer (a.k.a. load balancer). We have learned this the hard way, but if Fastly manages LE certificates for a domain, no other LE provisioner can be configured for that domain. Our load balancer needs TLS certificates as well, and if we use LE via Fastly, we canâ€™t get LE certificates for the load balancer. While this is a very specific example from our experience, it shows the type of situations that can take away time &amp; effort from higher-value work.</p><p> å°±æˆ‘ä»¬è€Œè¨€ï¼Œå³ä½¿æˆ‘ä»¬çš„CDNåˆä½œä¼™ä¼´Fastlyå¯ä»¥ä¸ºæˆ‘ä»¬ç®¡ç†Let's Encryptï¼ˆLEï¼‰è¯ä¹¦ï¼Œæˆ‘ä»¬ä¹Ÿéœ€è¦å¼„æ¸…æ¥šå¦‚ä½•åœ¨æˆ‘ä»¬çš„Linode NodeBalancerï¼ˆåˆç§°è´Ÿè½½å‡è¡¡å™¨ï¼‰ä¸­å¯¹å…¶è¿›è¡Œæ›´æ–°ã€‚æˆ‘ä»¬å¾ˆéš¾å­¦åˆ°è¿™ä¸€ç‚¹ï¼Œä½†æ˜¯ï¼Œå¦‚æœå¿«é€Ÿç®¡ç†æŸä¸ªåŸŸçš„LEè¯ä¹¦ï¼Œåˆ™æ— æ³•ä¸ºè¯¥åŸŸé…ç½®å…¶ä»–LEä¾›åº”å•†ã€‚æˆ‘ä»¬çš„è´Ÿè½½å‡è¡¡å™¨ä¹Ÿéœ€è¦TLSè¯ä¹¦ï¼Œå¦‚æœæˆ‘ä»¬é€šè¿‡â€œå¿«é€Ÿâ€ä½¿ç”¨LEï¼Œåˆ™æ— æ³•è·å¾—è´Ÿè½½å‡è¡¡å™¨çš„LEè¯ä¹¦ã€‚æ ¹æ®æˆ‘ä»¬çš„ç»éªŒï¼Œè¿™æ˜¯ä¸€ä¸ªéå¸¸å…·ä½“çš„ç¤ºä¾‹ï¼Œå®ƒæ˜¾ç¤ºäº†å¯èƒ½å ç”¨æ—¶é—´å’Œæ—¶é—´çš„æƒ…å†µç±»å‹ã€‚é«˜ä»·å€¼å·¥ä½œçš„åŠªåŠ›ã€‚ </p><p> In our Linode Kubernetes Engine setup,  cert-manager is responsible for managing certificates for all our domains, and  a job keeps them synchronised in Fastly. If we need to check on the state of all our certificates, we can do it from the same pane of glass mentioned earlier:</p><p>åœ¨æˆ‘ä»¬çš„Linode Kubernetes Engineè®¾ç½®ä¸­ï¼Œcert-managerè´Ÿè´£ç®¡ç†æˆ‘ä»¬æ‰€æœ‰åŸŸçš„è¯ä¹¦ï¼Œå¹¶é€šè¿‡ä¸€é¡¹å·¥ä½œä½¿å®ƒä»¬ä¿æŒå¿«é€ŸåŒæ­¥ã€‚å¦‚æœæˆ‘ä»¬éœ€è¦æ£€æŸ¥æ‰€æœ‰è¯ä¹¦çš„çŠ¶æ€ï¼Œåˆ™å¯ä»¥ä»å‰é¢æåˆ°çš„åŒä¸€çª—æ ¼ä¸­è¿›è¡Œæ£€æŸ¥ï¼š</p><p>   A single  Linode CLI command gets us a managed Kubernetes cluster, and a few commands later we have changelog.com and all its dependent services up and running, including DNS, TLS &amp; CDN integration.  We no longer provision load balancers, or configure DNS; we simply describe the resources that we need, and Kubernetes makes it happen.</p><p>   å•ä¸ªLinode CLIå‘½ä»¤ä¸ºæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªæ‰˜ç®¡çš„Kubernetesé›†ç¾¤ï¼Œéšåè¿˜æœ‰ä¸€äº›å‘½ä»¤ä½¿æˆ‘ä»¬å¯ä»¥è¿è¡Œchangelog.comåŠå…¶æ‰€æœ‰ç›¸å…³æœåŠ¡ï¼ŒåŒ…æ‹¬DNSï¼ŒTLSå’ŒAmpã€‚ CDNé›†æˆã€‚æˆ‘ä»¬ä¸å†æä¾›è´Ÿè½½å¹³è¡¡å™¨æˆ–é…ç½®DNSï¼›æˆ‘ä»¬åªéœ€æè¿°æˆ‘ä»¬éœ€è¦çš„èµ„æºï¼ŒKuberneteså³å¯å®ç°ã€‚</p><p> Yes, we need to install additional components such as  ingress-nginx,  cert-manager,  postgres-operator and a few others, but once we do this, we get to use higher-level resources such as PostgreSQL clusters, Letâ€™s Encrypt certificates and ingresses that handle load balancer provisioning.</p><p> æ˜¯çš„ï¼Œæˆ‘ä»¬éœ€è¦å®‰è£…å…¶ä»–ç»„ä»¶ï¼Œä¾‹å¦‚ingress-nginxï¼Œcert-managerï¼Œpostgres-operatorå’Œå…¶ä»–ä¸€äº›ç»„ä»¶ï¼Œä½†æ˜¯ä¸€æ—¦å®Œæˆï¼Œæˆ‘ä»¬å°±å¯ä»¥ä½¿ç”¨æ›´é«˜çº§åˆ«çš„èµ„æºï¼Œä¾‹å¦‚PostgreSQLé›†ç¾¤ï¼ŒåŠ å¯†è¯ä¹¦å’Œå…¥å£å¤„ç†è´Ÿè½½å‡è¡¡å™¨é…ç½®ã€‚</p><p> Did I mention that the Kubernetes is managed? That means updates are taken care of, and the requested number of nodes will be kept running at all times. If one of the VMs that runs a Kubernetes node gets deleted, it will be automatically re-created a few minutes later, and everything that should be running on it will be restored. This changes the conversation from   the IaaS provisioner needs to run and converge on the desired state to   Kubernetes will automatically fix everything within 10 minutes.</p><p> æˆ‘æ˜¯å¦æåˆ°è¿‡Kubernetesæ˜¯æ‰˜ç®¡çš„ï¼Ÿè¿™æ„å‘³ç€å°†å¤„ç†æ›´æ–°ï¼Œå¹¶ä¸”è¯·æ±‚æ•°é‡çš„èŠ‚ç‚¹å°†å§‹ç»ˆä¿æŒè¿è¡Œã€‚å¦‚æœåˆ é™¤äº†è¿è¡ŒKubernetesèŠ‚ç‚¹çš„å…¶ä¸­ä¸€å°è™šæ‹Ÿæœºï¼Œåˆ™å‡ åˆ†é’Ÿåå°†è‡ªåŠ¨é‡æ–°åˆ›å»ºè¯¥è™šæ‹Ÿæœºï¼Œå¹¶ä¸”åº”åœ¨è¯¥è™šæ‹Ÿæœºä¸Šè¿è¡Œçš„æ‰€æœ‰å†…å®¹éƒ½å°†è¢«è¿˜åŸã€‚è¿™æ”¹å˜äº†ä»IaaSä¾›åº”å•†è¿è¡Œæ‰€éœ€çš„ä¼šè¯å¹¶æ”¶æ•›åˆ°æ‰€éœ€çŠ¶æ€åˆ°Kubernetesçš„å¯¹è¯ï¼Œå®ƒå°†åœ¨10åˆ†é’Ÿå†…è‡ªåŠ¨ä¿®å¤æ‰€æœ‰é—®é¢˜ã€‚</p><p>   We are convinced that you have heard about â€œproduction Kubernetesâ€ many times in recent years. Maybe you have even tried some examples out, and had it running on your local machine via  Kind or  k3sup. For the vast majority, the beginning is easy, but running production on Kubernetes is hard. Many who start donâ€™t fully complete the migration. Even with  Monzo and  Zalando sharing Kubernetes in production stories for years, it doesnâ€™t make it any easier for you.</p><p>   æˆ‘ä»¬åšä¿¡ï¼Œè¿‘å¹´æ¥æ‚¨å·²ç»å¤šæ¬¡å¬è¯´è¿‡â€œç”Ÿäº§Kubernetesâ€ã€‚ä¹Ÿè®¸æ‚¨ç”šè‡³å°è¯•äº†ä¸€äº›ç¤ºä¾‹ï¼Œå¹¶é€šè¿‡Kindæˆ–k3supåœ¨æœ¬åœ°è®¡ç®—æœºä¸Šè¿è¡Œå®ƒã€‚å¯¹äºç»å¤§å¤šæ•°äººæ¥è¯´ï¼Œå¼€å§‹å¾ˆå®¹æ˜“ï¼Œä½†æ˜¯åœ¨Kubernetesä¸Šè¿è¡Œç”Ÿäº§å´å¾ˆå›°éš¾ã€‚è®¸å¤šæ–°æ‰‹å¹¶æ²¡æœ‰å®Œå…¨å®Œæˆè¿ç§»ã€‚å³ä½¿Monzoå’ŒZalandoåœ¨åˆ¶ä½œæ•…äº‹ä¸­åˆ†äº«Kuberneteså¤šå¹´äº†ï¼Œä¹Ÿå¹¶æ²¡æœ‰ä½¿æ‚¨æ›´è½»æ¾ã€‚</p><p> The best way that we can think of helping your production Kubernetes journey is  by making all our code available, including all the commit history that got us where we are today. If you spend time exploring past commits, you will even find links to public discussions with our technology partners that lead to specific improvements in Linode Kubernetes Engine (LKE), as well as other related areas such as  kube-prometheus &amp;  fastly-cli.</p><p> æˆ‘ä»¬å¯ä»¥è€ƒè™‘å¸®åŠ©æ‚¨çš„Kubernetesç”Ÿäº§è¿‡ç¨‹çš„æœ€ä½³æ–¹æ³•æ˜¯ä½¿æ‰€æœ‰ä»£ç å¯ç”¨ï¼ŒåŒ…æ‹¬ä½¿æˆ‘ä»¬ä»Šå¤©å¤„äºä»Šå¤©çš„æ‰€æœ‰æäº¤å†å²ã€‚å¦‚æœæ‚¨èŠ±æ—¶é—´æ¢ç´¢è¿‡å»çš„æäº¤ï¼Œæ‚¨ç”šè‡³ä¼šæ‰¾åˆ°ä¸æˆ‘ä»¬çš„æŠ€æœ¯åˆä½œä¼™ä¼´è¿›è¡Œå…¬å¼€è®¨è®ºçš„é“¾æ¥ï¼Œä»è€Œå¯¼è‡´Linode Kubernetes Engineï¼ˆLKEï¼‰ä»¥åŠå…¶ä»–ç›¸å…³é¢†åŸŸï¼ˆä¾‹å¦‚kube-prometheusï¼†amp;å¿«é€Ÿcliã€‚</p><p> Changelog.com is proof that running on production on Kubernetes can be straightforward, and it works well. Even if your team is small and time available for infrastructure-related work is limited, you can use our approach to get you going, then modify and adapt as and when needed. If changelog.com has been running on this setup with improved availability, responsiveness and resiliency, so can your production web app.</p><p> Changelog.comè¯æ˜äº†åœ¨Kubernetesä¸Šçš„ç”Ÿäº§ç¯å¢ƒä¸Šè¿è¡Œéå¸¸ç®€å•ï¼Œå¹¶ä¸”è¿è¡Œè‰¯å¥½ã€‚å³ä½¿æ‚¨çš„å›¢é˜Ÿå¾ˆå°ï¼Œå¯ç”¨äºåŸºç¡€æ¶æ„ç›¸å…³å·¥ä½œçš„æ—¶é—´ä¹Ÿå¾ˆæœ‰é™ï¼Œæ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨æˆ‘ä»¬çš„æ–¹æ³•å¼€å§‹å·¥ä½œï¼Œç„¶ååœ¨éœ€è¦æ—¶è¿›è¡Œä¿®æ”¹å’Œé€‚åº”ã€‚å¦‚æœchangelog.comå·²åœ¨æ­¤è®¾ç½®ä¸Šè¿è¡Œä¸”å…·æœ‰æ›´é«˜çš„å¯ç”¨æ€§ï¼Œå“åº”èƒ½åŠ›å’Œå¼¹æ€§ï¼Œé‚£ä¹ˆæ‚¨çš„ç”Ÿäº§Webåº”ç”¨ç¨‹åºä¹Ÿå¯ä»¥ã€‚ </p><p>  It all started at KubeCon + CloudNativeCon North America 2019 (remember  #374 &amp;  #375?), when I met with Hillary &amp; Mike from Linode. They gave me early access to Linode Kubernetes Engine (LKE) and one Linode CLI command later, we had our first three-node Kubernetes cluster running in Newark, New Jersey.</p><p>å½“æˆ‘ä¸å¸Œæ‹‰é‡Œï¼ˆHillaryï¼‰å’Œå®‰å¾·é²ï¼ˆHamparyï¼‰è§é¢æ—¶ï¼Œä¸€åˆ‡å§‹äºKubeCon + CloudNativeCon North America 2019ï¼ˆè¿˜è®°å¾—ï¼ƒ374å’Œï¼ƒ375ï¼Ÿï¼‰æ¥è‡ªLinodeçš„Mikeã€‚ä»–ä»¬è®©æˆ‘å¯ä»¥å°½æ—©ä½¿ç”¨Linode Kuberneteså¼•æ“ï¼ˆLKEï¼‰å’Œä¸€ä¸ªLinode CLIå‘½ä»¤ï¼Œä¹‹åï¼Œæˆ‘ä»¬åœ¨æ–°æ³½è¥¿å·çš„çº½ç“¦å…‹è¿è¡Œäº†æˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªä¸‰èŠ‚ç‚¹Kubernetesé›†ç¾¤ã€‚</p><p> By the way, if you want early access to new Linode new products before they hit the market and provide valuable feedback to influence product direction, you can join  Linode Green Light too.</p><p> é¡ºä¾¿è¯´ä¸€å¥ï¼Œå¦‚æœæ‚¨æƒ³åœ¨æ–°çš„Linodeæ–°äº§å“ä¸Šå¸‚ä¹‹å‰å°±å°½æ—©ä½¿ç”¨å®ƒä»¬å¹¶æä¾›æœ‰ä»·å€¼çš„åé¦ˆæ„è§æ¥å½±å“äº§å“å‘å±•æ–¹å‘ï¼Œé‚£ä¹ˆæ‚¨ä¹Ÿå¯ä»¥åŠ å…¥Linode Green Lightã€‚</p><p>   Monitoring was a bit trickier because we had to figure out how to do Prometheus &amp; Grafana in Kubernetes properly. There is some conflicting advice on how to get this up and running, and  the documentation didnâ€™t work for us, so we have contributed our solution. Out of all the available approaches, we have settled on the  kube-prometheus operator which gives us plenty of insight into Kubernetes and system metrics out of the box. We are yet to integrate kube-prometheus with other services such as ingress-nginx, PostgreSQL, Phoenix etc.</p><p>   ç›‘æ§æœ‰äº›æ£˜æ‰‹ï¼Œå› ä¸ºæˆ‘ä»¬å¿…é¡»å¼„æ¸…æ¥šPrometheusï¼†amp; amp;çš„æ–¹æ³•ã€‚åœ¨Kubernetesä¸­æ­£ç¡®ä½¿ç”¨Grafanaã€‚å…³äºå¦‚ä½•å¯åŠ¨å’Œè¿è¡Œå®ƒï¼Œå­˜åœ¨ä¸€äº›ç›¸äº’çŸ›ç›¾çš„å»ºè®®ï¼Œå¹¶ä¸”æ–‡æ¡£å¯¹æˆ‘ä»¬ä¸èµ·ä½œç”¨ï¼Œå› æ­¤æˆ‘ä»¬è´¡çŒ®äº†æˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆã€‚åœ¨æ‰€æœ‰å¯ç”¨çš„æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬é€‰æ‹©äº†kube-prometheusè¿ç®—ç¬¦ï¼Œè¯¥è¿ç®—ç¬¦ä½¿æˆ‘ä»¬å¯ä»¥å¼€ç®±å³ç”¨åœ°æ´æ‚‰Kuberneteså’Œç³»ç»ŸæŒ‡æ ‡ã€‚æˆ‘ä»¬å°šæœªå°†kube-prometheusä¸å…¶ä»–æœåŠ¡é›†æˆï¼Œä¾‹å¦‚ingress-nginxï¼ŒPostgreSQLï¼ŒPhoenixç­‰ã€‚</p><p> As soon as we had  grafana.changelog.com in place and all baseline components worked well together, we moved to setting up a static instance of changelog.com. This would give us a clear picture of how a simple changelog.com app would work in various conditions (upgrades, loss of nodes, etc.). The resulting artefact was temporary -  a stepping stone - which kept things simple and allowed us to observe web request latencies and error rates in various failure scenarios. Our findings gave us confidence in LKE and we gave it the green light by  sharing our Linode customer story.</p><p> ä¸€æ—¦æˆ‘ä»¬å®‰è£…äº†grafana.changelog.comå¹¶ä¸”æ‰€æœ‰åŸºçº¿ç»„ä»¶éƒ½å¯ä»¥å¾ˆå¥½åœ°ååŒå·¥ä½œï¼Œæˆ‘ä»¬ä¾¿ç€æ‰‹å»ºç«‹äº†changelog.comçš„é™æ€å®ä¾‹ã€‚è¿™å°†ä½¿æˆ‘ä»¬æ¸…æ¥šåœ°äº†è§£ç®€å•çš„changelog.comåº”ç”¨åœ¨å„ç§æƒ…å†µä¸‹ï¼ˆå‡çº§ï¼ŒèŠ‚ç‚¹ä¸¢å¤±ç­‰ï¼‰å¦‚ä½•å·¥ä½œã€‚ç”±æ­¤äº§ç”Ÿçš„ä¼ªåƒæ˜¯æš‚æ—¶çš„-ä¸€å—å«è„šçŸ³-ä½¿äº‹æƒ…å˜å¾—ç®€å•ï¼Œå¹¶å…è®¸æˆ‘ä»¬è§‚å¯Ÿå„ç§å¤±è´¥æƒ…å†µä¸‹çš„Webè¯·æ±‚å»¶è¿Ÿå’Œé”™è¯¯ç‡ã€‚æˆ‘ä»¬çš„å‘ç°ä½¿æˆ‘ä»¬å¯¹LKEå……æ»¡ä¿¡å¿ƒï¼Œå¹¶é€šè¿‡åˆ†äº«æˆ‘ä»¬çš„Linodeå®¢æˆ·æ•…äº‹ä¸ºå®ƒå¼€äº†ç»¿ç¯ã€‚</p><p> We settled on  Crunchy Data PostgreSQL Operator for running our production PostgreSQL on K8S. Just as we have been running PostgreSQL on Docker for years with no issues, PostgreSQL on K8S via Crunchy is an even better experience from our perspective. We are happy with the outcome, even if we donâ€™t leverage all of Crunchy PostgreSQL features just yet.</p><p> æˆ‘ä»¬é€‰æ‹©äº†Crunchy Data PostgreSQL Operatoråœ¨K8Sä¸Šè¿è¡Œç”Ÿäº§PostgreSQLã€‚æ­£å¦‚æˆ‘ä»¬åœ¨Dockerä¸Šè¿è¡ŒPostgreSQLå¤šå¹´æ²¡æœ‰é—®é¢˜ä¸€æ ·ï¼Œä»æˆ‘ä»¬çš„è§’åº¦æ¥çœ‹ï¼Œé€šè¿‡Crunchyåœ¨K8Sä¸Šè¿è¡ŒPostgreSQLæ˜¯ä¸€ä¸ªæ›´å¥½çš„ä½“éªŒã€‚å³ä½¿æˆ‘ä»¬å°šæœªå……åˆ†åˆ©ç”¨Crunchy PostgreSQLçš„æ‰€æœ‰åŠŸèƒ½ï¼Œæˆ‘ä»¬ä¹Ÿå¯¹ç»“æœæ„Ÿåˆ°æ»¡æ„ã€‚</p><p> For zero-downtime automatic app updates, we chose  Keel. CircleCI continues to build, test &amp; publish container images to DockerHub. The decision that we have made for our 2019 setup to keep a clean separation between CI &amp; production paid off: we didnâ€™t have to change anything in this part of our workflow. We could build a new second half of the workflow based on Kubernetes, while continuing to run the existing Docker Swarm one. In the new workflow, when the changelog.com container image gets updated, DockerHub sends a webhook event to Keel, which is just another deployment running in LKE. In response to this event, Keel updates the changelog.com deployment because we always want to run the latest commit that passes the build &amp; test stages. Yes, we know that GitOps via  Flux or  ArgoCD is a more comprehensive and robust approach, but in this case we chose the simplest thing that works for us. Having said that,  this discussion with @fwiles makes us think that we should revisit Flux soon.</p><p> å¯¹äºé›¶åœæœºæ—¶é—´è‡ªåŠ¨åº”ç”¨ç¨‹åºæ›´æ–°ï¼Œæˆ‘ä»¬é€‰æ‹©äº†Keelã€‚ CircleCIç»§ç»­æ„å»ºï¼Œæµ‹è¯•å’Œæµ‹è¯•å°†å®¹å™¨æ˜ åƒå‘å¸ƒåˆ°DockerHubã€‚æˆ‘ä»¬ä¸º2019å¹´çš„å®‰è£…æ–¹æ¡ˆåšå‡ºçš„å†³å®šæ˜¯ä¿æŒCIä¸Aç”Ÿäº§å¾—åˆ°å›æŠ¥ï¼šæˆ‘ä»¬æ— éœ€åœ¨å·¥ä½œæµç¨‹çš„è¿™ä¸€éƒ¨åˆ†è¿›è¡Œä»»ä½•æ›´æ”¹ã€‚æˆ‘ä»¬å¯ä»¥åœ¨Kubernetesçš„åŸºç¡€ä¸Šæ„å»ºæ–°çš„å·¥ä½œæµç¨‹çš„ååŠéƒ¨åˆ†ï¼ŒåŒæ—¶ç»§ç»­è¿è¡Œç°æœ‰çš„Docker Swarmã€‚åœ¨æ–°çš„å·¥ä½œæµç¨‹ä¸­ï¼Œå½“changelog.comå®¹å™¨æ˜ åƒæ›´æ–°æ—¶ï¼ŒDockerHubå°†ä¸€ä¸ªwebhookäº‹ä»¶å‘é€ç»™Keelï¼Œè¿™åªæ˜¯LKEä¸­è¿è¡Œçš„å¦ä¸€ä¸ªéƒ¨ç½²ã€‚ä¸ºå“åº”æ­¤äº‹ä»¶ï¼ŒKeelæ›´æ–°äº†changelog.coméƒ¨ç½²ï¼Œå› ä¸ºæˆ‘ä»¬å§‹ç»ˆå¸Œæœ›è¿è¡Œé€šè¿‡æ„å»ºå’Œæµ‹è¯•çš„æœ€æ–°æäº¤ã€‚æµ‹è¯•é˜¶æ®µã€‚æ˜¯çš„ï¼Œæˆ‘ä»¬çŸ¥é“é€šè¿‡Fluxæˆ–ArgoCDçš„GitOpsæ˜¯ä¸€ç§æ›´å…¨é¢ï¼Œæ›´å¼ºå¤§çš„æ–¹æ³•ï¼Œä½†æ˜¯åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬é€‰æ‹©äº†æœ€é€‚åˆæˆ‘ä»¬çš„æ–¹æ³•ã€‚è¯è™½å¦‚æ­¤ï¼Œä¸@fwilesçš„è®¨è®ºä½¿æˆ‘ä»¬è®¤ä¸ºæˆ‘ä»¬åº”è¯¥å°½å¿«é‡æ–°è®¿é—®Fluxã€‚</p><p> Throughout our entire changelog.com on Kubernetes journey, Andrew, a Linode engineer, was always there to help with a recommendation, fix or simply bounce some ideas off. We talked about Kubernetes upgrades and helped make them better (first implementation of  linode-cli lke pool recycle would update all nodes at once, meaning significant downtime), kicked off  PROXY protocol support for LoadBalancer Services, and touched on persistent volumes performance. If there was one thing that captures our interaction with Andrew best,  it is this highly detailed &amp; clear example on how to configure PROXY protocol support on LKE.</p><p> åœ¨æˆ‘ä»¬æ•´ä¸ªKubernetesæ—…ç¨‹çš„changelog.comä¸Šï¼ŒLinodeå·¥ç¨‹å¸ˆAndrewæ€»æ˜¯åœ¨åœºä»¥æä¾›å»ºè®®ï¼Œä¿®æ­£æˆ–ç®€å•åœ°æå‡ºä¸€äº›æƒ³æ³•ã€‚æˆ‘ä»¬è®¨è®ºäº†Kubernetesçš„å‡çº§å¹¶å¸®åŠ©ä»–ä»¬æé«˜äº†æ€§èƒ½ï¼ˆé¦–å…ˆå®æ–½linode-cli lkeæ± å›æ”¶å°†ç«‹å³æ›´æ–°æ‰€æœ‰èŠ‚ç‚¹ï¼Œè¿™æ„å‘³ç€å¤§é‡çš„åœæœºæ—¶é—´ï¼‰ï¼Œå¯åŠ¨äº†å¯¹LoadBalancer Servicesçš„PROXYåè®®æ”¯æŒï¼Œå¹¶è°ˆåˆ°äº†æŒä¹…å·æ€§èƒ½ã€‚å¦‚æœæœ‰ä¸€ä»¶äº‹æœ€èƒ½è¯´æ˜æˆ‘ä»¬ä¸å®‰å¾·é²çš„äº’åŠ¨ï¼Œé‚£å°±æ˜¯è¿™ä¸ªé«˜åº¦è¯¦ç»†çš„å†…å®¹ã€‚å…³äºå¦‚ä½•åœ¨LKEä¸Šé…ç½®PROXYåè®®æ”¯æŒçš„æ¸…æ™°ç¤ºä¾‹ã€‚ </p><p>   All put together, the time that it took us to migrate from Docker Swarm to Kubernetes was around three weeks. Most of this time was spent experimenting and getting comfortable with the baseline components such as ingress-nginx, external-dns, cert-manager etc. Taking into account  how complex the K8S ecosystem is regarded to be, I would say that our changelog.com migration to LKE worked well.</p><p>ç»¼ä¸Šæ‰€è¿°ï¼Œæˆ‘ä»¬ä»Docker Swarmè¿ç§»åˆ°Kubernetesæ‰€èŠ±è´¹çš„æ—¶é—´å¤§çº¦æ˜¯ä¸‰å‘¨ã€‚è¿™æ®µæ—¶é—´ä¸­çš„å¤§éƒ¨åˆ†æ—¶é—´éƒ½èŠ±åœ¨è¯•éªŒå’Œé€‚åº”åŸºçº¿ç»„ä»¶ä¸Šï¼Œä¾‹å¦‚ingress-nginxï¼Œexternal-dnsï¼Œcert-managerç­‰ã€‚è€ƒè™‘åˆ°K8Sç”Ÿæ€ç³»ç»Ÿè¢«è®¤ä¸ºå¤šä¹ˆå¤æ‚ï¼Œæˆ‘æƒ³è¯´ä¸€ä¸‹æˆ‘ä»¬çš„changelog.comè¿ç§»åˆ°LKEæ•ˆæœå¾ˆå¥½ã€‚</p><p> There is room for improving the performance of persistent volumes in LKE. When we disabled the CDN, disk utilisation for the persistent volume which serves all our media went up to 100% and stayed there for the entire duration of this test scenario. As unlikely as this is to happen, a cold cache would mean high latencies for all media assets. The worst part is that serving the same files from disks local to the VMs is 44x faster than from persistent volumes (267MB/s vs 6MB/s). Even with Fastly fronting all changelog.com media, this is what cache misses mean for disk IO utilisation &amp; saturation:</p><p> åœ¨LKEä¸­ä»æœ‰æ”¹è¿›æŒä¹…å·æ€§èƒ½çš„ç©ºé—´ã€‚å½“æˆ‘ä»¬ç¦ç”¨CDNæ—¶ï¼Œä¸ºæˆ‘ä»¬æ‰€æœ‰åª’ä½“æä¾›æœåŠ¡çš„æŒä¹…å·çš„ç£ç›˜åˆ©ç”¨ç‡ä¸Šå‡åˆ°100ï¼…ï¼Œå¹¶åœ¨æ­¤æµ‹è¯•æ–¹æ¡ˆçš„æ•´ä¸ªè¿‡ç¨‹ä¸­ä¸€ç›´ä¿æŒåœ¨é‚£é‡Œã€‚å°½ç®¡è¿™ç§æƒ…å†µä¸å¤ªå¯èƒ½å‘ç”Ÿï¼Œä½†å†·ç¼“å­˜å°†æ„å‘³ç€æ‰€æœ‰åª’ä½“èµ„äº§çš„é«˜å»¶è¿Ÿã€‚æœ€ç³Ÿç³•çš„éƒ¨åˆ†æ˜¯ï¼Œä»VMæœ¬åœ°ç£ç›˜æä¾›ç›¸åŒæ–‡ä»¶çš„é€Ÿåº¦æ¯”æŒä¹…å·çš„é€Ÿåº¦å¿«44å€ï¼ˆ267MB / så’Œ6MB / sï¼‰ã€‚å³ä½¿å¿«é€Ÿæ”¾ç½®æ‰€æœ‰changelog.comä»‹è´¨ï¼Œè¿™ä¹Ÿæ„å‘³ç€é«˜é€Ÿç¼“å­˜æœªå‘½ä¸­å¯¹ç£ç›˜IOåˆ©ç”¨ç‡å’Œé¥±å’Œï¼š</p><p>  During our migration, we had HTTP verification configured for Letâ€™s Encrypt (LE) certificates. With the DNS TTL set to 3600 seconds, after the changelog.com DNS A record was updated to point to LKE, a new certificate could not be obtained because LE was still using the previous, cached changelog.com A record for verification. Having hit the LE certificate request limit, combined with the fact that we didnâ€™t change the default DNS TTL of 3600 in external-dns, we ended with about 30 minutes of partial downtime until the A record revert propagated through the DNS network. We have since switched to DNS-based verification for LE, and are now using a wildcard certificate, but this aspect of the migration did not work as well as it could have.</p><p>  åœ¨è¿ç§»è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä¸ºâ€œåŠ å¯†â€ï¼ˆLEï¼‰è¯ä¹¦é…ç½®äº†HTTPéªŒè¯ã€‚å°†DNS TTLè®¾ç½®ä¸º3600ç§’ï¼Œå°†changelog.com DNS Aè®°å½•æ›´æ–°ä¸ºæŒ‡å‘LKEä¹‹åï¼Œç”±äºLEä»åœ¨ä½¿ç”¨å…ˆå‰ç¼“å­˜çš„changelog.com Aè®°å½•è¿›è¡ŒéªŒè¯ï¼Œå› æ­¤æ— æ³•è·å¾—æ–°è¯ä¹¦ã€‚è¾¾åˆ°LEè¯ä¹¦è¯·æ±‚é™åˆ¶åï¼Œå†åŠ ä¸Šæˆ‘ä»¬æ²¡æœ‰æ›´æ”¹å¤–éƒ¨DNSä¸­é»˜è®¤çš„3600çš„DNS TTLï¼Œæˆ‘ä»¬ç»“æŸäº†å¤§çº¦30åˆ†é’Ÿçš„éƒ¨åˆ†åœæœºï¼Œç›´åˆ°Aè®°å½•è¿˜åŸé€šè¿‡DNSç½‘ç»œä¼ æ’­ã€‚æ­¤åï¼Œæˆ‘ä»¬å·²åˆ‡æ¢åˆ°LEçš„åŸºäºDNSçš„éªŒè¯ï¼Œå¹¶ä¸”ç°åœ¨ä½¿ç”¨é€šé…ç¬¦è¯ä¹¦ï¼Œä½†æ˜¯è¿ç§»çš„è¿™ä¸€æ–¹é¢æ— æ³•æ­£å¸¸è¿è¡Œã€‚</p><p> Linode Kubernetes Engine (LKE) worked as expected. The interaction with Linode was spot on, all improvements that were required for our migration have shipped in a timely manner, support was always responsive and progress was constant.  Thank you Linode!</p><p> Linode Kubernetes Engineï¼ˆLKEï¼‰æ­£å¸¸å·¥ä½œã€‚ä¸Linodeçš„äº’åŠ¨éå¸¸æ˜æ˜¾ï¼Œæˆ‘ä»¬è¿ç§»æ‰€éœ€çš„æ‰€æœ‰æ”¹è¿›å‡å·²åŠæ—¶äº¤ä»˜ï¼Œæ”¯æŒå§‹ç»ˆåœ¨å“åº”ä¹‹ä¸­ï¼Œå¹¶ä¸”ä¸æ–­å–å¾—è¿›æ­¥ã€‚è°¢è°¢Linodeï¼</p><p>  To begin with, we are still playing the manual upgrade game. Now that we are on LKE, we want to automate K8S upgrades, baseline components upgrades (ingress-nginx, cert-manager etc.) as well as app dependency upgrades (Erlang &amp; Elixir, PostgreSQL, etc.). We know that itâ€™s a tall order, but is now within our reach considering the primitives that the Kubernetes ecosystem unlocks. The first step will be to upgrade our existing K8S v1.17 to v1.18 so that we are running on a supported version, as well as all other components that had significant releases in the last 6 months ( cert-manager for example shipped 1.0 since we started using it).</p><p>  é¦–å…ˆï¼Œæˆ‘ä»¬ä»åœ¨ç©æ‰‹åŠ¨å‡çº§æ¸¸æˆã€‚ç°åœ¨ï¼Œæˆ‘ä»¬ä½¿ç”¨LKEï¼Œæˆ‘ä»¬å¸Œæœ›è‡ªåŠ¨åŒ–K8Så‡çº§ï¼ŒåŸºå‡†ç»„ä»¶å‡çº§ï¼ˆingress-nginxï¼Œcert-managerç­‰ï¼‰ä»¥åŠåº”ç”¨ç¨‹åºä¾èµ–é¡¹å‡çº§ï¼ˆErlangï¼†amp; Elixirï¼ŒPostgreSQLç­‰ï¼‰ã€‚æˆ‘ä»¬çŸ¥é“è¿™æ˜¯ä¸€ä¸ªè‰°å·¨çš„ä»»åŠ¡ï¼Œä½†è€ƒè™‘åˆ°Kubernetesç”Ÿæ€ç³»ç»Ÿè§£é”çš„åŸå§‹è¦ç´ ï¼Œç°åœ¨å·²ç»å¯ä»¥å®ç°ã€‚ç¬¬ä¸€æ­¥æ˜¯å°†ç°æœ‰çš„K8S v1.17å‡çº§åˆ°v1.18ï¼Œä»¥ä¾¿æˆ‘ä»¬åœ¨å—æ”¯æŒçš„ç‰ˆæœ¬ä»¥åŠæœ€è¿‘6ä¸ªæœˆä¸­å…·æœ‰é‡è¦ç‰ˆæœ¬çš„æ‰€æœ‰å…¶ä»–ç»„ä»¶ä¸Šè¿è¡Œï¼ˆä¾‹å¦‚cert-manager 1.0ï¼‰å› ä¸ºæˆ‘ä»¬å¼€å§‹ä½¿ç”¨å®ƒï¼‰ã€‚</p><p> We would very much like to address the persistent volume limited performance on LKE. Even if with some work we could migrate all media assets to S3, we would still have to contend with our PostgreSQL performance, as well as Prometheus which is highly dependent on disk performance.  The managed db service that Linode has on the 2020 roadmap might solve the PostgreSQL performance, and Grafana Cloud may be a solution to Prometheus, but all these approaches seem to be solving a potential LKE improvement by going outside of LKE, which doesnâ€™t feel like a step in the right direction. As an aside, I am wondering if our friends at Upbound -  hi Jared &amp; Dan ğŸ‘‹ğŸ» - would recommend  Rook for pooling local SSD storage for persistent volumes on LKE? ğŸ¤”</p><p> æˆ‘ä»¬éå¸¸æƒ³è§£å†³LKEä¸ŠæŒç»­çš„å®¹é‡å—é™æ€§èƒ½é—®é¢˜ã€‚å³ä½¿å¯ä»¥è¿›è¡Œä¸€äº›å·¥ä½œï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥å°†æ‰€æœ‰åª’ä½“èµ„äº§è¿ç§»åˆ°S3ï¼Œä½†æˆ‘ä»¬ä»ç„¶å¿…é¡»åº”å¯¹PostgreSQLçš„æ€§èƒ½ä»¥åŠé«˜åº¦ä¾èµ–äºç£ç›˜æ€§èƒ½çš„Prometheusã€‚ Linodeåœ¨2020å¹´è·¯çº¿å›¾ä¸Šæä¾›çš„æ‰˜ç®¡æ•°æ®åº“æœåŠ¡å¯èƒ½ä¼šè§£å†³PostgreSQLçš„æ€§èƒ½ï¼Œè€ŒGrafana Cloudå¯èƒ½æ˜¯Prometheusçš„è§£å†³æ–¹æ¡ˆï¼Œä½†æ˜¯æ‰€æœ‰è¿™äº›æ–¹æ³•ä¼¼ä¹éƒ½é€šè¿‡æ‘†è„±LKEæ¥è§£å†³æ½œåœ¨çš„LKEæ”¹è¿›ï¼Œè¿™å¹¶ä¸ä»¤äººæ„Ÿåˆ°å°±åƒæœæ­£ç¡®æ–¹å‘è¿ˆå‡ºçš„ä¸€æ­¥ã€‚é¡ºä¾¿è¯´ä¸€å¥ï¼Œæˆ‘æƒ³çŸ¥é“æˆ‘ä»¬åœ¨Upboundçš„æœ‹å‹-Jaredï¼†amp; DanğŸ‘‹ğŸ»-æ˜¯å¦ä¼šå»ºè®®Rookä¸ºLKEä¸Šçš„æŒä¹…å·åˆå¹¶æœ¬åœ°SSDå­˜å‚¨ï¼Ÿ ğŸ¤”</p><p> I am very much looking forward to integrating all our services with Grafana &amp; Prometheus, as well as Loki which is still on our TODO list from last year. We are missing many metrics that are likely to highlight areas of improvement, as well as problems in the making that we are simply not aware of. Centralised logging running on LKE, alongside metrics, would be very nice to have, especially since this would enable us to start deriving more business outside of the app, which is currently hand-rolled with PostgreSQL &amp; Elixir. While this approach works well, I know that we could do so much better, and take business insights into directions that right now are unfathomable.</p><p> æˆ‘éå¸¸æœŸå¾…å°†æˆ‘ä»¬çš„æ‰€æœ‰æœåŠ¡ä¸Grafanaï¼†amp;æ™®ç½—ç±³ä¿®æ–¯ï¼ˆPrometheusï¼‰ä»¥åŠä»å»å¹´èµ·ä»åœ¨TODOåå•ä¸­çš„Lokiã€‚æˆ‘ä»¬ç¼ºå°‘è®¸å¤šå¯èƒ½çªå‡ºæ”¹è¿›é¢†åŸŸçš„æŒ‡æ ‡ï¼Œä»¥åŠæˆ‘ä»¬æ ¹æœ¬ä¸çŸ¥é“çš„åˆ¶é€ é—®é¢˜ã€‚åœ¨LKEä¸Šä¸æŒ‡æ ‡ä¸€èµ·è¿è¡Œçš„é›†ä¸­æ—¥å¿—è®°å½•éå¸¸å¥½ï¼Œå°¤å…¶æ˜¯å› ä¸ºè¿™å°†ä½¿æˆ‘ä»¬èƒ½å¤Ÿå¼€å§‹åœ¨åº”ç”¨ç¨‹åºä¹‹å¤–æ´¾ç”Ÿæ›´å¤šä¸šåŠ¡ï¼Œè€Œè¯¥åº”ç”¨ç¨‹åºç›®å‰ç”±PostgreSQLï¼†amp;é•¿ç”Ÿä¸è€è¯ã€‚è™½ç„¶è¿™ç§æ–¹æ³•è¡Œä¹‹æœ‰æ•ˆï¼Œä½†æˆ‘çŸ¥é“æˆ‘ä»¬å¯ä»¥åšå¾—æ›´å¥½ï¼Œå¹¶å°†ä¸šåŠ¡æ´å¯ŸåŠ›å¸¦å…¥ç›®å‰æ— æ³•ä¼åŠçš„æ–¹å‘ã€‚ </p><p> Another thing on our radar is picking up  OpenFaaS for certain tasks that we still run outside of LKE, and for which we use Ruby scripting. While we could rewrite them to Elixir so that everything runs within the same app, it seems easier to lift and shift the Ruby code that has been working fine for years into one or more functions running on OpenFaaS. Well spotted, the hint was in the video above. ğŸ˜‰</p><p>æˆ‘ä»¬è¦æ³¨æ„çš„å¦ä¸€ä»¶äº‹æ˜¯ä¸ºä»åœ¨LKEä¹‹å¤–è¿è¡Œçš„æŸäº›ä»»åŠ¡é€‰æ‹©OpenFaaSï¼Œå¹¶ä¸”æˆ‘ä»¬ä½¿ç”¨Rubyè„šæœ¬ã€‚å°½ç®¡æˆ‘ä»¬å¯ä»¥å°†å®ƒä»¬é‡å†™ä¸ºElixirï¼Œä»¥ä¾¿æ‰€æœ‰å†…å®¹éƒ½å¯ä»¥åœ¨åŒä¸€åº”ç”¨ç¨‹åºä¸­è¿è¡Œï¼Œä½†å°†å¤šå¹´æ¥è¿è¡Œè‰¯å¥½çš„Rubyä»£ç æå‡å’Œè½¬ç§»åˆ°åœ¨OpenFaaSä¸Šè¿è¡Œçš„ä¸€ä¸ªæˆ–å¤šä¸ªå‡½æ•°ä¸­ä¼¼ä¹æ›´åŠ å®¹æ˜“ã€‚å¾ˆå¥½å‘ç°ï¼Œæç¤ºåœ¨ä¸Šé¢çš„è§†é¢‘ä¸­ã€‚ ğŸ˜‰</p><p> As always, our focus is on steady improvement. I feel that what we delivered in 2020 is a significant improvement to what we had before. I am really looking forward to what we get to build for 2021, as well as sharing it with you all!</p><p> ä¸å¾€å¸¸ä¸€æ ·ï¼Œæˆ‘ä»¬çš„é‡ç‚¹æ˜¯ç¨³æ­¥æ”¹å–„ã€‚æˆ‘è§‰å¾—æˆ‘ä»¬åœ¨2020å¹´äº¤ä»˜çš„äº§å“ä¸ä»¥å‰ç›¸æ¯”æœ‰äº†å¾ˆå¤§çš„æ”¹è¿›ã€‚æˆ‘éå¸¸æœŸå¾…æˆ‘ä»¬ä¸º2021å¹´å»ºç«‹çš„åŸºç¡€ï¼Œå¹¶ä¸å¤§å®¶åˆ†äº«ï¼</p><p>     changelog.com availability SLO is 4 nines, (99.99%) which translates to just over 50 minutes of downtime in one year. Our 2019 availability SLI was 3 nines and a 6 (99.96%) with over 220 minutes of downtime. A misconfigured Docker service was the main reason, but we also had 50 micro downtimes of a minute or two.</p><p>     changelog.comçš„å¯ç”¨æ€§SLOä¸º4ä¸ª9ï¼ˆ99.99ï¼…ï¼‰ï¼Œè¿™æ„å‘³ç€ä¸€å¹´å†…åœæœºæ—¶é—´ä»…è¶…è¿‡50åˆ†é’Ÿã€‚æˆ‘ä»¬çš„2019å¹´å¯ç”¨æ€§SLIä¸º3ä¸ª9å’Œ6ï¼ˆ99.96ï¼…ï¼‰ï¼Œåœæœºæ—¶é—´è¶…è¿‡220åˆ†é’Ÿã€‚ DockeræœåŠ¡é…ç½®é”™è¯¯æ˜¯ä¸»è¦åŸå› ï¼Œä½†æˆ‘ä»¬ä¹Ÿæœ‰50åˆ†é’Ÿæˆ–ä¸€ä¸¤åˆ†é’Ÿçš„å¾®åœæœºæ—¶é—´ã€‚</p><p> With an ongoing LKE migration, our 2020 availability SLI stands at 4 nines, with 38 minutes of downtime currently. Most of it was due to my DNS &amp; LE certificate blunder hit during the migration to LKE, but with 2 months to go, our availability SLI is almost 6 times better this year compared to the previous one.</p><p> éšç€LKEçš„æŒç»­è¿ç§»ï¼Œæˆ‘ä»¬çš„2020å¹´å¯ç”¨æ€§SLIä¸º4ä¸ª9ï¼Œç›®å‰åœæœºæ—¶é—´ä¸º38åˆ†é’Ÿã€‚å¤§éƒ¨åˆ†æ˜¯ç”±äºæˆ‘çš„DNSï¼†amp;åœ¨è¿ç§»åˆ°LKEæœŸé—´ï¼ŒLEè¯ä¹¦å‡ºç°äº†ä¸¥é‡é”™è¯¯ï¼Œä½†æ˜¯è¿˜æœ‰2ä¸ªæœˆçš„æ—¶é—´ï¼Œä»Šå¹´æˆ‘ä»¬çš„å¯ç”¨æ€§SLIæ¯”ä¸Šä¸€ä¸ªè¦å¥½6å€ã€‚ </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://changelog.com/posts/the-new-changelog-setup-for-2020">https://changelog.com/posts/the-new-changelog-setup-for-2020</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/2020/">#2020</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/è®¾ç½®/">#è®¾ç½®</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/setup/">#setup</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è®¾è®¡/">#è®¾è®¡</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/åˆ›æ„/">#åˆ›æ„</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/æ‘„å½±/">#æ‘„å½±</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/æ¸¸æˆ/">#æ¸¸æˆ</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/å›¾ç‰‡/">#å›¾ç‰‡</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è½¯ä»¶/">#è½¯ä»¶</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è§†é¢‘/">#è§†é¢‘</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/æ‰‹æœº/">#æ‰‹æœº</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/å¹¿å‘Š/">#å¹¿å‘Š</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/ç½‘ç«™/">#ç½‘ç«™</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/å…è´¹/">#å…è´¹</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/ä¸‹è½½/">#ä¸‹è½½</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è‹¹æœ/">#è‹¹æœ</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/å¾®è½¯/">#å¾®è½¯</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/éŸ³ä¹/">#éŸ³ä¹</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/åšå®¢/">#åšå®¢</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/æ¶æ/">#æ¶æ</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è‰ºæœ¯/">#è‰ºæœ¯</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/å·¥å…·/">#å·¥å…·</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/åˆ†äº«/">#åˆ†äº«</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>