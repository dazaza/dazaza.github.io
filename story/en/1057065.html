<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>æˆ‘ä»¬å°†Github APIç¼©æ”¾äº†Redisä¸­çš„åˆ†ç‰‡ï¼Œå¤åˆ¶çš„é€Ÿç‡é™åˆ¶å™¨ We scaled the GitHub API with a sharded, replicated rate limiter in Redis</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">We scaled the GitHub API with a sharded, replicated rate limiter in Redis<br/>æˆ‘ä»¬å°†Github APIç¼©æ”¾äº†Redisä¸­çš„åˆ†ç‰‡ï¼Œå¤åˆ¶çš„é€Ÿç‡é™åˆ¶å™¨ </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-04-09 00:01:43</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/4/50b167e95326ddd1d22749c631dcdc2a.png"><img src="http://img2.diglog.com/img/2021/4/50b167e95326ddd1d22749c631dcdc2a.png" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>About a year ago, we migrated an old rate limiter in order to serve more traffic and accommodate a more resilient platform architecture. We adopted a replicated Redis backend with client-side sharding. In the end, it worked out great, but we learned some lessons along the way.</p><p>å¤§çº¦ä¸€å¹´å‰ï¼Œæˆ‘ä»¬è¿ç§»äº†ä¸€ä¸ªæ—§çš„é€Ÿç‡é™åˆ¶å™¨ï¼Œä»¥ä¾¿ä¸ºæ›´å¤šçš„æµé‡æä¾›æœåŠ¡ï¼Œå¹¶é€‚åº”æ›´å…·å¼¹æ€§çš„å¹³å°æ¶æ„ã€‚æˆ‘ä»¬é‡‡ç”¨äº†ä¸€ä¸ªå¸¦å®¢æˆ·ç«¯åˆ†çº§çš„å¤åˆ¶redisåç«¯ã€‚æœ€åï¼Œå®ƒçš„æ•ˆæœå¾ˆæ£’ï¼Œä½†æˆ‘ä»¬æ²¿é€”å­¦åˆ°äº†ä¸€äº›æ•™è®­ã€‚</p><p>   In Memcached, increment the value of that key, setting it to 1 if there wasnâ€™t any current value</p><p>   åœ¨memcachedä¸­ï¼Œå°†è¯¥é”®çš„å€¼é€’å¢ï¼Œå¦‚æœæ²¡æœ‰ä»»ä½•å½“å‰å€¼ï¼Œå°†å…¶è®¾ç½®ä¸º1</p><p> Also, if there wasnâ€™t already one, set a â€œreset atâ€ value in Memcached, using a related key (eg, â€œ #{key}:reset_atâ€œ)</p><p> æ­¤å¤–ï¼Œå¦‚æœå°šæœªå­˜åœ¨ä¸€ä¸ªï¼Œè¯·ä½¿ç”¨ç›¸å…³é”®ï¼ˆä¾‹å¦‚ï¼Œâ€œï¼ƒ{key}ï¼šreset_atâ€ï¼‰è®¾ç½®â€œåœ¨â€memcachedä¸­çš„å€¼â€œé‡ç½®â€å€¼ã€‚</p><p> When incrementing, if the â€œreset atâ€ value is in the past, ignore the existing value and set a new â€œreset atâ€</p><p> é€’å¢æ—¶ï¼Œå¦‚æœè¿‡å»çš„â€œé‡ç½®â€å€¼ï¼Œè¯·å¿½ç•¥ç°æœ‰å€¼å¹¶è®¾ç½®æ–°çš„â€œå¤ä½â€</p><p> At the beginning of each request, if the value for the key is above the limit, and â€œreset atâ€ is in the future, then reject the request</p><p> åœ¨æ¯ä¸ªè¯·æ±‚çš„å¼€å¤´ï¼Œå¦‚æœå¯†é’¥çš„å€¼é«˜äºé™åˆ¶ï¼Œå¹¶ä¸”å°†æ¥â€œé‡ç½®ä¸ºâ€ï¼Œç„¶åæ‹’ç»è¯·æ±‚</p><p> (There might have been more nuance to it, but thatâ€™s the main idea.)</p><p> ï¼ˆå¯èƒ½å¯¹å®ƒæœ‰æ›´å¤šç»†å¾®å·®åˆ«ï¼Œä½†è¿™æ˜¯ä¸»è¦çš„æƒ³æ³•ã€‚ï¼‰</p><p>  Our Memcached architecture was due to change. Since it was  mostly used as a caching layer, we were going to switch from a single, shared Memcached to one Memcached per datacenter. Although thatâ€™d work fine for application caching, it would cause our rate limiter to behave very strangely if client requests were routed to different data centers.</p><p>  æˆ‘ä»¬çš„Memcachedæ¶æ„æ˜¯ç”±äºå˜åŒ–ã€‚ç”±äºå®ƒå¤§å¤šæ•°ç”¨ä½œç¼“å­˜å±‚ï¼Œå› æ­¤æˆ‘ä»¬å°†ä»å•ä¸ªå…±äº«MEMCACHEDä»å•ä¸ªå…±äº«çš„MEMCACHEåˆ‡æ¢åˆ°æ¯ä¸ªæ•°æ®ä¸­å¿ƒçš„ä¸€ä¸ªMEMCACHEDã€‚è™½ç„¶é€‚ç”¨äºåº”ç”¨ç¨‹åºç¼“å­˜ï¼Œä½†å¦‚æœå®¢æˆ·è¦æ±‚è¢«è·¯ç”±åˆ°ä¸åŒçš„æ•°æ®ä¸­å¿ƒï¼Œåˆ™ä¼šå¯¼è‡´æˆ‘ä»¬çš„é€Ÿç‡é™åˆ¶å™¨éå¸¸å¥‡æ€ªã€‚ </p><p> Memcached â€œpersistenceâ€ wasnâ€™t working for us. The Memcached backend was shared by the rate limiter and other application caches which meant that, when it filled up, it would sometimes evict rate limiter data, even when it was still active. (As a result, clients would get â€œfreshâ€ rate limit windows when they shouldnâ€™t. Sometimes, only  one key would be evicted â€“ theyâ€™d keep the same â€œusedâ€ value, but get new, future, â€œreset atâ€ values!)</p><p>Memcachedâ€œæŒä¹…æ€§â€ä¸é€‚åˆæˆ‘ä»¬ã€‚ Memcachedåç«¯ç”±é€Ÿç‡é™åˆ¶å™¨å’Œå…¶ä»–åº”ç”¨ç¨‹åºç¼“å­˜å…±äº«ï¼Œè¿™æ„å‘³ç€ï¼Œå½“å®ƒå¡«æ»¡æ—¶ï¼Œå®ƒæœ‰æ—¶ä¼šé˜»æ­¢é€Ÿç‡é™åˆ¶å™¨æ•°æ®ï¼Œå³ä½¿å®ƒä»ç„¶å¤„äºæ´»åŠ¨çŠ¶æ€ã€‚ ï¼ˆç»“æœï¼Œå®¢æˆ·ç«¯å°†è·å¾—â€œæ–°é²œâ€çš„é€Ÿç‡é™åˆ¶çª—å£ã€‚æœ‰æ—¶ï¼Œåªæœ‰ä¸€ä¸ªå…³é”®å°±ä¼šè¢«é©±é€ - ä»–ä»¬ä¼šä¿æŒç›¸åŒçš„â€œä½¿ç”¨â€ä»·å€¼ï¼Œä½†è·å¾—æ–°çš„æœªæ¥ï¼Œâ€œé‡ç½®â€ä»·å€¼è§‚ï¼ï¼‰</p><p>   Use Redis, since it has a more appropriate persistence system and simple sharding and replication setups</p><p>   ä½¿ç”¨Redisï¼Œå› ä¸ºå®ƒå…·æœ‰æ›´åˆé€‚çš„æŒä¹…æ€§ç³»ç»Ÿå’Œç®€å•çš„åˆ†ç‰‡å’Œå¤åˆ¶è®¾ç½®</p><p> Shard inside the application: the app would pick, for each key, which Redis cluster to read and write from</p><p> åº”ç”¨ç¨‹åºå†…çš„ç¢ç‰‡ï¼šè¯¥åº”ç”¨ç¨‹åºå°†ä¸ºæ¯ä¸ªå¯†é’¥é€‰æ‹©ï¼Œredisç¾¤é›†è¯»å–å’Œå†™å…¥</p><p> To mitigate the CPU-bound nature of Redis, put a single primary (for writes) and several replicas (for reads) in each cluster</p><p> è¦ç¼“è§£REDISçš„CPUç»‘å®šæ€§è´¨ï¼Œè¯·åœ¨æ¯ä¸ªç¾¤é›†ä¸­æ”¾ç½®å•ä¸ªä¸»è¦ï¼ˆç¼–å†™ï¼‰å’Œå‡ ä¸ªå‰¯æœ¬ï¼ˆå¯¹äºè¯»å–ï¼‰</p><p> Instead of writing â€œreset atâ€ in the database, use Redis expiration to make values disappear when they no longer apply</p><p> è€Œä¸æ˜¯åœ¨æ•°æ®åº“ä¸­å†™å…¥â€œé‡ç½®â€ï¼Œè€Œæ˜¯åœ¨ä¸å†é€‚ç”¨çš„æƒ…å†µä¸‹ä½¿ç”¨redisåˆ°æœŸæ¥ä½¿å€¼æ¶ˆå¤±</p><p> Implement the storage logic in Lua, to guarantee atomicity of operations (this was an improvement over the previous design)</p><p> åœ¨Luaå®ç°å­˜å‚¨é€»è¾‘ï¼Œä¿è¯æ“ä½œçš„åŸå­æ€§ï¼ˆè¿™æ˜¯å¯¹ä»¥å‰çš„è®¾è®¡æ”¹è¿›ï¼‰</p><p> One option we  considered but decided against was using our MySQL-backed KV store ( GitHub::KV) for storage. We didnâ€™t want to add traffic to already-busy MySQL primaries: usually, we use replicas for GET requests, but rate limit updates would require write access to a primary. By choosing a different storage backend, we could avoid the additional (and substantial) write traffic to MySQL.</p><p> æˆ‘ä»¬è€ƒè™‘çš„ä¸€ä¸ªé€‰é¡¹ï¼Œä½†å†³å®šä½¿ç”¨æˆ‘ä»¬çš„MySQLæ”¯æŒçš„KVå•†åº—ï¼ˆGitHub :: kVï¼‰è¿›è¡Œå­˜å‚¨ã€‚æˆ‘ä»¬ä¸æƒ³å‘å·²ç»å¿™ç¢Œçš„MySQLåŸåºæ·»åŠ æµé‡ï¼šé€šå¸¸ï¼Œæˆ‘ä»¬ä½¿ç”¨å‰¯æœ¬è·å–è¯·æ±‚ï¼Œä½†é€Ÿç‡é™åˆ¶æ›´æ–°éœ€è¦å¯¹ä¸»è¦çš„å†™è®¿é—®æƒé™ã€‚é€šè¿‡é€‰æ‹©ä¸åŒçš„å­˜å‚¨åç«¯ï¼Œæˆ‘ä»¬å¯ä»¥é¿å…å‘MySQLæä¾›é¢å¤–çš„ï¼ˆå’Œå®è´¨æ€§ï¼‰ã€‚ </p><p> Another advantage to using Redis is that itâ€™s a well-traveled path. We could take inspiration from two excellent existing resources:</p><p>ä½¿ç”¨Redisçš„å¦ä¸€ä¸ªä¼˜ç‚¹æ˜¯å®ƒæ˜¯ä¸€ä¸ªè‰¯å¥½çš„è·¯å¾„ã€‚æˆ‘ä»¬å¯ä»¥ä»ä¸¤ä¸ªä¼˜ç§€çš„ç°æœ‰èµ„æºä¸­è·å–çµæ„Ÿï¼š</p><p> Stripeâ€™s technical blog post, â€œ Scaling your API with Rate Limitersâ€, which includes a Ruby and Redis  example implementation</p><p> Stripeçš„æŠ€æœ¯åšå®¢æ–‡ç« â€œç”¨é€Ÿç‡é™åˆ¶ä»ªç¼©æ”¾æ‚¨çš„APIâ€ï¼Œå…¶ä¸­åŒ…æ‹¬Rubyå’ŒRedisç¤ºä¾‹å®ç°</p><p>  To roll out this change, we isolated the current persistence logic into a  MemcachedBackend class, and built a new  RedisBackend class for the rate limiter. We used a feature flag to gate access to the new backend. This allowed us to gradually increase the percentage of clients using the new backend. We could change the percentage  without a deploy, which meant, if something went wrong, we could quickly switch back to the old implementation.</p><p>  è¦æ¨å‡ºæ­¤æ›´æ”¹ï¼Œæˆ‘ä»¬å°†å½“å‰çš„æŒä¹…æ€§é€»è¾‘åˆ†ç¦»ä¸ºMemcachedBackendç±»ï¼Œå¹¶ä¸ºé€Ÿç‡é™åˆ¶å™¨æ„å»ºäº†ä¸€ä¸ªæ–°çš„RedisBackendç±»ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªåŠŸèƒ½æ ‡å¿—æ¥é—¨è®¿é—®æ–°çš„åç«¯ã€‚è¿™å…è®¸æˆ‘ä»¬ä½¿ç”¨æ–°çš„åç«¯é€æ­¥å¢åŠ å®¢æˆ·çš„ç™¾åˆ†æ¯”ã€‚æˆ‘ä»¬å¯ä»¥æ”¹å˜æ²¡æœ‰éƒ¨ç½²çš„ç™¾åˆ†æ¯”ï¼Œè¿™æ„å‘³ç€ï¼Œå¦‚æœå‡ºç°é—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥å¿«é€Ÿåˆ‡æ¢å›æ—§çš„å®ç°ã€‚</p><p> The release went smoothly, and when it was done, we removed the feature flag and the  MemcachedBackend class, and integrated  RedisBackend directly with the  Throttler class that delegated to it.</p><p> è¯¥å‘å¸ƒé¡ºåˆ©è¿›è¡Œï¼Œå½“å®Œæˆåï¼Œæˆ‘ä»¬åˆ é™¤äº†åŠŸèƒ½æ ‡å¿—å’ŒMemcachedBackendç±»ï¼Œå¹¶ç›´æ¥ä¸å§”æ‰˜ç»™å®ƒçš„Throttlerç±»é›†æˆäº†RedisBackendã€‚</p><p>   A lot of integrators watch their rate limit usage very closely. We got two really interesting bug reports in the weeks following our release:</p><p>   å¾ˆå¤šé›†æˆå•†å¯†åˆ‡å…³æ³¨ä»–ä»¬çš„é€Ÿç‡é™åˆ¶ä½¿ç”¨ã€‚åœ¨æˆ‘ä»¬å‘å¸ƒåçš„å‡ å‘¨å†…ï¼Œæˆ‘ä»¬æœ‰ä¸¤ä¸ªéå¸¸æœ‰è¶£çš„é”™è¯¯æŠ¥å‘Šï¼š</p><p> Some clients observed that their  X-RateLimit-Reset header value â€œwobbledâ€ â€“ it might show  2020-01-01 10:00:00 for one request, but  2020-01-01 10:00:01 on another request (with one second difference).</p><p> ä¸€äº›å®¢æˆ·è§‚å¯Ÿåˆ°ä»–ä»¬çš„x-RATELIMIT-RESETæ ‡é¢˜å€¼â€œæ‘†åŠ¨â€ - å®ƒå¯èƒ½ä¼šæ˜¾ç¤ºä¸€ä¸ªè¯·æ±‚çš„10:00:00ï¼Œä½†å¦ä¸€ä¸ªè¯·æ±‚ï¼ˆå¸¦ä¸€ä¸ªï¼‰ç¬¬äºŒåŒºåˆ«ï¼‰ã€‚</p><p> Some clients had their requests  rejected for being over the limit, but the response headers said  X-RateLimit-Remaining: 5000. That didnâ€™t make sense: if they had a full rate limit window ahead of them, why was the request rejected?</p><p> ä¸€äº›å®¢æˆ·çš„è¦æ±‚æ‹’ç»è¿‡åº¦é™åˆ¶ï¼Œä½†å“åº”æ ‡é¢˜è¡¨ç¤ºX-RATELIMIT  - å‰©ä½™æ—¶é—´ï¼š5000.è¿™æ²¡æœ‰æ„ä¹‰ï¼šå¦‚æœä»–ä»¬åœ¨ä»–ä»¬é¢å‰æœ‰ä¸€ä¸ªå…¨é¢é™åˆ¶çª—å£ï¼Œä¸ºä»€ä¹ˆè¯·æ±‚è¢«æ‹’ç»ï¼Ÿ </p><p>   We were optimistic about using Redisâ€™s built-in time-to-live (TTL) to implement our â€œreset atâ€ feature. But it turns out, my implementation caused the â€œwobbleâ€ described above.</p><p>æˆ‘ä»¬å¯¹ä½¿ç”¨Redisçš„å†…ç½®æ—¶é—´åˆ°Liveï¼ˆTTLï¼‰æŒä¹è§‚æ€åº¦ï¼Œä»¥å®ç°æˆ‘ä»¬çš„â€œé‡ç½®â€åŠŸèƒ½ã€‚ä½†äº‹å®è¯æ˜ï¼Œæˆ‘çš„å®ç°å¯¼è‡´ä¸Šé¢æè¿°çš„â€œæ‘†åŠ¨â€ã€‚</p><p> The Lua script returned the TTL of the clientâ€™s rate limit value, and then in Ruby, it was added to  Time.now.to_i to get a timestamp for the  X-RateLimit-Reset header. The problem was, time  passes between the call to TTL (in Redis) and  Time.now.to_i (in Ruby). Depending exactly how much time, and where it fell on the clockâ€™s second boundary, the resulting timestamp might be different. For example, consider the following calls:</p><p> Luaè„šæœ¬è¿”å›äº†å®¢æˆ·ç«¯çš„é€Ÿç‡é™åˆ¶å€¼çš„TTLï¼Œç„¶ååœ¨Rubyä¸­ï¼Œå®ƒè¢«æ·»åŠ åˆ°Time.Now.to_Iè·å–X-RATELIMITå¤ä½æ ‡é¢˜çš„æ—¶é—´æˆ³ã€‚é—®é¢˜æ˜¯ï¼Œæ—¶é—´åœ¨TTLï¼ˆredisä¸­ï¼‰å’Œtime.now.to_iï¼ˆåœ¨Rubyï¼‰ä¹‹é—´ä¼ é€’ã€‚æ ¹æ®éœ€è¦å¤šå°‘æ—¶é—´ï¼Œå¹¶ä¸”åœ¨æ—¶é’Ÿçš„ç¬¬äºŒä¸ªè¾¹ç•Œä¸Šè½ä¸‹çš„ä½ç½®ï¼Œæ‰€äº§ç”Ÿçš„æ—¶é—´æˆ³å¯èƒ½æ˜¯ä¸åŒçš„ã€‚ä¾‹å¦‚ï¼Œè€ƒè™‘ä»¥ä¸‹å‘¼å«ï¼š</p><p>  In that case, since the second boundary happened  between the call to TTL and  Time.now, the resulting timestamp was one second  bigger than the previous ones.</p><p>  åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç”±äºç¬¬äºŒä¸ªè¾¹ç•Œå‘ç”Ÿåœ¨TTLå’ŒTime.Nowä¹‹é—´ï¼Œå› æ­¤å¾—åˆ°çš„æ—¶é—´æˆ³æ¯”å‰ä¸€ä¸ªæ›´å¤§çš„æ—¶é—´ã€‚</p><p> We could have tried increasing the precision of this operation (eg, Redis PTTL), but there would  still have been some wobble, even if it was greatly reduced.</p><p> æˆ‘ä»¬æœ¬å¯ä»¥å°è¯•æé«˜æ­¤æ“ä½œçš„ç²¾åº¦ï¼ˆä¾‹å¦‚ï¼ŒRedis PTTLï¼‰ï¼Œä½†å³ä½¿å®ƒè¢«å¤§å¤§é™ä½ï¼Œä»ç„¶å­˜åœ¨ä¸€äº›æ‘†åŠ¨ã€‚</p><p> Another possibility was to calculate the time using  only Redis, instead of mixing Ruby and Redis calls to create it. Redisâ€™s  TIME command could have been used as the source of truth. (Old Redis versions didnâ€™t allow  TIME in Lua scripts, but Redis 5+ does.) We avoided this design because it would have been harder to test: by using Rubyâ€™s time as the source of truth, I could time-travel in my tests with Timecop, asserting that expired keys were handled correctly without actually  waiting for Redisâ€™s calls to the system clock to return true, future times. (I still had to wait on Redis to test the  EXPIRE-based database cleanup, but since  expires_at came from Ruby-land, I could inject very short expiration windows to simplify testing.)</p><p> å¦ä¸€ç§å¯èƒ½æ€§æ˜¯ä»…ä½¿ç”¨Redisè®¡ç®—æ—¶é—´ï¼Œè€Œä¸æ˜¯æ··åˆRubyå’ŒRediså‘¼å«æ¥åˆ›å»ºå®ƒã€‚ redisçš„æ—¶é—´å‘½ä»¤å¯èƒ½è¢«ç”¨ä½œçœŸç†æ¥æºã€‚ ï¼ˆæ—§çš„redisç‰ˆæœ¬æ²¡æœ‰æ—¶é—´åœ¨Luaè„šæœ¬ä¸­å…è®¸æ—¶é—´ï¼Œä½†redis 5+ç¡®å®å¦‚æ­¤ã€‚ï¼‰æˆ‘ä»¬é¿å…äº†è¿™ç§è®¾è®¡ï¼Œå› ä¸ºå®ƒä¼šæ›´åŠ éš¾ä»¥æµ‹è¯•ï¼šé€šè¿‡ä½¿ç”¨Rubyçš„æ—¶é—´ä½œä¸ºçœŸç†çš„æ¥æºï¼Œæˆ‘å¯ä»¥åœ¨æˆ‘çš„æ—¶é—´é‡Œæ—…è¡Œä½¿ç”¨TimeCopè¿›è¡Œæµ‹è¯•ï¼Œæ–­è¨€å·²è¿‡æœŸå¯†é’¥ï¼Œæ— éœ€å®é™…ç­‰å¾…Rediså¯¹ç³»ç»Ÿæ—¶é’Ÿè¿”å›Trueï¼Œæœªæ¥æ—¶é—´çš„å‘¼å«ã€‚ ï¼ˆæˆ‘ä»ç„¶ä¸å¾—ä¸ç­‰å¾…Redisæ¥æµ‹è¯•åŸºäºè¿‡æœŸçš„æ•°æ®åº“æ¸…ç†ï¼Œä½†æ˜¯ç”±äºExpires_atæ¥è‡ªRuby-Landï¼Œæˆ‘å¯ä»¥æ³¨å…¥éå¸¸çŸ­çš„åˆ°æœŸçª—å£æ¥ç®€åŒ–æµ‹è¯•ã€‚ï¼‰</p><p> Instead, we decided to  persist the â€œreset atâ€ time from Ruby in the database. That way, we could be sure it wouldnâ€™t wobble. (Wobbling was an effect of the  calculation â€“ but reading from the database would guarantee a stable value.) Instead of reading TTL from Redis, we stored another value in the database (effectively doubling our storage footprint, but OK).</p><p> ç›¸åï¼Œæˆ‘ä»¬å†³å®šæŒç»­åˆ°æ•°æ®åº“ä¸­çš„Rubyçš„â€œé‡ç½®â€æ—¶é—´ã€‚è¿™æ ·ï¼Œæˆ‘ä»¬å¯ä»¥ç¡®å®šå®ƒä¸ä¼šæ‘†åŠ¨ã€‚ ï¼ˆWobblingæ˜¯è®¡ç®—çš„æ•ˆæœ - ä½†ä»æ•°æ®åº“ä¸­çš„è¯»å–å°†ä¿è¯ç¨³å®šçš„å€¼ã€‚ï¼‰è€Œä¸æ˜¯ä»Redisè¯»å–TTLï¼Œæˆ‘ä»¬åœ¨æ•°æ®åº“ä¸­å­˜å‚¨äº†å¦ä¸€ä¸ªå€¼ï¼ˆæœ‰æ•ˆåœ°åŠ å€æˆ‘ä»¬çš„å­˜å‚¨ç©ºé—´ï¼Œä½†æ˜¯ç¡®å®šï¼‰ã€‚</p><p> We still applied a TTL to rate limit keys, but they were set for one second  after the â€œreset atâ€ time. That way, we could use Redisâ€™s own semantics to clean up â€œdeadâ€ rate limit windows.</p><p> æˆ‘ä»¬ä»ç„¶å°†TTLåº”ç”¨äºé€Ÿç‡é™åˆ¶é”®ï¼Œä½†åœ¨â€œåœ¨â€æ—¶é—´â€œæ—¶ï¼Œå®ƒä»¬è¢«è®¾ç½®ä¸ºä¸€ç§’é’Ÿã€‚è¿™æ ·ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨Redisçš„è‡ªå·±çš„è¯­ä¹‰æ¸…ç†â€œæ­»äº¡â€é€Ÿç‡é™åˆ¶çª—å£ã€‚ </p><p>  Weirdly, many clients reported  rejections that included  X-RateLimit-Remaining: 5000 headers. Whatâ€™s going on!?</p><p>å¥‡æ€ªåœ°ï¼Œè®¸å¤šå®¢æˆ·æŠ¥å‘Šæ‹’ç»åŒ…å«X-RATELIMITçš„æ‹’ç»ï¼š5000ä¸ªæ ‡é¢˜ã€‚è¿™æ˜¯æ€ä¹ˆå›äº‹ï¼ï¼Ÿ</p><p>  At the beginning of the request, check the clientâ€™s current rate limit value. If itâ€™s over the maximum allowed limit, prepare a rejection response.</p><p>  åœ¨è¯·æ±‚çš„å¼€å¤´ï¼Œæ£€æŸ¥å®¢æˆ·çš„å½“å‰é€Ÿç‡é™åˆ¶å€¼ã€‚å¦‚æœå®ƒè¶…è¿‡æœ€å¤§å…è®¸çš„é™åˆ¶ï¼Œåˆ™å‡†å¤‡æ‹’ç»å“åº”ã€‚</p><p> Before delivering the response, increment the current rate limit value, and use the response to populate the  X-RateLimit-... headers.</p><p> åœ¨æä¾›å“åº”ä¹‹å‰ï¼Œå°†å½“å‰é€Ÿç‡é™åˆ¶å€¼é€’å¢ï¼Œå¹¶ä½¿ç”¨å“åº”å¡«å……X-RATELIMIT -...æ ‡é¢˜ã€‚</p><p> Well, it turned out that Step 1 above hit a Redis  replica, since it was a read operation. The read operation returned information about the clientâ€™s previous window, and the application prepared a rejection response.</p><p> å—¯ï¼Œäº‹å®è¯æ˜ï¼Œä¸Šé¢çš„æ­¥éª¤1å‡»ä¸­äº†Rediså‰¯æœ¬ï¼Œå› ä¸ºå®ƒæ˜¯è¯»å–æ“ä½œã€‚è¯»å–æ“ä½œè¿”å›äº†æœ‰å…³å®¢æˆ·ç«¯ä¸Šä¸€ä¸ªçª—å£çš„ä¿¡æ¯ï¼Œå¹¶ä¸”åº”ç”¨ç¨‹åºå‡†å¤‡äº†æ‹’ç»å“åº”ã€‚</p><p> Then, Step 2 would hit a Redis  primary. During that database call, Redis would expire the previous window data and return data for a  fresh rate limit. This is a known limitation of Redis: replicas donâ€™t expire data until they receive instructions to do so from their primaries, and primaries donâ€™t expire keys until theyâ€™re accessed ( GitHub issue). (In fact, primaries  do randomly sample keys from time to time, expiring them as appropriate, see â€œ How Redis Expires Keysâ€.)</p><p> ç„¶åï¼Œæ­¥éª¤2å°†å‡»ä¸­redis primaryã€‚åœ¨è¯¥æ•°æ®åº“è°ƒç”¨æœŸé—´ï¼ŒRediså°†è¿‡æœŸå‰ä¸€ä¸ªçª—å£æ•°æ®å¹¶è¿”å›æ–°é€Ÿç‡é™åˆ¶çš„æ•°æ®ã€‚è¿™æ˜¯ä¸€ä¸ªå·²çŸ¥çš„redisé™åˆ¶ï¼šå‰¯æœ¬ä¸ä¼šè¿‡æœŸæ•°æ®ï¼Œç›´åˆ°ä»–ä»¬ä»åŸåˆçº§æ¥æ”¶åˆ°è¿™æ ·åšï¼Œå¹¶ä¸”åˆé€‰ä¸ä¼šåˆ°æœŸé”®ï¼ˆGitHubé—®é¢˜ï¼‰ç›´åˆ°å®ƒä»¬æ‰ä¼šè¿‡æœŸï¼ˆå®é™…ä¸Šï¼ŒPrimariesä¸æ—¶ä¼šéšæœºé‡‡æ ·é”®ï¼Œé€‚å½“åˆ°æœŸï¼Œè¯·å‚é˜…â€œRediså¦‚ä½•åˆ°æœŸé”®â€ã€‚ï¼‰</p><p>  Basically, the same fix as above: instead of relying on Redisâ€™s TTL to expire old rate limit windows, we needed to manage that feature in the application. (The application should be prepared to read stale data from replicas, then ignore it.)</p><p>  åŸºæœ¬ä¸Šï¼Œå¦‚ä¸Šæ‰€è¿°ï¼Œç›¸åŒçš„ä¿®å¤ï¼šè€Œä¸æ˜¯ä¾èµ–äºRedisçš„TTLæ¥è¿‡æœŸæ—§é€Ÿç‡é™åˆ¶çª—å£ï¼Œè€Œæ˜¯éœ€è¦åœ¨åº”ç”¨ç¨‹åºä¸­ç®¡ç†è¯¥åŠŸèƒ½ã€‚ ï¼ˆåº”ç”¨ç¨‹åºåº”å‡†å¤‡å¥½ä»å‰¯æœ¬é˜…è¯»é™ˆæ—§æ•°æ®ï¼Œç„¶åå¿½ç•¥å®ƒã€‚ï¼‰</p><p> Even after fixing that, a better design was required: in the case of rate-limited requests, we should avoid a second call to the database. The clientâ€™s window might expire between the two calls, resulting in the kind of inconsistent response described above. This fix required improving the Ruby code that prepared responses so that the response from Step 1 above was used to populate  X-RateLimit-... headers.</p><p> å³ä½¿åœ¨ä¿®å¤ä¹‹åï¼Œè¿˜éœ€è¦æ›´å¥½çš„è®¾è®¡ï¼šåœ¨é€Ÿç‡æœ‰é™çš„è¯·æ±‚çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬åº”è¯¥é¿å…å¯¹æ•°æ®åº“çš„ç¬¬äºŒä¸ªè°ƒç”¨ã€‚å®¢æˆ·ç«¯çš„çª—å£å¯èƒ½ä¼šåœ¨ä¸¤ä¸ªå‘¼å«ä¹‹é—´è¿‡æœŸï¼Œä»è€Œå¯¼è‡´ä¸Šè¿°å“åº”çš„ç±»å‹ã€‚æ­¤ä¿®å¤ç¨‹åºéœ€è¦æ”¹è¿›ç¼–å†™å“åº”çš„Rubyä»£ç ï¼Œä»¥ä¾¿ä¸Šé¢æ­¥éª¤1çš„å“åº”å¡«å……X-Ratelimit -...æ ‡é¢˜ã€‚ </p><p>   -- RATE_SCRIPT:-- count a request for a client-- and return the current state for the client-- rename the inputs for clarity belowlocal rate_limit_key = KEYS[1]local increment_amount = tonumber(ARGV[1])local next_expires_at = tonumber(ARGV[2])local current_time = tonumber(ARGV[3])local expires_at_key = rate_limit_key .. &#34;:exp&#34;local expires_at = tonumber(redis.call(&#34;get&#34;, expires_at_key))if not expires_at or expires_at &lt; current_time then -- this is either a brand new window, -- or this window has closed, but redis hasn&#39;t cleaned up the key yet -- (redis will clean it up in one more second) -- initialize a new rate limit window redis.call(&#34;set&#34;, rate_limit_key, 0) redis.call(&#34;set&#34;, expires_at_key, next_expires_at) -- tell Redis to clean this up _one second after_ the expires-at time. -- that way, clock differences between Ruby and Redis won&#39;t cause data to disappear. -- (Redis will only clean up these keys &#34;long after&#34; the window has passed) redis.call(&#34;expireat&#34;, rate_limit_key, next_expires_at + 1) redis.call(&#34;expireat&#34;, expires_at_key, next_expires_at + 1) -- since the database was updated, return the new value expires_at = next_expires_atend-- Now that the window is either known to already exist _or_ be freshly initialized,-- increment the counter (`incrby` returns a number)local current = redis.call(&#34;incrby&#34;, rate_limit_key, increment_amount)return { current, expires_at } -- CHECK_SCRIPT:-- Getting both the value and the expiration-- of key as needed by our algorithm needs to be ran-- in an atomic way, hence the script.-- rename the inputs for clarity belowlocal rate_limit_key = KEYS[1]local expires_at_key = rate_limit_key .. &#34;:exp&#34;local current_time = tonumber(ARGV[1])local tries = tonumber(redis.call(&#34;get&#34;, rate_limit_key))local expires_at = nil -- maybe overridden belowif not tries then -- this client hasn&#39;t initialized a window yet -- let this fall through to returning {nil, nil}, -- where the application will provide defaultselse -- we found a number of tries, now check -- if this window is actually expired expires_at = tonumber(redis.call(&#34;get&#34;, expires_at_key)) if not expires_at or expires_at &lt; current_time then -- this window hasn&#39;t been cleaned up by Redis yet, but it has closed. -- (maybe it was _partly_ cleaned up, if we found `tries` but not `expires_at`) -- ignore the data in the database; return a fresh window instead tries = nil expires_at = nil endend-- Maybe {nil, nil} if the window is brand new (or expired)return { tries, expires_at }</p><p>-  Rate_scriptï¼š - è®¡ç®—å®¢æˆ·ç«¯çš„è¯·æ±‚ - å¹¶è¿”å›å®¢æˆ·ç«¯çš„å½“å‰çŠ¶æ€ - é‡å‘½åè¾“å…¥çš„è¾“å…¥ä¸‹è°ƒä»¥ä¸‹æœ¬åœ°incly_key =é”®[1]æœ¬åœ°increntment_amount = tonumberï¼ˆargv [1]ï¼‰local next_expires_at = tonumber ï¼ˆargv [2]ï¼‰æœ¬åœ°current_time = tonumberï¼ˆargv [3]ï¼‰æœ¬åœ°expires_at_key = paters_limit_key ..ï¼†ï¼ƒ34;ï¼šexpï¼†ï¼ƒ34; local affires_at = tonumberï¼ˆredis.callï¼ˆï¼†ï¼ƒ34; getï¼†ï¼ƒ34; expires_kate_key ï¼‰ï¼‰å¦‚æœæ²¡æœ‰expires_atæˆ–affires_atï¼†lt; thount_timeç„¶å - è¿™æ˜¯ä¸€ä¸ªå…¨æ–°çš„çª—å£ï¼Œ - æˆ–è€…è¿™ä¸ªçª—å£å·²ç»å…³é—­ï¼Œä½†æ˜¯redis hasnï¼†ï¼ƒ39; tæ¸…ç†äº†é’¥åŒ™ - ï¼ˆRedisä¼šå†æ¬¡æ¸…é™¤å®ƒï¼‰ - åˆå§‹åŒ–ä¸€ä¸ªæ–°çš„é€Ÿç‡é™åˆ¶çª—å£redis.callï¼ˆï¼†ï¼ƒ34; setï¼†ï¼ƒ34; hate_limit_keyï¼Œ0ï¼‰redis.callï¼ˆï¼†ï¼ƒ34; setï¼†ï¼ƒ34; affires_at_keyï¼Œnexp_expires_atï¼‰ - å‘Šè¯‰Redisæ¸…é™¤_one _åˆ°æœŸå_one _one - æ—¶é—´ã€‚ - è¿™ç§æ–¹å¼ï¼ŒRubyå’ŒRedisä¹‹é—´çš„æ—¶é’Ÿå·®å¼‚Wonï¼†ï¼ƒ39; tå¯¼è‡´æ•°æ®æ¶ˆå¤±ã€‚ - ï¼ˆRedisåªä¼šæ¸…ç†è¿™äº›é’¥åŒ™ï¼†ï¼ƒ34;çª—å£å·²ç»è¿‡å»ï¼‰rediS.callï¼ˆï¼†ï¼ƒ34; expireatï¼†ï¼ƒ34;ï¼Œpater_limit_keyï¼Œnext_expires_at + 1ï¼‰redis.callï¼ˆï¼†ï¼ƒ34 ; expireatï¼†ï¼ƒ34;ï¼Œaffires_at_keyï¼Œnexp_expires_at + 1ï¼‰ - ç”±äºæ›´æ–°äº†æ•°æ®åº“ï¼Œå› æ­¤è¿”å›æ–°å€¼expires_at = next_expires_atend--ç°åœ¨çª—å£å·²çŸ¥å·²å­˜åœ¨_or_åˆšåˆšåˆå§‹åŒ–ï¼Œ - å¢é‡è®¡æ•°å™¨ï¼ˆ `Incby`è¿”å›ä¸€ä¸ªæ•°å­—ï¼‰æœ¬åœ°Current = redis.callï¼ˆï¼†ï¼ƒ34; incrbyï¼†ï¼ƒ34; ratury_limit_keyï¼Œincrentment_amountï¼‰return {currentï¼Œappires_at}  -  check_scriptï¼š - è·å¾—é”®çš„å€¼å’Œåˆ°æœŸæ ¹æ®éœ€è¦çš„ç®—æ³•éœ€è¦ä»¥åŸå­æ–¹å¼ran--å› æ­¤è„šæœ¬.--é‡å‘½åè¾“å…¥çš„è¾“å…¥ï¼Œä»¥ä¾¿æ¸…æ¥šåœ°ä¸‹é™è‡³æœ¬åœ°rate_key_key = keys [1] local expires_at_key = rate_limit_key ..ï¼†ï¼ƒ34;ï¼šexpï¼†ï¼ƒ34;å½“åœ°current_time = tonumberï¼ˆargv [1]ï¼‰æœ¬åœ°å°è¯•= tonumberï¼ˆredis.callï¼ˆï¼†ï¼ƒ34; getï¼†ï¼ƒ34; pare_limit_keyï¼‰ï¼‰local appires_at = nil  - å¯èƒ½è¢«è¦†ç›–ä»¥ä¸‹åŸå› ï¼Œè€Œä¸æ˜¯å°è¯• -  thi Så®¢æˆ·ç«¯HASNï¼†ï¼ƒ39; Tåˆå§‹åŒ–äº†ä¸€ä¸ªçª—å£ - è®©è¿™ä¸€ç‚¹æ‰èƒ½è¿”å›{nilï¼Œnil}ï¼Œ - åœ¨åº”ç”¨ç¨‹åºæä¾›defaultselseçš„åœ°æ–¹ - æˆ‘ä»¬å‘ç°äº†è®¸å¤šå°è¯•ï¼Œç°åœ¨æ£€æŸ¥ - å¦‚æœè¿™ä¸ªçª—å£æ˜¯å®é™…ä¸Šå·²è¿‡æœŸexpires_at = tonumberï¼ˆredis.callï¼ˆï¼†ï¼ƒ34; getï¼†ï¼ƒ34; getï¼†ï¼ƒ34; expires_at_keyï¼‰ï¼‰ï¼Œå¦‚æœæ²¡æœ‰expires_atæˆ–affires_at <tï¼†lt; Current_Timeé‚£ä¹ˆ - è¿™ä¸ªçª—å£Hasnï¼†ï¼ƒ39; redisæ¸…ç†è¿‡æ¥ï¼Œä½†å®ƒå·²å…³é—­ã€‚ - ï¼ˆä¹Ÿè®¸å®ƒæ˜¯_Partly_æ¸…ç†ï¼Œå¦‚æœæˆ‘ä»¬å‘ç°äº†ï¼Ÿå°è¯•ä½†ä¸æ˜¯`Expires_at`ï¼‰ - å¿½ç•¥æ•°æ®åº“ä¸­çš„æ•°æ®;è¿”å›ä¸€ä¸ªæ–°é²œçš„çª—å£ï¼Œè€Œä¸æ˜¯tries = nil expires_at = nil endend--å¦‚æœçª—å£æ˜¯å…¨æ–°çš„ï¼ˆæˆ–è¿‡æœŸï¼‰è¿”å›{triesï¼Œaffires_at}ï¼Œé‚£ä¹ˆ{nilï¼Œnil}</p><p>  Weâ€™ve learned a lot from this new approach, but thereâ€™s still one shortcoming weâ€™re considering: the current implementation doesnâ€™t increment the â€œcurrentâ€ rate limit value until after the request is  finished. We do this because we donâ€™t charge clients for  304 Not Modified responses (this can happen when the client provides an E-Tag). A better implementation might increment the value when the request  starts, then refunds the client if the response is  304. That would prevent some edge cases where a client can exceed its limit when the final allowed request is still being processed.</p><p>  æˆ‘ä»¬ä»è¿™ç§æ–°æ–¹æ³•å­¦åˆ°äº†å¾ˆå¤šï¼Œä½†æˆ‘ä»¬ä»ç„¶å­˜åœ¨ä¸€ä¸ªç¼ºç‚¹ï¼šå½“å‰çš„å®ç°ä¸ä¼šé€’å¢â€œå½“å‰â€é€Ÿç‡é™åˆ¶å€¼ï¼Œç›´åˆ°è¯·æ±‚å®Œæˆåã€‚æˆ‘ä»¬è¿™æ ·åšæ˜¯å› ä¸ºæˆ‘ä»¬ä¸å‘å®¢æˆ·æ”¶å–304çš„å®¢æˆ·ç«¯ï¼Œè€Œä¸æ˜¯ä¿®æ”¹çš„å“åº”ï¼ˆå½“å®¢æˆ·ç«¯æä¾›ç”µå­æ ‡ç­¾æ—¶å¯èƒ½ä¼šå‘ç”Ÿï¼‰ã€‚æ›´å¥½çš„å®ç°å¯èƒ½ä¼šåœ¨è¯·æ±‚å¼€å§‹æ—¶é€’å¢å€¼ï¼Œç„¶åå¦‚æœå“åº”ä¸º304ï¼Œåˆ™ä¼šé€€å›å®¢æˆ·ç«¯ã€‚è¿™å°†é˜»æ­¢å®¢æˆ·ç«¯åœ¨ä»åœ¨å¤„ç†æœ€ç»ˆå…è®¸çš„è¯·æ±‚æ—¶å®¢æˆ·ç«¯å¯ä»¥è¶…è¿‡å…¶é™åˆ¶çš„è¾¹ç¼˜æƒ…å†µã€‚</p><p> After working out the issues described in this post, the new rate limiter has worked great. It has improved reliability, fixed issues for clients, and reduced our support load (eventually ğŸ˜‰) and the architecture is ready for our next wave of platform improvements.</p><p> åœ¨è§£å†³è¿™ç¯‡æ–‡ç« ä¸­æè¿°çš„é—®é¢˜ä¹‹åï¼Œæ–°çš„é€Ÿç‡é™åˆ¶å™¨å·²ç»å¾ˆå¥½ã€‚å®ƒå…·æœ‰æ”¹è¿›çš„å¯é æ€§ï¼Œå›ºå®šå®¢æˆ·çš„é—®é¢˜ï¼Œå¹¶å‡å°‘äº†æˆ‘ä»¬çš„æ”¯æŒè´Ÿè½½ï¼ˆæœ€ç»ˆğŸ˜‰ï¼‰ï¼Œå¹¶ä¸”è¯¥æ¶æ„å·²å‡†å¤‡å¥½è¿›è¡Œæˆ‘ä»¬çš„ä¸‹ä¸€æ³¢å¹³å°æ”¹è¿›ã€‚ </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://github.blog/2021-04-05-how-we-scaled-github-api-sharded-replicated-rate-limiter-redis/">https://github.blog/2021-04-05-how-we-scaled-github-api-sharded-replicated-rate-limiter-redis/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/github/">#github</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/api/">#api</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/redis/">#redis</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è®¾è®¡/">#è®¾è®¡</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/åˆ›æ„/">#åˆ›æ„</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/æ‘„å½±/">#æ‘„å½±</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/å›¾ç‰‡/">#å›¾ç‰‡</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/æ¸¸æˆ/">#æ¸¸æˆ</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è½¯ä»¶/">#è½¯ä»¶</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è§†é¢‘/">#è§†é¢‘</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/æ‰‹æœº/">#æ‰‹æœº</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/å¹¿å‘Š/">#å¹¿å‘Š</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/ç½‘ç«™/">#ç½‘ç«™</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/å…è´¹/">#å…è´¹</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/ä¸‹è½½/">#ä¸‹è½½</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/å¾®è½¯/">#å¾®è½¯</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/éŸ³ä¹/">#éŸ³ä¹</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/åšå®¢/">#åšå®¢</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è‹¹æœ/">#è‹¹æœ</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/æ¶æ/">#æ¶æ</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è‰ºæœ¯/">#è‰ºæœ¯</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/è°·æ­Œ/">#è°·æ­Œ</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/å·¥å…·/">#å·¥å…·</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>